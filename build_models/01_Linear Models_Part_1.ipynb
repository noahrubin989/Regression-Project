{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7319fad3",
   "metadata": {},
   "source": [
    "# **Linear Models (Part 1)**\n",
    "\n",
    "OLS, Ridge and Lasso Regression\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "May 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97df144",
   "metadata": {},
   "source": [
    "# **Ordinary Least Squares (OLS)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bca4e",
   "metadata": {},
   "source": [
    "### <u>Main Ideas</u>\n",
    "\n",
    "- OLS tries to create a fit that minimises the sum of the squared residuals (sometimes called residual sum of squares, RSS)\n",
    "- Residuals are the (red) vertical distances from the fit to our data points\n",
    "- OLS with multiple independent variables is referred to as Multiple Regression\n",
    "![mr_viz](OLS_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b3dd5",
   "metadata": {},
   "source": [
    "### <u>Algorithm Details</u>\n",
    "\n",
    "In the population, the multiple regression model can be given as:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + ... + \\beta_p x_{i,p} + \\epsilon_i$$\n",
    "\n",
    "* $y_i$ represents the $i^{\\text{th}}$ observation for our response variable\n",
    "* $\\beta_0$ is the true intercept in the population\n",
    "* $\\beta_1$ is the true coefficient for our first predictor\n",
    "* $\\beta_2$ is the true coefficient for our second predictor\n",
    "* $\\beta_p$ is the true coefficient for our pth predictor\n",
    "* $x_1, x_2,..., x_p$ are predictor variables\n",
    "* $\\epsilon_i$ represent the unobservables/error term which is assumed to be distributed normally with:\n",
    "\n",
    "$$E(\\epsilon_i) = 0$$\n",
    "$$Var(\\epsilon_i) = \\sigma^2 \\text{(constant but unknown error variance)}$$ \n",
    "\n",
    "It turns out that when deriving OLS with multiple predictor variables, it is easier to see it work as a process of manipulating matrices. Applying the logic from above, it can be seen that\n",
    "\n",
    "$$y_1 = \\beta_0(1) + \\beta_1 x_{1,1} + \\beta_2 x_{1,2} + ... + \\beta_p x_{1,p} + \\epsilon_1$$\n",
    "$$y_2 = \\beta_0(1) + \\beta_1 x_{2,1} + \\beta_2 x_{2,2} + ... + \\beta_p x_{2,p} + \\epsilon_2$$\n",
    "$$y_3 = \\beta_0(1) + \\beta_1 x_{3,1} + \\beta_2 x_{3,2} + ... + \\beta_p x_{3,p} + \\epsilon_3$$\n",
    "$$\\vdots$$\n",
    "$$y_n = \\beta_0(1) + \\beta_1 x_{n,1} + \\beta_2 x_{n,2} + ... + \\beta_p x_{n,p} + \\epsilon_n$$\n",
    "\n",
    "\n",
    "From this it can be seen that we can store our all of our $y_i$ values in an $(n \\times 1)$ column vector and our of errors in a vector of the same size, such that:\n",
    "\n",
    "$$\\overrightarrow{y} = \\begin{pmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{pmatrix} \n",
    "\\text{, } \n",
    "\\overrightarrow{\\epsilon} = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Each of the $\\beta_j$'s never change across observations so we can store all of our population beta coefficients in a column vector with dimension $((p+1) \\times 1)$ as we have $p$ predictor variables as well as an intercept term. Hence, our coefficient vector of betas can be expressed as \n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\beta} = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\vdots\\\\\\beta_p\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Lastly if we look at each row we see that each beta is multiplied by an $x_{i,j}$ term, though $\\beta_0$ is just multiplied by 1. So if we take all of our $x_{i, j}$'s out and place them in a [design matrix](http://gradientdescending.com/design-matrix-for-regression-explained/), we simply get all of our $p$ predictor variables as well as a column of ones (which caters for the existence of an intercept term). Hence our design matrix is of dimension $(n \\times (p+1))$ and takes the form\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & ... & x_{1,p}\\\\\n",
    "1 & x_{2,1} & x_{2,2} & ... & x_{2,p}\\\\\n",
    "1 & x_{3,1} & x_{3,2} & ... & x_{3,p}\\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    "1 & x_{n,1} & x_{n,2} & ... & x_{n,p}\\\\\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "If we multiply our design matrix and our vector of coefficients, and then add the vector of error terms, the following equation allows us to find each individual $y_i$ using matrix multiplication and vector addition\n",
    "\n",
    "$$\\begin{pmatrix}y_1\\\\y_2\\\\y_3\\\\\\vdots\\\\y_n\\end{pmatrix} = \\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & ... & x_{1,p}\\\\\n",
    "1 & x_{2,1} & x_{2,2} & ... & x_{2,p}\\\\\n",
    "1 & x_{3,1} & x_{3,2} & ... & x_{3,p}\\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    "1 & x_{n,1} & x_{n,2} & ... & x_{n,p}\\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}{\\beta_0}\\\\\\beta_1\\\\{\\beta_2}\\\\\\vdots\\\\{\\beta_p}\\end{pmatrix} + \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\epsilon_3\\\\\\vdots\\\\\\epsilon_n\\end{pmatrix}$$\n",
    "\n",
    "In short,\n",
    "\n",
    "$$\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888aaa7",
   "metadata": {},
   "source": [
    "As we wish to minimise the sum of squared deviations between the actual value of $y$ and its expectated value $X\\vec{\\beta}$ (assuming errors are noramlly distribited with mean zero and unknown but constant variance),\n",
    "we can minimise the quantity \n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - (X\\vec{\\beta})_i)^2 = \\vec{\\epsilon}^T \\vec{\\epsilon} = \\epsilon_1^2 + \\epsilon_2^2 + ... + \\epsilon_n^2$$\n",
    "\n",
    "Also expressable as \n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\beta)^T(\\vec{y} - X\\beta)$$ \n",
    "\n",
    "Expanding the brackets,\n",
    "\n",
    "$$J(\\vec{\\beta}) = \\vec{y}^T\\vec{y} -2\\vec{b}^TX^T \\vec{y} + \\vec{\\beta}^TX^TX\\vec{\\beta}$$\n",
    "\n",
    "Now, as we wish to minimise this cost function $J(\\vec{\\beta})$ we can take the partial derivative with respect to $\\vec{\\beta}$ and set this to zero. It always ends up being a minimum as $J(\\vec{\\beta})$ is known to be a [convex function](https://en.wikipedia.org/wiki/Convex_function) with a positive definite Hessian matrix when considering second derivatives\n",
    "\n",
    "Thus, taking the partial derivative with respect to beta (using matrix calculus) we obtain, \n",
    "\n",
    "$$\\frac{\\partial J(\\vec{\\beta})}{\\partial \\vec{\\beta}} = -2X{^T}\\vec{y} + 2X^TX\\vec{\\beta}$$ \n",
    "\n",
    "Setting this quantity to 0 for a mininum, we can obtain an estimnator $\\vec{b}$ for $\\vec{\\beta}$\n",
    "\n",
    "$$-2X{^T}\\vec{y} + 2X^TX\\vec{b} = 0$$\n",
    "\n",
    "Dividing by two and rearranging,\n",
    "\n",
    "$$X^TX\\vec{b} = X{^T}\\vec{y}.$$\n",
    "\n",
    "\"Pre-multiplying\" both sides by $(X^TX)^{-1}$ we minimise the sum of squared residuals through \n",
    "\n",
    "$$\\vec{b} = (X^TX)^{-1}X{^T}\\vec{y}$$\n",
    "\n",
    "...assuming the model is a full rank linear model where $(X^TX)$ is non-singular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5990d",
   "metadata": {},
   "source": [
    "#### <ins>Expectation & Variance</ins>\n",
    "\n",
    "Assuming a full rank linear model, the OLS estimator $\\vec{b}$ is unbiased under multiple regression assumptions 1-4, specified in [Introductory Econometrics: A Modern Approach](https://economics.ut.ac.ir/documents/3030266/14100645/Jeffrey_M._Wooldridge_Introductory_Econometrics_A_Modern_Approach__2012.pdf) since:\n",
    "\n",
    "$$E(\\vec{b}) = E((X^TX)^{-1}X^{T}\\vec{y}) = (X^TX)^{-1}X^{T}E(\\vec{y})$$\n",
    "\n",
    "Since $E(\\vec{y}) = E(X\\vec{\\beta} + \\vec{\\epsilon}) = X\\vec{\\beta}$ it follows that:\n",
    "\n",
    "$$E(\\vec{b}) = (X^TX)^{-1}X^{T}X\\vec{\\beta} = \\vec{\\beta}$$\n",
    "\n",
    "---\n",
    "\n",
    "Similarly the variance of our $\\vec{b}$ estimator is given as:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\text{Var}((X^TX)^{-1}X^{T}\\vec{y})$$\n",
    "\n",
    "As $(X^TX)^{-1}X^{T}$ is a $(p+1) \\times n$ matrix of real numbers and $\\vec{y}$ is an $n \\times 1$ column-vector-valued random variable, it follows that\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = (X^TX)^{-1}X^T\\text{Var}\\vec{(y)}((X^TX)^{-1}X^T)^T$$\n",
    "\n",
    "Using matrix transpose laws, and the fact that $\\text{Var}\\vec{(y)} = \\text{Var}(X\\vec{\\beta} + \\vec{\\epsilon}) = \\sigma^2I$ (where $I$ is an $n \\times n$ identity matrix)\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = (X^TX)^{-1}X^T\\sigma^2IX((X^TX)^T)^{-1}$$\n",
    "\n",
    "Assuming constant error variance, \n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\sigma^2I(X^TX)^{-1}X^TX((X^TX)^T)^{-1}$$\n",
    "\n",
    "Cancelling terms $(X^TX)^{-1}$ and $X^TX$, it follows that \n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\sigma^2((X^TX)^T)^{-1}$$\n",
    "\n",
    "Since $X^TX$ is symmetric,\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\sigma^2(X^TX)^{-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafbf08",
   "metadata": {},
   "source": [
    "#### <ins>The Gauss Markov Theorem</ins>\n",
    "\n",
    "The Gauss Markov Theorem implies that the OLS estimator has the lowest sampling variance out of all linear unbiased estimators if Gauss Markov Assumptions are adhered to. Since we are dealing with unbiased estimators, the fact that OLS has the lowest sampling variance implies it has the lowest mean squared error.\n",
    "\n",
    "Consider $\\vec{b^*}$, an arbitrary linear unbiased estimator of $\\beta$ where $\\vec{b^*} = ((X^TX)^{-1}X^T + M)\\vec{y}$, where $M$ is a $((p+1) \\times n)$ non-zero matrtix\n",
    "\n",
    "Taking the expectation of $\\vec{b^*}$, we obtain \n",
    "\n",
    "$$E(\\vec{b^*}) = E((X^TX)^{-1}X^T + M)\\vec{y})$$\n",
    "\n",
    "Substituting $\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}$\n",
    "\n",
    "$$E(\\vec{b^*}) = E((X^TX)^{-1}X^T + M)(X\\vec{\\beta} + \\vec{\\epsilon}))$$\n",
    "\n",
    "Expanding the brackets, and through linearity of the expectation operator\n",
    "\n",
    "$$E(\\vec{b^*}) = E([(X^TX)^{-1}X^T + M)]X\\vec{\\beta}) + [(X^TX)^{-1}X^T + M)]E(\\vec{\\epsilon})$$\n",
    "\n",
    "Since we assume $E(\\epsilon_i) = 0$\n",
    "\n",
    "$$E(\\vec{b^*}) = E([(X^TX)^{-1}X^T + M)]X\\vec{\\beta})$$\n",
    "\n",
    "Expanding the brackets, and the fact that $X$ is not considered random,\n",
    "\n",
    "$$E(\\vec{b^*}) = (X^TX)^{-1}X^TX\\vec{\\beta} + MX\\vec{\\beta}$$\n",
    "\n",
    "Due to cancellation in the first term\n",
    "\n",
    "$$E(\\vec{b^*}) = \\vec{\\beta} + MX\\vec{\\beta} = (I + MX)\\vec{\\beta}$$\n",
    "\n",
    "implying that this estimator can only be unbiased if $MX = 0.$\n",
    "\n",
    "---\n",
    "\n",
    "If we consider the variance of this arbitrary estimator\n",
    "\n",
    "\n",
    "\\begin{align} \n",
    "\\text{Var}(\\vec{b^*}) &= \\text{Var}((X^TX)^{-1}X^T + M)\\vec{y}) \\\\\n",
    "&= ((X^TX)^{-1}X^T + M)\\text{Var}(\\vec{y})((X^TX)^{-1}X^T + M)^T \\tag{since $((X^TX)^{-1}X^T + M)$ is not random}\\\\ \n",
    "&= \\sigma^2((X^TX)^{-1}X^T + M)((X^TX)^{-1}X^T + M)^T \\tag{assuming constant, unknown error variance}\\\\ \n",
    "&= \\sigma^2[((X^TX)^{-1}X^T + M)(X(X^TX)^{-1} + M^T)] \\tag{applying matrix transpose laws} \\\\ \n",
    "&= \\sigma^2[  (X^TX)^{-1}X^TX(X^TX)^{-1} + (X^TX)^{-1}X^TM^T + MX(X^TX)^{-1} + MM^T] \\tag{expanding the brackets} \\\\  \n",
    "&= \\sigma^2(X^TX)^{-1} + \\sigma^2(X^TX)^{-1}X^TM^T + \\sigma^2MX(X^TX)^{-1} + \\sigma^2MM^T \\tag{expanding $\\sigma^2$ out} \\\\\n",
    "&= \\text{Var}(\\vec{\\beta}) + \\sigma^2(X^TX)^{-1}(MX)^T + \\sigma^2MX(X^TX)^{-1} + \\sigma^2MM^T \\tag{since $\\text{Var}(\\vec{\\beta}) = \\sigma^2(X^TX)^{-1}$}\\\\\n",
    "&= \\text{Var}(\\vec{\\beta}) + \\sigma^2MM^T \\tag{as $MX=0$ for unbiased estimate} \\\\\n",
    "&\\geq \\text{Var}(\\vec{\\beta}) \\tag{since $MM^T$ is positive semi-definite}\n",
    "\\end{align} \n",
    "\n",
    "Hence, as $MM^T$ is positive semi-definite, then by the Gauss Markov theorem, $\\text{Var}(\\vec{b}) \\leq \\text{Var}(\\vec{b^*})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf364c7a",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39fdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing to 5dp\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python files\n",
    "import data_prep\n",
    "import helper_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646b6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674c5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b9acd",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "* Log the GDP Variable\n",
    "* Apply one hot encoding to the 'Status' feature\n",
    "* Perform [KNN imputation](https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e)\n",
    "* Once the previous steps have been implemented, run OLS regression on the training data and through cross validation eventually see which value of K was optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7e410c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('numeric',\n",
       "                                                  Pipeline(steps=[('logger',\n",
       "                                                                   FunctionTransformer(func=<ufunc 'log'>))]),\n",
       "                                                  ['GDP_cap']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['Status'])])),\n",
       "                ('imputation', KNNImputer()), ('model', LinearRegression())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function included in all the steps\n",
    "ols_pipeline = data_prep.create_pipeline(LinearRegression())\n",
    "ols_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f73e9b",
   "metadata": {},
   "source": [
    "**Hyperparamter Tuning**\n",
    "\n",
    "While linear regression does not have any hyperparameters to tune, the KNN imputation beforehand is governed by a value of $k$, which through our pipeline and grid search, we can find what value of $k$, when followed by a linear regression minimised mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f81cc280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputation__n_neighbors': 3, 'imputation__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'imputation__n_neighbors':np.arange(3, 16, 2), 'imputation__weights': ['uniform', 'distance']}\n",
    "\n",
    "best_estimator, best_params = data_prep.exhaustive_search(X_train, \n",
    "                                                          y_train, \n",
    "                                                          ols_pipeline, \n",
    "                                                          param_grid, \n",
    "                                                          cv=5, \n",
    "                                                          scoring='neg_mean_squared_error')\n",
    "final_model = best_estimator.fit(X_train, y_train)\n",
    "print(best_params)\n",
    "\n",
    "# print(final_model.named_steps['model'].intercept_)\n",
    "# print(final_model.named_steps['model'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a98b5",
   "metadata": {},
   "source": [
    "## **Model Evaluation**\n",
    "* Statsmodels is useful for model evaluation and hypothesis testing, though it does not support preprocessing and model building pipelines the way sklearn does\n",
    "* Will therefore perform the previous steps from scratch so that we can evaluate the model in statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d74a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually applies all the steps specified in the pipeline\n",
    "X_train_statsmodels, X_test_statsmodels = helper_funcs.apply_preprocessing_steps(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdfa03c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Life_exp</td>     <th>  R-squared:         </th> <td>   0.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3069.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 08 Jan 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:50:33</td>     <th>  Log-Likelihood:    </th> <td> -4754.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2056</td>      <th>  AIC:               </th> <td>   9527.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2047</td>      <th>  BIC:               </th> <td>   9578.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>  -17.0359</td> <td>   21.452</td> <td>   -0.794</td> <td> 0.427</td> <td>  -59.105</td> <td>   25.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Year</th>              <td>    0.0404</td> <td>    0.011</td> <td>    3.753</td> <td> 0.000</td> <td>    0.019</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>InfantMortality</th>   <td>   -0.2106</td> <td>    0.005</td> <td>  -45.826</td> <td> 0.000</td> <td>   -0.220</td> <td>   -0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Health_exp</th>        <td>    0.2969</td> <td>    0.024</td> <td>   12.134</td> <td> 0.000</td> <td>    0.249</td> <td>    0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Employment</th>        <td>    0.0274</td> <td>    0.006</td> <td>    4.601</td> <td> 0.000</td> <td>    0.016</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Status</th>            <td>   -1.7930</td> <td>    0.210</td> <td>   -8.545</td> <td> 0.000</td> <td>   -2.204</td> <td>   -1.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MeanSchooling</th>     <td>   -0.4358</td> <td>    0.034</td> <td>  -12.825</td> <td> 0.000</td> <td>   -0.502</td> <td>   -0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ElectricityAccess</th> <td>    0.0849</td> <td>    0.005</td> <td>   18.368</td> <td> 0.000</td> <td>    0.076</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ln(GDP_cap)</th>       <td>    0.8012</td> <td>    0.074</td> <td>   10.805</td> <td> 0.000</td> <td>    0.656</td> <td>    0.947</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>33.965</td> <th>  Durbin-Watson:     </th> <td>   2.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  58.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.110</td> <th>  Prob(JB):          </th> <td>2.47e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.793</td> <th>  Cond. No.          </th> <td>7.99e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.99e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               Life_exp   R-squared:                       0.923\n",
       "Model:                            OLS   Adj. R-squared:                  0.923\n",
       "Method:                 Least Squares   F-statistic:                     3069.\n",
       "Date:                Sat, 08 Jan 2022   Prob (F-statistic):               0.00\n",
       "Time:                        22:50:33   Log-Likelihood:                -4754.6\n",
       "No. Observations:                2056   AIC:                             9527.\n",
       "Df Residuals:                    2047   BIC:                             9578.\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const               -17.0359     21.452     -0.794      0.427     -59.105      25.033\n",
       "Year                  0.0404      0.011      3.753      0.000       0.019       0.062\n",
       "InfantMortality      -0.2106      0.005    -45.826      0.000      -0.220      -0.202\n",
       "Health_exp            0.2969      0.024     12.134      0.000       0.249       0.345\n",
       "Employment            0.0274      0.006      4.601      0.000       0.016       0.039\n",
       "Status               -1.7930      0.210     -8.545      0.000      -2.204      -1.382\n",
       "MeanSchooling        -0.4358      0.034    -12.825      0.000      -0.502      -0.369\n",
       "ElectricityAccess     0.0849      0.005     18.368      0.000       0.076       0.094\n",
       "ln(GDP_cap)           0.8012      0.074     10.805      0.000       0.656       0.947\n",
       "==============================================================================\n",
       "Omnibus:                       33.965   Durbin-Watson:                   2.041\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               58.062\n",
       "Skew:                          -0.110   Prob(JB):                     2.47e-13\n",
       "Kurtosis:                       3.793   Cond. No.                     7.99e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.99e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary table for our model\n",
    "model = sm.OLS(y_train, X_train_statsmodels).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76e1c9",
   "metadata": {},
   "source": [
    "### **Heteroskedasticity**\n",
    "\n",
    "* Homoskedasticity in regression analysis is the idea that the variance of the unobserved factors $\\epsilon$ should not change across different segments of the population\n",
    "* Hence $\\text{Var}(\\epsilon|x_1, x_2, ... x_p) = \\sigma^2$ (constant error variance) must be adhered to to ensure homoskedasticity\n",
    "* OLS coefficients can still remain unbiased in the presence of heteroskedasticity, and we can still interpret $R^2$ and adjusted $R^2$ to assess goodness-of-fit. Though the problem is that with heteroskedasticity our $F$ statistic (in the above table) is no longer F-distributed and the various $t$ statistics are not t-distributed. Hypothesis testing on our $\\hat{\\beta}_j$ coefficients can't be done as our p-values will be distorted due to incorrect standard errors in bthe table above\n",
    "* This means that most of the output in the table above can't be interpreted, and hence we are less sure of the effects of certain variables\n",
    "\n",
    "<u>Detecting Heteroscedasticity</u>\n",
    "\n",
    "* To detect heteroskedasticity, we can use tools such as the [Breusch-Pagan Test](https://www.real-statistics.com/multiple-regression/heteroskedasticity/breusch-pagan-test/), or we can simply graph a residual plot. \n",
    "* There are also other methods such as using [White's Test](https://www.youtube.com/watch?v=M5xqpKzhyAM), which is similar to the Breusch Pagan test, but considers squared terms and well as cross products of independent variable combinations.\n",
    "\n",
    "---\n",
    "\n",
    "**The Breusch-Pagan Test**\n",
    "\n",
    "We define a null and alternate hypothesis\n",
    "\n",
    "$$H_0: \\text{Var}(\\epsilon|x_1, x_2, ... x_p) = \\sigma^2 (\\text{ homoskedastic errors})$$\n",
    "\n",
    "$$H_1: \\text{Var}(\\epsilon|x_1, x_2, ... x_p) \\neq \\sigma^2 (\\text{ heteroskedastic errors})$$\n",
    "\n",
    "It is also possible to define the null hypothesis as $E(\\epsilon^2|x_1, x_2, ... x_p) = \\sigma^2$ since  $\\text{Var}(\\epsilon|x_1, x_2, ... x_p) = E(\\epsilon^2|x_1, x_2, ... x_p) - [E(\\epsilon|x_1, x_2, ... x_p)]^2$. Due to the zero conditional mean assumption of no correlation between any of the regressors and the error term, this second term disappears, and we can therefore test whether $\\epsilon^2$ is related (in expected value) to one or more of the regressors in our model\n",
    "\n",
    "---\n",
    "\n",
    "1. Run a regression, with our dependent variable $y$ and all of our predictor $x$ variables\n",
    "2. As the regression estimate will likely never be a perfect fit for the data, there will be residuals present\n",
    "3. Store the squared residuals and then let it be the dependent variable for a new regression model, taking the form:\n",
    "\n",
    "$$\\hat{u}^2 = \\hat\\delta_0 + \\hat\\delta_1x_1 + \\hat\\delta_2x_2 + \\hat\\delta_3x_3 + ... \\hat\\delta_px_p$$\n",
    "\n",
    "where $\\hat{u}^2$ represents the fitted values for the squared residuals which became our new dependent variable, the $\\hat\\delta$'s represents the parameters that minimise RSS for this new model, and the $x$ variables represents InfantMortality. MeanSchooling etc.\n",
    "\n",
    "4. Calculate the $R^2$ coefficient of determination for this model and use this to calculate a test statistic known as the <ins>Lagrange Multiplier</ins>. This is given as:\n",
    "\n",
    "$$\\text{LM}=nR^{2}$$\n",
    "\n",
    "where $n$ refers to the number of observations in our training set. Under the null hypothesis, this is distributed as a [Chi-squared](https://www.sciencedirect.com/topics/mathematics/chi-square-distribution) random variable with $p$ degrees of freedom, where $p$ is the number of covariates ($x$ variables) in our current model. Depending on the significance level and $p$-values we either reject or fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562b5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Breusch Pagan Test...\n",
      "\n",
      "Lagrange Multiplier = 314.78029411402736, p-value = 2.9336818369803655e-63\n",
      "\n",
      "Heteroskedasticity is present in this model (at the 5% significance level)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heteroskedasticity is present in this model\n",
    "helper_funcs.test_heteroskedasticity(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa98f9",
   "metadata": {},
   "source": [
    "### **Multicollinearity**\n",
    "\n",
    "* It is nice to have features strongly correlated with life expectancy, but what if our predictor variables are correlated with each other?\n",
    "* \"In statistics, multicollinearity is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\" ~ Wikipedia\n",
    "\n",
    "--- \n",
    "\n",
    "**Why it can be problematic?**\n",
    "\n",
    "* Essentially if multicollinearity exists, it becomes very difficult to discern which variable is playing a larger role in our regression model as the two (or more) variables themselves are so similar.\n",
    "* The idea in regression is that our coefficients represent the change in $y$ when there is a one unit increase in a predictor variable $x$, Ceteris Paribus\n",
    "* But in cases where multicolinearity exists, often it is not possible to hold other predictor variables constant when one increases \n",
    "\n",
    "---\n",
    "\n",
    "**Mathematically...**\n",
    "\n",
    "- A model with significant multicolinearity will inflate the standard errors of the beta coefficients impacted by multicollinearity\n",
    "- As well as standard errors increasing in the prescence of multicollinearity, multicollinearity also makes our coefficients very sensitive to large swings. For example, taking a variable out or adding one in might cause large changes in our current coefficients. Hence we become less certain about what our coefficients might be in the population\n",
    "- Large standard errors will also distort your p-values as the standard error is used to calculate the t-statistic, which is then used to calculate the p-value. We might then conclude that a variable is insignificant and has no impact on $y$ when in fact it might\n",
    "- Though even when multicollinearity is present, the least-squares estimator can still be unbiased and efficient.\n",
    "- Though in cases where there is [perfect multicollinearity](https://www.dummies.com/education/economics/econometrics/perfect-multicollinearity-and-your-econometric-model/), the matrix $X^TX$ becomes singular, which means that OLS will not have a unique solution\n",
    "\n",
    "---\n",
    "\n",
    "**Checking for multicollinearity**\n",
    "\n",
    "1. Using a correlation heatmap\n",
    "    \n",
    "* This would involve looking at our correlation heatmap from before and seeing which **predictor** variables have correlation with one another close to -1 and 1.\n",
    "* This is nice for eyeballing 'red flags' but Variance Inflation Factor is a better option\n",
    "\n",
    "2. Using [Variance Inflation Factors (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor)\n",
    "\n",
    "The variance of the estimated coefficient for the jth predictor in our regression model is given as:\n",
    "\n",
    "$$\\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{\\sum_{i=1}^{n} {(x_{ij} - \\bar{x}_j)^2}} \\times \\text{VIF}_j$$\n",
    "\n",
    "Hence VIF is the factor by which $\\text{Var}(\\hat{\\beta}_j)$ increases due there being relationships between the $j^{th}$ predictor and other $x$ variables\n",
    "\n",
    "* VIF for each predictor variable is calculated using something known as **tolerance**. \n",
    "\n",
    "$$\\text{tolerance} = 1 - R^2_{\\text{When $x_j$ is the response variable}}$$\n",
    "\n",
    "* The VIF is then computed as:\n",
    "\n",
    "$$\\text{VIF} = \\frac{1}{\\text{tolerance}}$$\n",
    "\n",
    "---\n",
    "\n",
    "When calculating VIF for each predictor variable, we build auxilliary regression models. With an auxiliary regression model, we set one of our $x$ variables to be the response. There is no $y$ here as we are just trying to detect multicollinearity among our regressors\n",
    "\n",
    "$$\\hat{x}_1 = \\hat\\delta_0 + \\hat\\delta_1x_2 + \\hat\\delta_2x_3  + \\hat\\delta_3x_4 +...+ \\hat\\delta_px_p$$\n",
    "\n",
    "In similar fashion to this if we wanted to calculate the VIF for $x_2$ we would build an auxilliary regression of the form\n",
    "\n",
    "$$\\hat{x}_2 = \\hat\\gamma_0 + \\hat\\gamma_1x_1 + \\hat\\gamma_2x_3  + \\hat\\gamma_3x_4 +...+ \\hat\\gamma_px_p$$\n",
    "\n",
    "Now, because we don't want a model where one $x$ variable can be linearly predicted from other $x$ variables, we want $R^2$ to be rather low. A high $R^2$ (i.e. one very close to 1) will mean that **tolerance** gets closer to 0, which would then mean that our VIF becomes very large, such that:\n",
    "\n",
    "\\\\[\\frac{\\text{1}}{\\text{a small tolerance number close to 0}} = \\text{a very large number} \\\\]\n",
    "\n",
    "Typically if our VIF for a particular variable is over 5 (some argue 10), then the prescence of the variable is causing multicolinearity. Statsmodels suggests in its documentation to go for 5 as the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf95cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAG+CAYAAACZGmL+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2UklEQVR4nO3deZgdZZn38W+TBJFN2SPKEEC5UQR1AgIBAREHBERlSSQQiMAQEF9xBBlAQGZkUUQdFJQ1rOKAEUEURRTDGoxERFC5IzCisoRFw2LYQvr9o6qTTpOk08vp6ur6fq7rXF2nTp2q+3QOze8851na2tvbkSRJkppmmaoLkCRJkqpgEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjDa+6AElDX0T8EvhZZp7WZf+RwHaZuXtE/BnYKzPvioipwLrAM52Pz8x3d3n+MOBq4O3ANzLzrF7Wt8jrASdn5pRenG8ixWvZrZvjtgd+AiTQDrQBc4H/yszrenrdLue+D/gUMBOYkpljlnDsesAZmblnH653EfByZk7qsn9P4MTMfNdSnmd3YMfM/PQSjhkF3JeZKy7isZOA1TPzUz0oX1JDGYQlDYSzgVOB07rs/3dgcYHnc0sRQt8M7ASskJmv9q3EpbpeKzzYOeBHxLuA2yNivcx8sq8nz8xHgcWG4NK6QPTxUmcDv4iIz2TmC532H1I+tlQy84fAD/tYiyQtFYOwpIFwDXBmRLwvM28FiIjtKFpAb+zNCSNiJeCnwAhgRtnyuDbwFWB54GXg+Mz8adlCexCwAvBMZr6/h9c6EJgELAusCnwpM79dPnYscABFS+6fgInl094UET8G/qV8bHxm/rG7a2XmPRExB1g3Ig4HtgLeBPwuM/eLiM8De1J0bfsz8MnMfDQi3gFMLl/7/eVrXaj1NCKGA6cDu5U13QEcDlwAvDkibsjMnSLio8AXgGHAs8BnM3N62dq6UD2d6r4rIhLYC7is07U3A/ZY3O+w678NcAlla3pEbFnW+7rymjdm5kHlJZeJiAuA0cArwKcz884u/25vBs4q/w1GAP+bmad2928gqTnsIyyp5TJzLnAeReDpcAjwrcxc3PKWX4mI33a67dLlnM8BuwAvlC2qs4EpwBGZuSlFOL28/NofYGNg+yWE4K7X+21ErBYRK1K0XO+Sme8BxlGEs46v8ScCW2XmO4H/o+iOALB+WcsmwC3AUd39nspz7gHMA/5Q7loX+NcyBO8PbAK8t3zN11OEWIDvAOeXr/3M8nldfZIiOL4LeCewEjAWOJiiZXqniNgIOAfYszzXicC1EbFy13oWcf6zWfjf+N+BSyk+8Czyd1ha3L/NERTdKrYA3gHsHhGjy8deTxGM3wOcAFwVEct2ef5lwOTMHA28F9gxIsYuom5JDWWLsKSBch7wh7IldwRFl4ZPLuH4nnZV2AJ4IDN/BZCZv4+I24HtKfrf/i4zn+3N9SJiN2DXiHgb8G6go2/qjsD3MvMf5TU/Wx4/EZiemQ+Ux/0W2GMx190gIn5bbo8A/gp8JDPnRATAneUHCShact8L3FU+NgxYPiJWAzalCJ1k5u1lH+GudgQu69R1YVxZ7/adjtkB+EVmPlSe66aIeIIiQHetp6srgTMiYgPgYYoPCdtn5vNL+B3C4v9tDgB2iYjjgI0oWrtXBJ4GZmfmlWWNN0REW3kM5WtaAdgOWDUivljuXrG89lWLqV9SwxiEJQ2IzHwsIm4EPk7xNfiUzOw6OK0vFvUN1zIU4fJl4PnenDQi3gJMowjyt1G0OncMgptLEbI7jn0j8Mby7iudTtMxEG5RFuojvAid6x4GfLlTt4zXAat0qqHzNRYVVrvWuxav/b0t6ffYtZ6FZOaL5aC5A4FfA/dm5p+6+R0u6Zy3AvdQdIG5iuLDTsdr7NonvI2Ff+fDyn1jMnMOQESsDry4uPolNY9dIyQNpG8B+1K09C31AKqldCcQEfFeio2NgW2BqX0872bAkxQzSNxAGeDKGSt+TtH/taPbwEnAZ/t4vSW5ATi40/X+m6KF9+/ADIouDkTEv1J0oejq58D4iHhdRCwDfBvYhyIgdwTdm4B/i4j1y3PtAKwD/GopazyHoqV5Igv+jZf0O1ykiFilfN5/ZubVFAMj30oRcAFWK1uZiYgPUwTcP3U8v2xhvpPy36P8kHI78JGlfB2SGsAWYUkDJjOnll/j/z0z7+3ncz8VEXsD34yI5Sn62X4iM2dGRHezJkDRR/j4LvuuBs6gaOHMiPgnMJ0i1L01M68vB6ndXnZV+D1FX9heT0PWjQsoAuGdEdEO/IUFg/P2AS6KiMOAB4BFDcw7FxhFEZrbKD4kfIOir/CrETGdotX1k8DV5eC6OcCHM/OZ8jUuUWY+VA6a2wT4cbn7Zyzmd7iE8/wjIk4DfhMRTwNPUQTZtwIPAk8Ae0bEyWWNe2bm3C41jgfOioh7KQbpfTczv9Pti5DUGG3t7YsbpyJJkiQNXXaNkCRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiNVMn3ajBkznKpCkiRJA2L06NGLXNSosnmER48e3f1BkiRJUh/MmDFjsY/ZNUKSJEmNZBCWJElSIxmEJUmS1EgGYUmSJDWSQViSJEmNZBCWJElSIxmEJUmS1EgtmUc4IiYCE8u7ywHvBkZm5uxWXE+SJEnqqZYE4cy8GLgYICLOBiYbgiVJkjSYtHRluYjYDNg4Mw9v5XUGwoQJE9hmm22YNGnSQvsnT57M9OnTOeecc9hhhx0488wz2WSTTZgwYQKPPPIIK6200kLHX3vttfO3n332WSZMmADAnDlzmDVrFuuttx4AY8aM4W1vexs33HAD5557bkteU0Qwbdo0Vl111aV+zoQJE9h3333ZeeedF9p/7733csQRR3DTTTf1up7nnnuOMWPGsP7668/fd+yxx7Llllv2+pySJEmL0+ollo8D/qvF1xgQ++67L1//+tdfE4Svuuoqjj/++EU+5+ijj35NYOxs5ZVXnh+Mf/WrX/HFL35xoaB89dVX90Pl9fHb3/6WzTffnMmTJ1ddiiRJaoCWBeGIeCMQmfnLVl1jIO24446ccsop3HXXXWy22WYATJ8+nfb2drbeeuuWXffJJ5/kkEMO4bHHHmPYsGF89atfZYMNNmDChAm84Q1v4KGHHmKfffbhox/9KKeccgozZ87klVdeYauttuLoo49m+PDhfOMb3+DGG29kxIgRrLLKKpx22mmsueaaAHzzm9/knnvuYfbs2Rx00EHsu+++AJx99tn8+Mc/ZtiwYay33nqccMIJrLHGGgvVdsUVV3DJJZew4oorsuGGGy6y/gceeIAjjzzyNfv3339/9txzz4X23X333cyePZt99tmHF154gbFjxzJ+/Pj++DVKkiS9RitbhLcFftHns2y/fffH7LYbHHXUguMnTixuTz0Fe+215OdOnbpUZQwfPpxx48YxZcqU+UH4yiuvZPz48bS1tS3yOaeffjrf/va359//7Gc/y3bbbbdU1+vw17/+la9//eusu+66nHzyyVx44YWceuqpQNGifP311wNFF4KNN96YL33pS7z66qscc8wxXHTRRey2225ccsklTJs2jWWXXZbJkyfzu9/9jh133BGAddZZhy984Qv84Q9/YNy4cYwdO5Yf/vCH3HrrrUyZMoXll1+eb37zmxxzzDFceOGF8+v64x//yFlnncW1117LGmuswYknnrjI+t/61rcu1Mq9JMOGDWOHHXbgsMMO46mnnmL//fdnzTXXnF+rJElSf2plEA7goRaef8CNHTuWXXfdleeff565c+dy2223cdJJJy32+O66RiyNTTfdlHXXXReAt7/97dx4443zH+sI5ABTp07l3nvvZcqUKQC8+OKLAKy11lpstNFGfOxjH2Pbbbdl2223Zauttpr/vN12223+uV9++WWef/55brnlFvbYYw+WX355oGi9Peecc3j55ZfnP2/atGlsvfXW81uJx40bx2233faa+nvSInz44Qu6kq+11lqMGzeOG2+80SAsSQ00cuQoZs16uOoyurXWWuvy+ON/rroM9VLLgnBmfqVfTrSULbaLPH711Xv+/CVYc801GTNmDNdffz1z5sxhp512es1guP42fPiCf6K2tjba29vn3+8IqgDz5s3jzDPPZIMNNgCKgXhtbW0ss8wyXH755dx7771MmzaNU089lS222GJ+v+aO83e0are3ty90jY5zz507d6F9XWsZNmzYIuvvSYvwZZddxgc+8AHWXnvt+bV0fv2SpOYoQnB7t8dVbdasRX8rrHpwQY0eGj9+PNdddx3XXHPN/P60g8E222zDxRdfTHt7Oy+//DKHHXYYl19+Offffz+77bYbG2ywAZMmTWLixIlkZrfnuvrqq5kzZw5QBNTNN9+cZZdddv4xY8aM4fbbb+fxxx8H4Ac/+EGfX8OMGTPmd7+YPXs2U6ZMYZdddunzeSVJkhbF5rYe2mKLLTj55JN5wxveQERUXc58n//85znllFP48Ic/zCuvvMKYMWM4+OCDGTFiBB/60IfYc889WX755VluueUWO8tFh7322ovHHnuMvffem3nz5rHuuutyxhlnLHRMRPC5z32OAw44gBVWWIFNN920z6/hxBNP5MQTT2TXXXdl7ty57Lvvvi0diChJkpqtrevX4ANhxowZ7aNHjx7w60qSpHoouuwN/q4R0PaaLoUaXGbMmMHo0aMX2YfFrhGSJElqJIOwJEmSGskgLEmSpEYyCEuSJKmRDMKSJElqJIOwJEmSGskgLEmSpEYa9EF45MhRtLW1tew2cuSoql+iJEmSKjDoV5Zr9VrjS7tG+IQJE9hmm22YNGnSQvsnT57M9OnTmTlzJmeeeSabbLIJEyZM4JFHHmGllVZa6Nhrr7223+qWJElS3wz6FuHBYt999+Xqq69+zf6rrrqK/fbb7zX7jz76aK699tqFbpIkSRo8DMJLaccdd2TOnDncdddd8/dNnz6d9vZ2tt566workyRJUm8YhJfS8OHDGTduHFOmTJm/78orr2T8+PHleugLO/300/nIRz4y/3bzzTcPZLmSJEnqxqDvIzyYjB07ll133ZXnn3+euXPnctttt3HSSSct8tijjz6anXfeeWALlCRJ0lIzCPfAmmuuyZgxY7j++uuZM2cOO+2002sGxEmSJKkeDMI9NH78eM466yyee+45vvzlL1ddjiRJknpp0PcRXmutdYG2lt2K8y+9LbbYgtmzZ7PiiisSEX19eZIkSapIW3t76+boXZwZM2a0jx49esCvK0mS6qEYiD7wGaXn2qgiS2npzZgxg9GjRy9y4YhB3yIsSZIktYJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNdLwVp04Io4FdgeWBb6VmRe26lqSJElST7WkRTgitgfGAFsD2wHrtOI6kiRJUm+1qkV4J+Be4AfAysDnWnQdSZIkqVda1Ud4dWAzYG/gUOA7EdHWomtJkiRJPdaqFuGngfsz82UgI+JFYA3giRZdT5IkSeqRVrUI3wbsHBFtEbE2sAJFOJYkSZIGhZYE4cz8EXA3MB24Djg8M19txbUkSZKk3mjZ9GmZeXSrzi1JkiT1lQtqSJIkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwpIkSWokg7AkSZIaySAsSZKkRjIIS5IkqZEMwlqkkSNH0dbWNuhvI0eOqvpXJUmSamp41QVocJo162GgveoyujVrVlvVJUiSpJqyRViSJEmN1LIW4Yj4DfBseff/MvMTrbqWJEmS1FMtCcIRsRzQlpnbt+L8kiRJUl+1qkX4XcDyEfGz8hrHZeadCx2x/fZLPsNuu8FRRy04duLE4vbUU7DXXt1X0PX4I4+ED38YMmHSpO6f3/X4U0+FMWPgjjvguOO6f37X4889FyLguuvgq1/t/vldj58yBVZfHS6+uLh1p+vxU6cW+884A370o+6fXzqSM9iKaezF94uXxbFsxbQlPudpVlvo+NV4mkmcV7wsDmFDZi7x+TPZcKHjn2Y1juO04mWxJ6vx9MJP6Ppe2morOK04nj33LO53fi91x/dete+9zsdPmwbfL95LHHtscX9JVltt4eOffhrOK95LHHIIzFzye48NN1z4+NVWW/i99PTTi38u+N7zvbfgeN97fX/vAVtxB6fS/XvvOE5lGmPmHz+Jc5lJsBvXcSTdv/e6Hr8XU3ia1TmAi5nIxd3X/9RTvvc6jh+M770l/P1pVR/hOcAZwE7AocB3IsKBeZIkSRo02trb+39mgIh4HbBMZr5Q3p8O7JmZfwWYMWNG++jRo/v9uuo/bW1t1GHWCGijFe9hSVK1/P+Q+suMGTMYPXr0IqeZalWL8IFQfBcREWsDKwOPtehakiRJUo+1qrvChcDFEXEbxce5AzNzbouuJUmSJPVYS4JwZr4MjG/FuSVJkqT+4IIakiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJQ9zIkaNoa2sb9LeRI0cN6O+lVUssS5IkaZCYNethoL3qMro1a1bbgF7PFmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkvrJyJGjaGtrG/S3kSNHVf2rkgaF4VUXIEnSUDFr1sNAe9VldGvWrLaqS5AGBVuEJUmS1EgtaxGOiDWBGcAHM/P+Vl1HkiRJ6o2WtAhHxAjgXOCFVpxfkiRJ6qtWdY04AzgHeLRF55ckSZL6pN+DcERMBJ7MzBv6+9ySJElSf2lFi/CBwAcjYirwbuDSiBjZgutIkiRJvdbvg+Uyc9uO7TIMH5qZj/f3dSRJkqS+cPo0SZIkNVJLF9TIzO1beX5JkiSpt2wRliRJUiMZhCVJktRIBmFJkiQ1kkFYkiRJjWQQliRJUiMZhCVJktRIBmFJkiQ10mKDcERM6bT9oYEpR5IkSRoYS2oRXq3T9udaXYgkSZI0kJa2a0RbS6uQJEmSBtiSllhui4gRFGG5Y7sNIDNfHojiJEmSpFZZUhAeBSQLWoNnlj/bgfVbWJMkSZLUcosNwpk5agDrkCRJkgbUYoNwRByyuMcy87zWlCNJkiQNjCV1jTgBuAL4Jw6WkyRJ0hCzpCB8NbAXcANwbmbeMzAlSZIkSa232OnTMvMIYCPgJuDUiLg9Ig6KiOUHrDpJkiSpRZbUIkxmvgJMAaZExNrAp4G/AKsPQG2SJElSyywxCANExHLAx4D9gZWAo1tdlCRJktRqS5o1YnvgAOD9wDXA5zLzvoEpS5IkSWqtJbUInwScBxyamS8NTDmSJEnSwFjSghrbD2AdkiRJ0oBa7KwRkiRJ0lBmEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSMNbcdKIGAacDwTQDhyamfe14lqSJElSb7SqRfjDAJm5NXA8cEqLriNJkiT1SkuCcGZeAxxS3l0XmN2K60iSJEm91ZKuEQCZOTciLgE+BuzVqutIkiRJvdHSwXKZeQCwIXB+RKzQymtJkiRJPdGSIBwREyLi2PLuHGBeeZMkSZIGhVZ1jbgauCgibgFGAJ/JzBdadC1JkiSpx1oShDPzn8DYVpxbkiRJ6g8uqCFJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJDTZy5Cja2toG/W3kyFFV/6okDUEtW1lOkjT4zZr1MNBedRndmjWrreoSJA1BtghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhrJICxJkqRGMghLkiSpkQzCkiRJaiSDsCRJkhppeH+fMCJGAJOBUcDrgJMz84f9fR1JkiSpL1rRIrwf8HRmvg/YGTirBdeQJEmS+qTfW4SB7wFTyu02YG4LriFJkiT1Sb8H4cx8HiAiVqIIxMf39zUkSZKkvmrJYLmIWAf4JXBZZl7RimtIkiRJfdGKwXJrAT8DPpWZv+jv80uSJEn9oRV9hI8DVgFOiIgTyn0fyswXWnAtSZIkqVda0Uf4COCI/j6vJEmS1J9cUEOSJEmNZBCWJElSIxmEJUmS1EgGYUmSJDWSQViSJEmNZBCWJElSIxmEJUmS1EgGYUmSJDWSQViSJEmNZBCWJElSIxmEJUmS1EgGYUmSJDWSQViSJEmNZBCWJElSIxmEJUmS1EgGYUmSJDWSQViSJEmNZBCWJElSIxmEJdXKyJGjaGtrG/S3kSNHVf2rkiR1Y3jVBUhST8ya9TDQXnUZ3Zo1q63qEiRJ3bBFWJIkSY1kEJYGgF/nS5I0+Ng1QhoAfp0vSdLgY4uwJEmSGskgLEmSpEYyCEuSJKmRDMKSJElqJIOwJEmSGskgLEmSpEYyCEuSJKmRDMKSJElqJIOwJEmSGmnIBGGXsJUkSVJPDJklll3CVpIkST0xZFqEJUmSpJ4wCEuSJKmRDMKSJElqpJYF4YjYIiKmtur8kiRJUl+0ZLBcRBwNTAD+2YrzS5IkSX3VqhbhB4E9WnRuSZIkqc9aEoQz8/vAK604tyRJktQfHCwnSZKkRjIIS5IkqZEMwpIkSWqkli2xnJl/BrZs1fklSZKkvrBFWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY00vBUnjYhlgG8B7wJeAg7OzAdacS1JkiSpN1rVIvxRYLnM3Ao4Bvhqi64jSZIk9UqrgvA2wE8BMvNOYLMWXUeSJEnqlbb29vZ+P2lEXAB8PzN/Ut7/C7B+Zs4FmDFjRv9fVJIkSVqE0aNHty1qf0v6CAPPAit1ur9MRwheUjGSJEnSQGlV14jbgV0AImJL4N4WXUeSJEnqlVa1CP8A+GBE3AG0AZ9o0XUkSZKkXmlJH+E6i4jIzKy6DkmSJLWWC2q81oVVFzCURIQzhvSziFgmItaKCPvaS5LUB7YIdxERNwB/ABKYB5CZ51VaVI1FxP8Co4DLgcszc3alBdVcROwBfA34B8WA1MMy88Zqq6qviDg4My/odP/TmfmNKmuqs3IxpTZgDPCrzHy54pJqKyImd9n1CvBX4OzM/EcFJdVaROxI0R10GeCbwAmZeUW1VdVTRLwT2B5YDXgC+EVmzqy0qD5oVR/hOruj/LlW+dNPCn2QmR+PiFWA8cD3IuIJ4PzMnFptZbV1AvDezHwiItYCrgMMwj0UEfsAuwPvj4gdyt3DgHcCBuFeiIj/Af4IrAv8KzALOKDKmmru9cCDwK3AlsDmFKHjEor3rnrmFIr/D50NbA1cBRiEeyAi3g6cAcyhmAThUWAV4LSIGA4cl5m/r7DEXjEId5GZ/xURbwJGULRsrF1xSUPBWsC/AKtTtLbvVbbE7VdtWbX0dGY+AZCZsyLi2aoLqqmfAo9RtGicW+6bRxE81DubZ+ZnIuKXmfn+iPhF1QXV3BqZuU+5fUNE/CwzT4iIWyqtqr7mUHw4m5uZj0eEjVw9Nw4Yn5nPdH2gbPD6D+DEAa+qjwzCXUTEhcBWwAoUn8gfovg0rl6IiF9R/AE6HzgxM18q999QaWH19Vz5u7sZGA0sHxGnAmTmcZVWViPlV8tTgakRsSawXPmQfxN7b1hEjAb+HBHLsvBc8uq5lSNio8y8PyI2AlaKiNWAFasurKaepfgAfF5EHE7Ruq4eyMyTOrbLblBrAE9kZnv5N7V2IRj8o78o7wI2pmglOg6YUm05tfepzPx1x52I2C4zb87Mnaosqsau6bT9SFVFDBURcTawK8VXfG0UXaHGVFpUfV0KfAs4EDidBS3t6p1PAd+JiLWBvwCHU7TInVJpVfU1FtggM/8QERsDF3T3BC1a17EqEVHrsSoG4dd6OjPbI2KFzHwqIqqup5Yi4n3AO4D/iIivlbuHUfwxf2dlhdXf6zPzHICIGAGclplHVVxTnW1Bsfz7vKoLqbvM/BZFEAb4TIWlDAmZOZ3iW5/O7qqiliEigBUiYgvg1PJm953eGVJjVQzCrzUjIo4CHi1nPHh91QXV1D+AkcDrgDeV++YBR1dW0dCwZfkh4yvAt4GfVFxP3T1A0S1iTtWF1F1E/B8LDy5+NjPfXVE5tRcR+wPHsKDbDpm5fnUV1d45FK3s/wV8nuJbC4Nw7wypsSoG4S4y87iIWBF4EfgQML3ikmopM+8D7ouI8zLzsarrGSoyc2JEXATMACZ1nvpLvfIvwMMR8UB5vz0z7RrROxuVP9soWjL3rrCWoeA/KWaH+GvVhQwRLwK/B5bNzDsj4tWqC6qxITVWxSDcRUS8GfgysCbwPYo5cGdVWVMdRcSUzNwL+E2n0bltFEHDmTh6KSKuoGghGgP8T0SskZmnVVxWne3T/SFaGh0DYUu3R4Tvy755KDMf6P4wLaV2in7s10fEWIp5mdU713Tarv1YFYPwa50HfJWiD8wtFHM2OmtED5UhmMx8U3fHqkduK/tiEhHbUnxoU+/NZeEPvr8DHq60opoqg2/Hh943US5IpF6bExE/AX5L+XutY2vbIDKOol/r9RHxfuDjVRdUY9+hmNd6/jSzmfndakvqPYPwa70+M2+KiOMzMyPixaoLqqOI+C6LWYwkM8cPcDlDyXkRcRDFogU3UQz4UO/5wbf/3N9p+x7sv95X11ddwBDzEjAmIvYCfgSsCvy92pJq6wcUIfjNFIPgHwVqG4SXqbqAwSIiNik3X4yInSjmxNySol+Reu4ciumTFnVT751DEYI/SDFP66XVllN7r8/Mmyi67CT+994Xm2fmJeXtSuCsqguqo4jYrNx8bBE39d5kinUB3gY8DlxYbTm1tnpm7gz8iqKP8HLdHD+oGYQX+F5EfAY4BPgExSpoRwGHVVlUXZVzBd9MsQzj2hThbRTO0dpXG2TmicALmXkd8IaqC6o5P/j2UUQcHhGPAQdHxKPl7TGK1iL13AfKn/t0uflVft+slpmTgVcy8w7MP33RMcvOCpn5Aov59rcu7BqxwGYUU1JdAHzCmQ76zQ+APwKbUIQMp6nqm+ERsTpARKyE/TD76hDgDPzg22uZeTZwdkQcl5l21em7r5cr802qupChplyhj4h4C8X4APXO1RFxAnBPRNwJPF91QX1hEC5l5vPAYRGxHXBbuTRwx2P2ae29tsw8NCImAwcDt1ZdUM19HridYjDSncAR1ZZTb5n5t4g4hJp/tTdInBMR+7DwABpnjui55LUtbB2rHjqPcO99GrgIeDvFirGfrLac+srMsyNixcx8PiLuAH7d7ZMGMYNwJ+WnxVOBqdj3sr/MjYjlgBUo/pD7nuuDzLwFiIhYA3gqM9spdkzKTPtf91BEXApsDTzDgrDxr5UWVV8d3/5sCryA3/70Smau1/l+RKxJsYCB8972zZ+AT2bm3RHxUYpue+qFiPgCxWJZxwH/j2LFw9rOYGQoKUXEMRRfRX0qM39cdT1DyNkUy63+jGJi+NsqrWaIyMwnu+wahwMReyMyc4Oqixgi/PanH0XE9hQDup4FVomIf8/M2i5jOwh8B/gxcDewITAW8Nve3tk9M0cDZObeEXE7NQ7CdhZfYDSwmSG43z2cmV/KzAuBd1AsC6z+11Z1ATU1PSKi6iKGCL/96V8nA+/LzPdQfGtxcsX11N2bM/MigMw8naJ7mXpnXtmPnYgYQc2zpH+oSpnpcqD9KCLeRxF8/yMivlbuXoZirfd3VlbY0FXrUbsVegb4dUQ8jysf9pXf/vSvVzPzUYDMfMQ57fusPSI2zMyZEbEBxfy36p1zgPsi4l6KpdVr2xoMBmG1zj+AkRT9iDo+ec8Djq6sIum1dgBWzUxHkPfdw5n5fYCI+B7wnorrqbtnI+L/USz0si0u/tBX/wFcGRFrUSwAcWjF9dRWZl4YET+kGLz5YGY+BRARH8nMa6utrucMwmqJzLyP4hNje2b+d9X1NIBdI3pnJrAW8EjVhdTVYr79GQYcjt/+9MV+wPEUXSL+CBxYbTm191vgwE6D5e6ptpx6K8epdB2rcgRgEJa62D4iTnHEc/+IiGEULW3Ld+wrZ5Kwpb13tgb+HBFPlfftGtFznb/9GUmxdO3T+J7sk8x8JiKmUoSNzMx/VFxS3TlYrvVq2SBjEFarrQE8GhH/R9GPtT0zXV2u96YAb6RYIhSK3+ktmVnreRwrtK2L5/TZssDHgO0oFiY6hyIc2+LWBxFxGsVywLcBB0TEtpl5ZMVl1dlCg+Ui4pdVFzQE1XKsikFYrbZb1QUMMatn5vuqLmII+V5EPEkxTdX1melKfT33FWD/zPxLRPwU2Bl4APgJ8MNKK6u3bTNza4CIOJNiAR31noPltEi1nvJCtfAqxRK21wP/Q02/OhlEHo6IdaouYqjIzG0oVuvbDrgjIk6JCFfv6plhmfm7iFgbWCEzf5OZz+Ly3301IiI6/h/dsdiLeu8zFIPlHgOupBg8p/5Vy/+/2yKsVjufYu7gW4DtKVrePlBlQXVU/vFup1gKeGxEPF0+ZJ/WvnsEeIhiLvF3AmdGxO8z85hqy6qNV8qfOwM/h/lzi65UWUVDw5XA7RFxJ7BFeV+9lJnTKWcyiYjVKRZ9mVFpUTUUEe8E5mTmQ4t4+GuL2DfoGYTVastlZsfXo9dExGcrraamMvNNABGxTmb+tWN/uSy4eikirqIIv5cD+3XM2xoRd1VaWL38vFxZah1g9/Jr57MwuPVJZn41Im6gmKf1gsz8fdU11V1EbE4xl/2/Ad+vuJzaiYiTKaacXDYizszMyzo/npnXVVNZ3xiE1WrDI2KTzLw3IjbBr/d6pfwUvjZwekR8juIrqGWALwHvrrC0ujt/McvWbjPgldRUZn65nFP0mcx8tAzC52XmD6qurc4iYjNgIsUMMbtEBJnpFGo9VK6Atg/FdH4vASsD62fmC5UWVk/vz8wxEbE8cA1wWTfH14JBWK32aWBy2X/wEeCQiuupq1Uo/pivxYIpf+YB36qsoqFhVkT8GngLxUwcB5V9XF3Fqwcy84+dth8EHqywnKHi2xQt6493d6CW6M/Ad4F9M/NPEfETQ3CvvQSQmXMiYsjkxyHzQjQ4ZebdwOZV11F3mXkrcGtE/Gtm/qbqeoaQM4GDM/OeiHg3xTLBW1dbkgTAs5l5SdVFDAH/A+wLjIqIC6jpgC61Tlt7u99Uq/91mjf4NTLTUfk9FBHTWPzv03mZeykibs7M7RZ3XxpoEfFv5eahwF0UA7raATLzZ1XVVXcRsR3FALldgAuAy8oVULWUIuIZ4PcUHybe0Wm71usD2CKsVrmOYnL9GylW9Hm42nJq7+NVFzBEzY2I3YBbgW0pv/qTKrRP+fMZigU13lbebwcMwr2UmTcDN0fEG4EJFP1b31NpUfWzadUFtIItwmqZcg7Mf6Po07oqRef6q8o5RtULEfFWYG9gBMUn8bUzc1K1VdVXRKxLMc/124E/AJ/LTD+0qXLlFF/vycwbI+JTwOWZObvismorIn5EMZ3njzLz1arrqauI2BTYC1gd+Bvwvcz8U7VV9Y0LaqhlMnNeZv40M/cH9gc+iAM/+uqK8uc2wHrAahXWUlsRsWw5mvwxiv6Do4H9yvvSYPBd4HXl9t8ppvhT7x1F0f9/RkR8OSLe1t0TtLCI2BuYDPwV+CnwHPD9iPhIpYX1kUFYLRMRy0TEThFxMXATxaIF7622qtp7PjNPA/6WmRMpZpFQzyVwf6eff+y0LQ0GK2TmjwAy8wpghYrrqbXMvD8zjwZ2pJjz+r6IuDEitqq4tDo5AtguM8/PzB9m5jcpGmU+U21ZfWMfYbVERHyLos/lVIo5Re+otqIhoz0iRgIrRcQKwIpVF1RHmblex3ZEDAPWAJ7ITJcF1mDxckR8ELiTogHBr/P7ICI+RDEv89sp+gd/hqKL2fXAuyorrF7mZuY/O+/IzGcjotbvTYOwWuVQ4GlgT2DPiGhnwehSlwTuvf8CPkbxh/whhsiE5lWJiI9RLAv6D2DliDhsMQtsSAPtYIr+69+g6L/uWIC+2Q/4dmZO7bwzIk6qpJp6WlxDQa17FzhYTi0VESMy85VO91fJzH9UWVPdRcTKwCjgocx8vuJyai0i7gZ2yswnImIt4LrMtPuOBo1yVcmX6j4gqWoRcVZmfqrT/UvL8StaShExC/hFl91tFCvOjaygpH5hi7Baovz6fmXg0oiYwIIlgS/FfsK9FhF7AsdT/Ld7VUS0Z+bJFZdVZ09n5hMAmTkrIpzRRJUqu0NcCGwAHAgcDTwZERdk5gWVFldDEXE4xd/MVSJiD4r/F7VRzIGrnhm7mP3nDGgV/cwWYbVERHyUomP9u4HflrvnAXdk5gnVVFV/EXE7sAPFiN0dgLsyc3S1VdVXRPwAWB64mWLe65EU/drJzOOqq0xNFRG3Antn5uPlwkQfpBilPzUzHdjVSxFxXGaeWnUddRcRb8zM2eWHihUp5rf+387f/NaNLcJqicy8BrgmInbJzOurrmcImZeZL5Utwe0R8c/un6IluKbT9iNVFSF18koZgtcvtx8AqPuApKpExG7l7BtPR8QhnR/LzPMqKquWygau4ykaDU4EfkKxKMlI4CvVVdY3BmG12qPlDBLLdezIzAMrrKfubo2IK4C3RMQ5wPSqC6q5a4DtWPj9eVVl1UjFzDDDgV2BGwAiYkWKby7Ucx1zrde2D+sg8v+Ancrtf2TmsRHxBooVZA3C0mJcDJxF8dWeeikiOgZ13A/8BbgbeBGwT2vf/IxiRP7s8n47YBBWlS6hmNd6BLBDOVjucorZI9RDmXlJufkdYPPM/G5EfIma92utyDKZ+XS5fTNAZj4TEXMqrKnPDMJqtccd4NEv3t5pex+KFebagDdVU86Q8UxmfqLqIqQOmXlpRFxDMVPESxHxJuATmXl3xaXV3SXAkeX29RQDEj9QXTm19PqOjcw8qdP+YQNfSv9xsJxaqvz6/s8ULZjtAJn5syprqruI+GVmvr/qOoaCiDgS+CdFqzAAmXlLdRVJhYhYCfgQC3fbubS6iuotIm7PzK073ffvaA9FxNeBBzPzrE77DgXWL1ftqyVbhNVqrwOivEERhg3CfeOn1/7zPor36Hbl/XbAIKzB4FrgURZ0K/O/+76ZXQ6Wm0YxhedzFddTR58HJkfEgRQLOq1X/qz1fMwGYbVU16+dy6/5pMFixczcseoipEVYJjP3q7qIIeQAihkPPkbxDZCDtnsoM+cAHy8XHxoF/C0zaz/bjkFYLRUR/w0cBixLMep5JrBxpUXVUER8l6JFqA3YuJw5AoDMHF9ZYfV3X0TsA/yGBV13ZlZbkgTA7yJiC4p52Dvemy9XWlENRcRbMvNvwKrAtzo9tCrwVDVV1VNEnAKckZmzgFldHlsD+GxmHltJcX1gEFar7Q68Bfg68DUW/kOkpXfOYrbVN+8CNqX4gAFFf0wXLdBgsB3w4U7324H1K6qlzj5b3s5lQfeStnJ7h6qKqqmLgIsiog34HUUYfiOwJfAqxSqIteNgObVURPwkMz8UEZdl5oSImJqZ21ddl5otIq7MzHHl9lGZeUa57QAaaQiKiM9lZm3nuh1MImJDig9qqwNPUKx6+GC1VfWeLcJqtb+VHev/GRGnUXx6lKq2ZqftXYAzym1bBjQoRMTuwOEU8wm3Aatl5qbVVlVrH4qIr2WmK/T1UWbOjIjZdJrRpM4MwmqJiNi2nIbqCGAN4HvARMD+rBps2ro/RBpwJwOTgEOBXwIfrLac2luDYqXT/6P4wNuemWMqrqmWytVid6GY1aSjm0ltf5cGYbXKNyJia+DHFH/A2yj6aEmDQftitqXB4rHMnBYRh2bmxRExseqCam4PoPNgw1WrKmQIeC/F3MHzqi6kPxiE1So3UHSmXxtIFrS6OeBDg0HHzBttXbbfUW1Z0nwvRcS2wIiI2ImiP6Z6KCJGAisDlwITKP47X4aiYea9FZZWZw9SdIuo9dLKHQzCaonM/E/gPyPihMz8YtX1SF2M7bTtjBwajA4DNqLoIvHF8qd6bkuKLnpBEX7bgHkUjTXqnXWAhyPiARZM7VfbrhHOGqGWioh/AT7OwsuE/nd1FUlSPUTEB4ANgDuBmZn5YsUl1VZE7EIxu8GciFg7Mx+tuqa6KQe8t1OsKNdZe53ns7dFWK12FfBzFiwTKknqRkScSjEH+9uBl4BjgX0qLareNge2AY4DzoyIuzLzyxXXVDf3lz+z0ir6mUFYrfZcZh5fdRGSVDPbZOa25dzWl0TEYVUXVHO7Z+ZogMzcOyJuBwzCPZCZl1RdQysYhNVq90XEx4G7cQlbSVpawyNiOaA9IoZRrNyl3psXEctm5ssRMYJiwJxkEFbLvZtiGdvOXNZSkpbsa8BdFPPf/qq8r947h6Jh5l6KQYi2BgtwsJxaJCKmUbQAd12swEnMJWkxImJyp7urUTRYtQNPZOaB1VQ1NETEGhTTdz6YmU9VXY8GB1uE1Sofr7oASaqhzYDlgcuB/8WVD/tFRGxM0Sq8CnB5RNyXmT+quCwNArYIS5I0iETEO4H9KBZ8uAW4PDMfqLaqeouIX1AsWX0+xTziP8nMzaqtSoOBncUlSRpEMvO+zDwmM3cAbgJOi4g7q66r7soPE+2Z+STwXNX1aHCwa4QkSYNMRKwE7EExd/AKFF0l1Ht/j4hJwArlTEazK65Hg4RBWJKkQSIixlKMsVgX+D5waGb+udKihoaDKBbTeIqiH/ZB1ZajwcI+wpIkDRIRMY9iBa97yl3z/ydd52VsqxIRGy7uMee0F9giLEnSYPL+qgsYYs7ttN3xoaINWA7YauDL0WBjEJYkaZDIzJurrmGIeTIzxwJExFGZeUa5/ctqy9Jg4awRkiRpqFq90/YunbbtFyrAICxJkoautsVsS4BBWJIkDV3ti9mWAPsIS5KkoWvjiLiCojW48/Y7qi1Lg4VBWJIkDVVjO22fs5htNZjzCEuSJKmR7CMsSZKkRjIIS5IkqZEMwpLUYhFxc0Ts0GXfmRFxcDfPmxgRuy/h8YsjYucu+0ZFxJ19q1iSmsHBcpLUeucD+wM3AUTEssCHgeOW9KTMvLjllUlSgzlYTpJaLCKWA2YCG2XmnIjYG9gJWAtYDngTcHxmXhMR95XHvgzcDzxOEaTPBdYpj/1hZh4fERcDqwIrUDRsHATMBf43M7eMiO2AU4BXgQeBSZn5ygC9bEka9OwaIUktlpkvAtcAHyt3fQJI4KuZ+UHgEODw8rEVgS9m5sc7nWId4M7M3Al4L3Bop8fuyMwPAF8GTu/YGRFtFAF6j8zcDngEmNi/r0yS6s2uEZI0MM4HvhIRU4FVgOuB4yPiIIoVr0Z0Oja7PPfvwOYR8X7gWeB1nR67pfx5B/CVTvvXoGg9vioiAF4P3Ngvr0SShghbhCVpAGTmvcBKwKeBycAXgUszcwLwS4rVrjrM6/L0icDszNwX+CqwfNniC0ULMcD7gPs6Pecp4G/ARzJze4ouEjf11+uRpKHAFmFJGjiTKVpt/wV4HjgjIo6lCKyrL+F5vwCuiIitgJeAPwFrl49tGRE3UbQqH0gZqDNzXkQcAfw4IpahaEnev/9fkiTVl4PlJEmS1Eh2jZAkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY1kEJYkSVIjGYQlSZLUSAZhSZIkNZJBWJIkSY30/wFFagfMkF0AZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Potential concerns regarding multicollinearity\n",
    "helper_funcs.display_vif(X_train_statsmodels, threshold=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a8974",
   "metadata": {},
   "source": [
    "### <u>Distribution of the Error Term</u>\n",
    "\n",
    "Ideally, we would like the population error $\\epsilon$ to be independent of our regressors and distributed normally (with zero mean and constant error variance). While we don't have the population errors, we can use our sample residuals to make inferences about the population. Though in our case the fact that there was heteroskedasticity implies that there is non-constant error variance present. Nevertheless, it can still be useful to test for normality\n",
    "\n",
    "Some ways of testing for normality include\n",
    "\n",
    "- Plot a density curve\n",
    "- Measure skewness\n",
    "- QQ-plot\n",
    "- **Kolmogorov-Smirnov test**\n",
    "- Jarque-Bera test\n",
    "- Shapiro-Wilk test\n",
    "- Anderson-Darling test\n",
    "- D'Agostino-Pearson test\n",
    "- Other methods\n",
    "\n",
    "<u>Kolmogorov-Smirnov Test (KS Test)</u>\n",
    "* The KS Test is a non-parametric statistical test, so it doesn't assume anything about the distribution of the data\n",
    "* The KS Test relies on what it known as a [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (CDF). \n",
    "* The purpose of the KS Test is to test for differences in the overall shape of two distributions\n",
    "* The two distributions in our case refer to a the CDF of a standard normal distribution and an Empirical CDF (ECDF)\n",
    "* An ECDF represents a CDF but for empirical data (observed data), which in turn gives us the probability of observing an $x$ value less than or equal to the one in question\n",
    "* Though because we are comparing our observed data (the model residuals) against a standard normal distribution, we need to standardise our values as well\n",
    "* The data needs to be continous in order to do this test.\n",
    "\n",
    "---\n",
    "\n",
    "<u>Hypothesis Testing</u>\n",
    "\n",
    "$$H_0: \\text{The two samples have been drawn from the same population distribution}$$\n",
    "$$H_1: \\text{The two samples have NOT been drawn from the same population distribution}$$\n",
    "\n",
    "In this case, our test statistic 'D' simply refers to the absolute value of the largest vertical distance from our ECDF to the CDF of the standard normal distribution (generic example graphed below). The p-value is then interpreted as the following: \n",
    "\n",
    "\n",
    "* If the samples have been drawn from the same population distribution (i.e. if the null hypothesis is true), what is the probability of obtaining a D statistic value at least as extreme as the one that was calculated\n",
    "* We calculate our p-value based off the Kolmogorov distribution and if our p-value is less than 0.05, there is enough evidence to reject the null hypothesis\n",
    "---\n",
    "\n",
    "This graph shows a CDF (red) vs an ECDF (blue), with the D statistic being the maximum vertical distance shown (the black arrow)\n",
    "\n",
    "\n",
    "![Random Unsplash Image](https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64120952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov Smirnov test results:\n",
      "-----------------------------------\n",
      "D-statistic: 0.19236628913486353\n",
      "p-value: 4.1086043031909534e-67\n",
      "At the 5% significance level, the data does not follow a normal distribution\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function\n",
    "helper_funcs.check_residual_normality(model, residual_data=model.resid);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851f192",
   "metadata": {},
   "source": [
    "* While the density curve seems to look normal, the QQ-plot has blue dots that significantly drift from the red line where it is supposed to be. The p-value from the Kolmogorov-Smirnov test also indicates that there is sufficient evidence at the 5% significance level to infer that the alternate hypothesis is true.\n",
    "* However, requiring the error term to be normally distributed in the population is one of the 'weaker' assumptions of regrerssion models. \n",
    "* This is because of the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) which in our regression implies that \"regardless of the population distribution of the residuals, the OLS estimators, when properly standardized, have approximate standard normal distributions. This approximation comes about by the central limit theorem because the OLS estimators involvein a complicated waythe use of sample averages. Effectively, the sequence of distributions of averages of the underlying errors is approaching normality for virtually any population distribution\" **- Introductory Econometrics: A Modern. Approach, Fifth Edition. Jeffrey M. Wooldridge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f69c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ultimately, OLS Regression requires the following assumptions to be met for our coefficient estimates to be [unbiased](https://en.wikipedia.org/wiki/Bias_of_an_estimator).\n",
    "\n",
    "1. The model is linear in the parameters\n",
    "2. A random sample has been drawn from the population\n",
    "3. There is no perfect collinearity (there are no situations where one variable is an exact linear combination of others)\n",
    "4. The error term is not correlated with any of the regressors, hence $E(\\epsilon_i | x_1, x_2, ..., x_p) = 0$ (Zero Conditional Mean)\n",
    "\n",
    "If all four of these occur, we have unbiased $\\hat{\\beta}_j$ estimates such that \n",
    "$E(\\hat{\\beta}_j) = \\beta_j$, where $E$ denotes the expectation operator. The $\\hat{\\beta}_j$ coefficients are random variables due to the fact that we can take different samples and with each sample obtain different coefficient estimates, but if our estimates are unbiased, the distribution of each of the $\\hat{\\beta}_j$ estimators will be centered at the true population parameter $\\beta_j$\n",
    "\n",
    "\n",
    "Moreover, a fifth assumption of homoskedasticity that $\\text{Var}(\\epsilon|x_1, x_2, ... x_p) = \\sigma^2$ implies that the variance of the error term does not depend on our regressors and is constant, though the prescence of heteroskedasticity does not bias the OLS estimator. These first five assumptions collectively refer to the [Gauss Markov assumptions](https://clas.ucdenver.edu/marcelo-perraillon/sites/default/files/attached-files/week_7_diagnostics_0.pdf) of OLS regression and satisfy the results of the Gauss Markov theorem. The sixth assumption that the $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ requires normality of the error term and with assumptions 1-5 round out the **classical linear model assumptions** of OLS.\n",
    "\n",
    "---\n",
    "\n",
    "The fact that there is heteroskedasticity present in our OLS model means that I can't use this particular model for inference, though it can still be used for predictive purposes. Let's get the adjusted $R^2$ score for the unseen test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cb82b",
   "metadata": {},
   "source": [
    "###  Adjusted $R^2$ of the model\n",
    "\n",
    "* $R^2$ (coefficient of determination) is a goodness-of-fit statistic that measures the proportion of the variance in the dependent variable that is 'explained' by all the covariates in the model\n",
    "* For instance an $R^2$ of 0.85 implies that there was an 85% reduction in the variance of the dependent variable when the all the independent variables were taken into account\n",
    "\n",
    "$$ R^2 = 1- \\frac{\\text{sum of squared residuals around the fit}}{\\text{sum of squared residuals around the mean of the dependent variable}}$$\n",
    "\n",
    "---\n",
    "\n",
    "* But there is a small problem with this if there is more than one predictor variable in the model\n",
    "* For example, considering a hypothetical regression model with one predictor variable and 3 observations (rows), a line in two dimensional space might not neccessarily be able to pass through all three points, but adding another variable turns the fitted object becomes a plane, which can always pass through three distinct points no matter where they are\n",
    "* A plane can always pass through three points, so the coefficient of determination would then be one, regardless of whether the added variable was useful or not.\n",
    "* Reducing the degrees of freedom in the model in this sense inflates $R^2$, often under false pretences\n",
    "* Hence it is common to use the adjusted $R^2$ metric that penalises the score if useless variables are added\n",
    "\n",
    "---\n",
    "\n",
    "#### The Adjusted $R^2$\n",
    "\n",
    "Adjusted $R^2$ is given by: \\\\[\\text{Adjusted } R^2 = 1- \\frac{{(1-R^2)(n-1)}}{{n-k-1}}  \\\\]\n",
    "\n",
    "To account for the fact that simply adding more variables into the model will artificially increase $R^2$, we take the total degrees of freedom into account in order to calculate a more accurate metric known as the adjusted $R^2$. The degrees of freedom is simply given by $n - k - 1$ (the denominator) where $n$ represents the number of observations and $k$ represents the number of regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "552603be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R^2 for the model = 0.9311863193526383\n"
     ]
    }
   ],
   "source": [
    "# Based off this adjusted R^2, it seems that the model does a solid job in predicting y\n",
    "print(f'Adjusted R^2 for the model = {helper_funcs.adjusted_r2(final_model, X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2878a25",
   "metadata": {},
   "source": [
    "### Save piepline for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa0ae35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/Ordinary Least Squares.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(final_model, './saved_models/Ordinary Least Squares.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b9e8d",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "- Year 2046\n",
    "- Infant Mortality of 3.76\n",
    "- 25% of GDP is spent on health in 2046\n",
    "- GDP per capita is 87,550\n",
    "- Employment to population ration (age 15+) is 77.49%\n",
    "- Developed Country\n",
    "- Average years of schooling is 11.57\n",
    "- 100% of the population has access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f2e777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Life Expectancy (using OLS) = 86.92288085019038\n"
     ]
    }
   ],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/Ordinary Least Squares.joblib')\n",
    "\n",
    "input_data = [2046, 3.76, 25, 87550, 77.49, 'Developed', 11.57, 100]\n",
    "print(f\"Predicted Life Expectancy (using OLS) = {helper_funcs.make_prediction(input_data, final_model, X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5db7c7",
   "metadata": {},
   "source": [
    "# **Regularised Linear Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c92fef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Ridge Regression**\n",
    "\n",
    "#### **Main Ideas**\n",
    "\n",
    "* [Ridge regression](https://machinelearningmastery.com/ridge-regression-with-python/) extends the concepts of OLS but makes some subtle adjustments through [Tikhonov regularisation](http://anderson.ece.gatech.edu/ece6254/assets/11-regression-regularisation.pdf)\n",
    "* The idea behind ridge regression is to address the concept of the bias-variance tradeoff in machine learning that suggests that optimising one tends to degrade the other\n",
    "* Ridge regression purposely introduces bias into the regression model in an effort to reduce the variance, which can then potentially lower the mean squared error of our estimator, since $$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n",
    "* Even though by the Gauss-Markov theorem, OLS has the lowest sampling variance out of any linear unbiased estimator, there may be a biased estimator that can achieve a lower mean squared error, such as the ridge estimator\n",
    "* Ridge regression is also a tool to help reduce the impact of multicollinearity within our feature matrix \n",
    "\n",
    "---\n",
    "\n",
    "#### **Algorithm Details**\n",
    "\n",
    "The loss function for OLS regression is given as:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = RSS = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{i,j})^2.$$\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta})$$\n",
    "\n",
    "---\n",
    "\n",
    "Ridge regression makes a small modification to the OLS loss function, through adding a shrinkage penalty through [L2 regularisation penalty](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261#f810), hence for ridge regression:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{i,j})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j ^2$$\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta}) + \\lambda\\vec{\\beta}^{T}\\vec{\\beta}$$\n",
    "\n",
    "<b>By convention, columns in $X$ are assumed to have zero mean and unit variance (after scaling), and the response vector $\\vec{y}$ is sometimes centered to have mean zero.<b>\n",
    "\n",
    "---\n",
    "\n",
    "The lambda parameter $\\lambda \\in [0, \\infty)$ is a constant that can be chosen through resampling methods such as cross validation. Ultimately, if $\\lambda = 0$ in the final model, the shrinkage penalty (the second term) disappears and we get OLS coefficient estimates. As $\\lambda$ gets larger, the shrinkage penalty becomes increasingly pertinent, and coefficient estimates will tend towards zero (but will not be exactly zero). Since $\\lambda$ is a hyperparameter that can be tuned, we get different coefficient estimates depending on which value for $\\lambda$ is chosen. Ultimately the shrinkage penalty aims to encourage simpler models that have smaller values for the coefficients as \"it turns out that shrinking the coefficient estimates can significantly reduce their variance\" - *An Introduction to Statistical Learning: With Applications in R*.\n",
    "    \n",
    "Also, the size constraint on the coefficients in the ridge\n",
    "regression \"alleviates the problem of large coefficients (in absolute value) and its high variance, which may be a consequence of multicollinearity.\" - *Rice University STAT 410 Lecture Slides*\n",
    "\n",
    "[Resource linked here](https://cpb-us-e1.wpmucdn.com/blogs.rice.edu/dist/e/8375/files/2017/08/Lecture16-1l5v69b.pdf) \n",
    "\n",
    "--- \n",
    "\n",
    "Expanding the terms in the loss function, we get\n",
    "\n",
    "$$J(\\vec{\\beta}) = \\vec{y}^T\\vec{y} -2\\vec{\\beta}^TX^T \\vec{y} + \\beta^TX^TX\\vec{\\beta} + \\lambda\\vec{\\beta}^{T}\\vec{\\beta}$$\n",
    "\n",
    "which is a convex function with a closed form solution when optimising coefficients. Taking the derivative of the loss function with respect to the beta vector we obtain:\n",
    "\n",
    "$$\\frac{\\partial J(\\vec{\\beta})}{\\partial \\vec{\\beta}} = -2X^{T}\\vec{y} + 2X^{T}X\\vec{\\beta} + 2\\lambda\\vec{\\beta}$$\n",
    "\n",
    "Since $J(\\vec{\\beta})$ is convex, to minimise this quantity, we can set the derivative equal to 0 to find an estimate $\\vec{b}_{ridge}$ for $\\vec{\\beta}$ thus:\n",
    "\n",
    "$$-2X^{T}\\vec{y} + 2X^{T}X\\vec{b} + 2\\lambda\\vec{b} = 0$$\n",
    "\n",
    "Moving, $-2X^{T}\\vec{y}$ to the other side, and dividing terms by two, we get \n",
    "\n",
    "$$X^{T}X\\vec{b} + \\lambda\\vec{b} = X^{T}\\vec{y}$$\n",
    "\n",
    "Factorising out a common factor of $\\vec{b}$ we get\n",
    "\n",
    "$$(X^{T}X + \\lambda I)\\vec{b} = X^{T}\\vec{y}$$\n",
    "\n",
    "\"Pre-multiplying\" both sides by $(X^{T}X + \\lambda I)^{-1}$ allows us to obtain\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "Including a positive lambda ensures that we obtain a non singular matrix for $(X^{T}X + \\lambda I)^{-1}$, even if $X^TX$ is singular (not of full rank)\n",
    "\n",
    "This optimisation problem to find $\\vec{b}_{ridge}$ could have also been solved using [Lagrange Multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier), where we would find our estimator using the Karush Kuhn-Tucker (KKT) multiplier method.\n",
    "\n",
    "$$\\text{argmin}_{||\\vec{\\beta}||_2 ^2 \\leq c}||\\vec{y} - X\\vec{\\beta}||_2 ^2$$\n",
    "\n",
    "where we optimise the beta vector subject to the constraint that $\\sum_{j=1}^p \\beta_{j}^2 \\leq c$.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Proving that $\\vec{b}_{ridge}$ is biased:**\n",
    "\n",
    "From above,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "Let $M = X^{T}X$, then:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (M + \\lambda I)^{-1}M(M^{-1}X^{T}\\vec{y})$$\n",
    "\n",
    "Factorising $M$ out in the first term and substituting the expression for $M$ into the second term, we obtain:\n",
    "\n",
    "$$\\vec{b}_{ridge} = [M(I + \\lambda M^{-1})]^{-1}M[(X^TX)^{-1}X^T\\vec{y}]$$\n",
    "\n",
    "Since by matrix inverse laws, $(AB)^{-1} = B^{-1}A^{-1}$, and since $\\vec{b}_{ols} = (X^TX)^{-1}X^T\\vec{y}$:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (I + \\lambda M^{-1})^{-1}M^{-1}M\\vec{b}_{ols}$$\n",
    "\n",
    "Since $A^{-1}A$ is the identity matrix for a matrix $A$, then:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (I + \\lambda M^{-1})\\vec{b}_{ols}$$\n",
    "\n",
    "Taking the expectation of this simplified quantity, \n",
    "\n",
    "$$E(\\vec{b}_{ridge}) = E((I + \\lambda M^{-1})\\vec{b}_{ols})$$\n",
    "\n",
    "As $(I + \\lambda M^{-1})$ is not random and as the OLS estimator under Gauss Markov assumptions is unbiased, \n",
    "\n",
    "$$E(\\vec{b}_{ridge}) = (I + \\lambda M^{-1})\\vec{\\beta}_{ols}$$\n",
    "\n",
    "Which is not equal to $\\vec{\\beta}_{ols}$ if lambda is non-zero (and positive). But if lambda was zero then it is technically not ridge regression but rather just OLS.\n",
    "\n",
    "---\n",
    "\n",
    "**Variance of the ridge estimator**\n",
    "\n",
    "The variance of the OLS estimator was shown in a previous jupyter notebook to be given as:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ols}) = \\sigma^2(X^TX)^{-1}$$\n",
    "\n",
    "The ridge estimator of $\\vec{\\beta}$ can be given as \n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "This can also be expressed as,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}X(X^{T}X)^{-1}X^T\\vec{y}$$\n",
    "\n",
    "Since $(X^{T}X)^{-1}X^T\\vec{y} = \\vec{b}_{ols}$,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}X\\vec{b}_{ols}$$\n",
    "\n",
    "Taking the variance of both sides:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\text{Var}((X^{T}X + \\lambda I)^{-1}X^{T}X\\vec{b}_{ols})$$\n",
    "\n",
    "As $\\vec{b}_{ols}$ is a random vector, \n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = (X^{T}X + \\lambda I)^{-1}X^{T}X\\text{Var}(\\vec{b}_{ols})((X^{T}X + \\lambda I)^{-1}X^{T}X)^T$$\n",
    "\n",
    "Recognising that $\\text{Var}(\\vec{b}_{ols}) = \\sigma^2(X^TX)^{-1}$ under the homoskedasticity assumption, and by applying the idea that $(AB)^T = B^TA^T$ for matrices $A$ and $B$\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = (X^{T}X + \\lambda I)^{-1}X^{T}X\\sigma^2 (X^TX)^{-1}X^{T}X(X^{T}X + \\lambda I)^{-1}$$\n",
    "\n",
    "Cancelling terms out and assuming $\\sigma^2$ is constant,\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(X^{T}X + \\lambda I)^{-1}X^{T}X(X^{T}X + \\lambda I)^{-1}$$\n",
    "    \n",
    "The variance of the ridge estimator is always lower than that of OLS. The [proof](https://www.statlect.com/fundamentals-of-statistics/ridge-regression ) is quite long so consider a case where\n",
    "$X^TX = I$\n",
    "    \n",
    "If we substitute $X^TX = I$ into the equation above, we obtain\n",
    "    \n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(I + \\lambda)^{-1}(I + \\lambda)^{-1}$$\n",
    "\n",
    "Factorising out the identity matrix\n",
    "    \n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(1 + \\lambda)^{-1}(1 + \\lambda)^{-1}I$$\n",
    "\n",
    "Simplifying, we get\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(1 + \\lambda)^{-2}I$$\n",
    "    \n",
    "Which is certainly lower than the variance of the OLS estimator. Ultimately, different values of lambda will allow us to control both the magnitiude of the variance and the coefficients\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Useful property of the ridge estimator**\n",
    "\n",
    "In cases whether the columns of $X$ are orthonormal (i.e. the columns are orthogonal and each have unit length), then this orthogonal matrix $X$ adheres to:\n",
    "$$X^TX = X^{-1}X = I.$$ \n",
    "\n",
    "More profoundly, if can also be shown that when this condition is met, the ridge estimator is a multiple of the OLS estimator such that,\n",
    "\n",
    "$$\\vec{b}_{ridge} = \\frac{1}{1 + \\lambda}\\vec{b}_{ols}$$\n",
    "    \n",
    "If we were now to take the expectation of this quantity, we'd see that ridge estimator, on average, underestimates the true coefficient since \n",
    "$$E(\\vec{b}_{ridge}) = \\frac{1}{1+\\lambda}E(\\vec{b}_{ols}) = \\frac{1}{1+\\lambda}\\beta$$\n",
    "\n",
    "\n",
    "\n",
    "Extra resource [here](https://arxiv.org/pdf/1509.09169.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855b774",
   "metadata": {},
   "source": [
    "# **Lasso Regression**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* Like ridge, lasso regression aims to address the concept of the bias-variance tradeoff in machine learning that suggests that optimising one tends to degrade the other\n",
    "* Lasso purposely introduces bias into the regression model in an effort to reduce the variance, which can then potentially lower the mean squared error of our estimator, since $$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n",
    "* Even though by the Gauss-Markov theorem, OLS has the lowest sampling variance out of any linear unbiased estimator, there may be a biased estimator that can achieve a lower mean squared error, such as the lasso estimator\n",
    "* Lasso regression is also a tool to help reduce the impact of multicollinearity within our feature matrix, just like ridge can\n",
    "* One major advantage that lasso has over ridge is that while ridge can only shrink coefficients towards zero, lasso can shrink coefficients all the way to zero through adding an $L_1$ regaularisation penalty to our ols loss function. The loss function for lasso regression is defined as:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{i,j})^2 + \\lambda\\sum_{j=1}^p |\\beta_j|.$$\n",
    "\n",
    "In matrix form this is defined as\n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta}) + \\lambda||\\beta||_1$$ \n",
    "\n",
    "Because of the mathematical properties that follow from penalising the sum of the absolute values of the $\\beta_j$ coefficients, certain coefficients can be shrunk all the way to zero, \"...thus the lasso yields models that simultaneously use regularization to improve the model and to conduct feature selection.\" - Applied Predictive Modeling (By Max Kuhn and Kjell Johnson). In this sense, lasso further encourages parsimonious models through embedded feature selection methods\n",
    "\n",
    "---\n",
    "\n",
    "Both ridge and lasso are able to lessen the impact of multicollinearity, but the way that is done is different between the two models. In ridge regression, correlated predictors tend to be close to each other in value, while for lasso, out of the predictors correlated with each other, one tends to stand out while the remaining correlated predictors' coefficient values shrink close toward zero (or exactly zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ff8c2",
   "metadata": {},
   "source": [
    "# **Elastic Net Regression**\n",
    "\n",
    "What if it was possible take advantage of what both Ridge and Lasso Regression offer?\n",
    "\n",
    "Elastic Net incorporates both the $L_2$ regularisation penalty from Ridge regression but also the $L_1$ penalty from Lasso regression such that the cost function to minimise now becomes:\n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta}) + \\lambda_1||\\beta||_1 + \\lambda_2||\\beta||^2$$\n",
    "\n",
    "* Like the loss functionsn of OLS, Ridge and Lasso regression, the loss function for elastic net regression is also convex\n",
    "* Ultimately, if $\\lambda_1 = 0 \\text{ and } \\lambda_2 \\neq 0$ then we get ridge regression. If $\\lambda_2 = 0 \\text{ and } \\lambda_1 \\neq 0$ we then get lasso regression. Lastly, if $\\lambda_1 = 0 \\text{ and } \\lambda_2 = 0$ then we just have OLS.\n",
    "* However, sklearn has a slighly more nuanced loss function that it optimises, inspired by the [glmnet](https://glmnet.stanford.edu/articles/glmnet.html) package in R, that was based off [this academic paper](https://pubmed.ncbi.nlm.nih.gov/20808728/). The loss function glmnet optimises iteratively through an optimisation algorithm known as [coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent) is given as:\n",
    "\n",
    "$$J(\\vec{\\beta}) = \\frac{1}{2N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}) + \\lambda \\big[\\frac{(1-\\alpha)}{2}||\\vec{\\beta}||^2 + \\alpha||\\vec{\\beta}||_1 \\big]$$\n",
    "\n",
    "Here the elastic net penalty is controlled by the $\\alpha$ hyperparameter. In this implementation, if $\\lambda > 0$ we get lasso regression if $\\alpha = 1$ and ridge if $\\alpha = 0$. According to Jerome Friedman, Trevor Hastie and Rob Tibshirani of Stanford university, \"the tuning parameter $\\lambda$ controls the overall strength of the penalty.\" If this hyperparameter is zero, we just get OLS.\n",
    "\n",
    "Note: The $N$ refers to the number of observations. Though Sklearn does not include the $\\frac{1}{2N}$ term, which is fine since $N$ is constant\n",
    "\n",
    "Also, Sklearn refers to $\\alpha$ in the above formula is as the \"L1 ratio\" and refers to the $\\lambda$ in the above formula as \"alpha\", which is confusing.\n",
    "\n",
    "---\n",
    "\n",
    "# **Huber Regressor**\n",
    "\n",
    "Intro:\n",
    "* The [Huber Regressor](https://towardsdatascience.com/regression-in-the-face-of-messy-outliers-try-huber-regressor-3a54ddc12516) model is designed to try address the problem of outliers that may exist in the dataset and thus falls under a family of models known as robust regression models\n",
    "* Sometimes it can be used as a good alternative to OLS as OLS tends to pull the fit towards each datapoint, hence outliers can really distort the fit, catalysing inaccurate predictions\n",
    "* Algorithm was invented by Peter Jost Huber in 1964 though subtle adjustments have been made over time\n",
    "* As such, the sklearn documentation mentions that its implementation is based off [this academic paper](https://artowen.su.domains/reports/hhu.pdf), published in 2006.\n",
    "\n",
    "In the sklearn implementation, the Huber loss function applies a transformation to the error depending on it's value, in which we intend to minimise the quantity\n",
    "\n",
    "$$J = \\sum_{i=1}^n \\big(\\sigma + H_\\epsilon\\big(\\frac{y_i - \\hat{y}_i}{\\sigma} \\big) \\big) + \\alpha||\\vec{\\beta}||^2$$\n",
    "\n",
    "whereby $y_i - \\hat{y}_i$. The huber regressor finds an optimal value for $\\sigma \\in (0, \\infty)$ as well as finding the components of the $\\vec{\\beta}$ vector based on the minimisation of this loss function. The regularisation term $\\alpha||\\vec{\\beta}||^2$ acts as the $L_2$ shrinkage penalty and the function $H$ is piecewise and takes in scalar input $z$ such that\n",
    "\n",
    "$$H_\\epsilon(z) = \\begin{cases}\n",
    "z{^2} & \\text{if } |z| < \\epsilon,\\\\\n",
    "2\\epsilon|z| - \\epsilon^2  & \\text{if } |z| \\geq \\epsilon\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "According to Scikit-Learn documentation, this minimisation process \"makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.\" One other tip was to set the threshold parameter $\\epsilon$ to 1.35 \"to achieve 95% statistical efficiency\".\n",
    "\n",
    "Resources\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html\n",
    "* https://scikit-learn.org/stable/modules/linear_model.html#huber-regression\n",
    "* https://artowen.su.domains/reports/hhu.pdf\n",
    "* https://cvxr.rbind.io/cvxr_examples/cvxr_huber-regression/\n",
    "* https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
    "* https://en.wikipedia.org/wiki/Robust_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7040a1",
   "metadata": {},
   "source": [
    "### Code:\n",
    "\n",
    "* Constructs pipelines\n",
    "* Gets the relevant regression metrics like RMSE etc.\n",
    "* Saves each pipeline for further use\n",
    "* Appends the results to a pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81c052a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proprocessing and Model Building Pipeline for Ridge()\n",
      "\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('identity',\n",
      "                                                                   FunctionTransformer())]),\n",
      "                                                  ['GDP_cap']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('ohe',\n",
      "                                                                   OneHotEncoder(drop='first'))]),\n",
      "                                                  ['Status'])])),\n",
      "                ('imputation', KNNImputer()), ('ss', StandardScaler()),\n",
      "                ('model', Ridge())])\n",
      "\n",
      "Best parameters for Ridge(): {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.6744444444444445}\n",
      "\n",
      "Evaluation metrics for Ridge() model:\n",
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.935379177771174\n",
      "Mean Squared Error = 5.235320895860194\n",
      "Root Mean Squared Error = 2.288082362123399\n",
      "Mean Absolute Error = 1.7653975709524967\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proprocessing and Model Building Pipeline for Lasso()\n",
      "\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('identity',\n",
      "                                                                   FunctionTransformer())]),\n",
      "                                                  ['GDP_cap']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('ohe',\n",
      "                                                                   OneHotEncoder(drop='first'))]),\n",
      "                                                  ['Status'])])),\n",
      "                ('imputation', KNNImputer()), ('ss', StandardScaler()),\n",
      "                ('model', Lasso())])\n",
      "\n",
      "Best parameters for Lasso(): {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.01}\n",
      "\n",
      "Evaluation metrics for Lasso() model:\n",
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.9353958925569218\n",
      "Mean Squared Error = 5.2339667306843625\n",
      "Root Mean Squared Error = 2.2877864259332346\n",
      "Mean Absolute Error = 1.76660307595504\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proprocessing and Model Building Pipeline for ElasticNet()\n",
      "\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('identity',\n",
      "                                                                   FunctionTransformer())]),\n",
      "                                                  ['GDP_cap']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('ohe',\n",
      "                                                                   OneHotEncoder(drop='first'))]),\n",
      "                                                  ['Status'])])),\n",
      "                ('imputation', KNNImputer()), ('ss', StandardScaler()),\n",
      "                ('model', ElasticNet())])\n",
      "\n",
      "Best parameters for ElasticNet(): {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.01, 'model__l1_ratio': 0.0001}\n",
      "\n",
      "Evaluation metrics for ElasticNet() model:\n",
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.9352099713288163\n",
      "Mean Squared Error = 5.249029325942554\n",
      "Root Mean Squared Error = 2.2910760192412982\n",
      "Mean Absolute Error = 1.7685807788166499\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proprocessing and Model Building Pipeline for HuberRegressor()\n",
      "\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('identity',\n",
      "                                                                   FunctionTransformer())]),\n",
      "                                                  ['GDP_cap']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('ohe',\n",
      "                                                                   OneHotEncoder(drop='first'))]),\n",
      "                                                  ['Status'])])),\n",
      "                ('imputation', KNNImputer()), ('ss', StandardScaler()),\n",
      "                ('model', HuberRegressor())])\n",
      "\n",
      "Best parameters for HuberRegressor(): {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 3.0}\n",
      "\n",
      "Evaluation metrics for HuberRegressor() model:\n",
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.935238883315823\n",
      "Mean Squared Error = 5.246686992241186\n",
      "Root Mean Squared Error = 2.290564775823025\n",
      "Mean Absolute Error = 1.7442736614521617\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regularised_linear_models = [Ridge(), Lasso(), ElasticNet(), HuberRegressor()]\n",
    "results = []\n",
    "for model in regularised_linear_models:\n",
    "    \n",
    "    # Create and show pipeline\n",
    "    model_pipeline = data_prep.create_pipeline(model)\n",
    "    print(f\"Proprocessing and Model Building Pipeline for {model}\\n\")\n",
    "    print(model_pipeline)\n",
    "    \n",
    "    # Create parameter grid used in every model\n",
    "    param_grid = {\n",
    "        'imputation__n_neighbors': np.arange(3, 16, 2), \n",
    "        'imputation__weights': ['uniform', 'distance'],\n",
    "        'model__alpha': np.linspace(0.01, 3, 10)\n",
    "    }\n",
    "    \n",
    "    # In the case of ElasticNet there is also an l1_ratio to consider\n",
    "    if isinstance(model, ElasticNet) and not isinstance(model, Lasso):\n",
    "        param_grid.update({'model__l1_ratio': np.linspace(0.0001, 10, 10)})\n",
    "        \n",
    "    # Get the best hyperparameters for each model and use that in the final model\n",
    "    best_estimator, best_params = data_prep.exhaustive_search(X_train, \n",
    "                                                              y_train, \n",
    "                                                              model_pipeline, \n",
    "                                                              param_grid, \n",
    "                                                              cv=5, \n",
    "                                                              scoring='neg_mean_squared_error')\n",
    "    \n",
    "    final_model = best_estimator.fit(X_train, y_train)\n",
    "    print(f\"\\nBest parameters for {model}: {best_params}\\n\")\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    print(f\"Evaluation metrics for {model} model:\")\n",
    "    r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, final_model.predict(X_test))\n",
    "    \n",
    "    # Save final model, using the name of each model\n",
    "    joblib.dump(final_model, f'./saved_models/{str(model)[:-2]} Regression.joblib')\n",
    "    \n",
    "    results.append((str(model)[:-2], r2, mse, rmse, mae, best_params))\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb76db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>r2</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.93538</td>\n",
       "      <td>5.23532</td>\n",
       "      <td>2.28808</td>\n",
       "      <td>1.76540</td>\n",
       "      <td>{'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.6744444444444445}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.93540</td>\n",
       "      <td>5.23397</td>\n",
       "      <td>2.28779</td>\n",
       "      <td>1.76660</td>\n",
       "      <td>{'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.93521</td>\n",
       "      <td>5.24903</td>\n",
       "      <td>2.29108</td>\n",
       "      <td>1.76858</td>\n",
       "      <td>{'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.01, 'model__l1_ratio': 0.0001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>0.93524</td>\n",
       "      <td>5.24669</td>\n",
       "      <td>2.29056</td>\n",
       "      <td>1.74427</td>\n",
       "      <td>{'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 3.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model      r2     mse    rmse     mae  \\\n",
       "0           Ridge 0.93538 5.23532 2.28808 1.76540   \n",
       "1           Lasso 0.93540 5.23397 2.28779 1.76660   \n",
       "2      ElasticNet 0.93521 5.24903 2.29108 1.76858   \n",
       "3  HuberRegressor 0.93524 5.24669 2.29056 1.74427   \n",
       "\n",
       "                                                                                                           best_params  \n",
       "0               {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.6744444444444445}  \n",
       "1                             {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.01}  \n",
       "2  {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 0.01, 'model__l1_ratio': 0.0001}  \n",
       "3                              {'imputation__n_neighbors': 15, 'imputation__weights': 'distance', 'model__alpha': 3.0}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In table format\n",
    "results_table = pd.DataFrame(results, columns=['Model', 'r2', 'mse', 'rmse', 'mae', 'best_params'])\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b633b",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "Lasso Regression returned the lowest RMSE on the test set. Make a prediction using the lasso pipeline\n",
    "\n",
    "- Born in 2038\n",
    "- Infant Mortality of 2.91\n",
    "- 32.56% of GDP is spent on health in 2038\n",
    "- GDP per capita is 89,570\n",
    "- Employment to population ration (age 15+) is 64.49%\n",
    "- Developed Country\n",
    "- Average years of schooling is 11.79\n",
    "- 100% of the population has access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca7f6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted life expectancy (using Lasso regression) = 90.74998745729681\n"
     ]
    }
   ],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/Lasso Regression.joblib')\n",
    "input_data = [2038, 2.91, 32.56, 89570, 64.49, 'Developed', 11.79, 100]\n",
    "\n",
    "print(f\"Predicted life expectancy (using Lasso regression) = {helper_funcs.make_prediction(input_data, saved_pipeline, X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
