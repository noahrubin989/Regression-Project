{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca16b165",
   "metadata": {},
   "source": [
    "# **Lasso Regression in Python Notes**\n",
    "\n",
    "By Noah Rubin\n",
    "\n",
    "May 2021\n",
    "\n",
    "---\n",
    "\n",
    "* Like ridge, lasso regression aims to address the concept of the bias-variance tradeoff in machine learning that suggests that optimising one tends to degrade the other\n",
    "* Lasso purposely introduces bias into the regression model in an effort to reduce the variance, which can then potentially lower the mean squared error of our estimator, since $$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n",
    "* Even though by the Gauss-Markov theorem, OLS has the lowest sampling variance out of any linear unbiased estimator, there may be a biased estimator that can achieve a lower mean squared error, such as the lasso estimator\n",
    "* In essense, lasso regression can be used because OLS might fit the training data well, but may not generalise as nicely to out of sample data\n",
    "* Lasso regression is also a tool to help reduce the impact of multicollinearity within our feature matrix, just like ridge can\n",
    "* One major advantage that lasso has over ridge is that while ridge can only shrink coefficients towards zero, lasso can shrink coefficients all the way to zero through adding an L1 regaularisation penalty to our ols loss function. The loss function for lasso regression is defined as:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{i,j})^2 + \\lambda\\sum_{j=1}^p |\\beta_j|.$$\n",
    "\n",
    "In matrix form this is defined as\n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta}) + \\lambda||\\beta||_1$$ \n",
    "\n",
    "Because of the mathematical properties that follow from penalising the sum of the absolute values of the $\\beta_j$ coefficients, certain coefficients can be shrunk all the way to zero, \"...thus the lasso yields models that simultaneously use regularization to improve the model and to conduct feature selection.\" - Applied Predictive Modeling (By Max Kuhn and Kjell Johnson). In this sense, lasso further encourages parsimonious models through embedded feature selection methods\n",
    "\n",
    "---\n",
    "\n",
    "Both ridge and lasso are able to lessen the impact of multicollinearity, but the way that is done is different between the two models. In ridge regression, correlated predictors tend to be close to each other in value, while for lasso, out of the predictors correlated with each other, one tends to stand out while the remaining correlated predictors' coefficient values shrink close toward zero (or exactly zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f107ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing only 2dp\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python files\n",
    "import data_prep\n",
    "import helper_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646b6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674c5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c350087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('numeric',\n",
       "                                                  Pipeline(steps=[('identity',\n",
       "                                                                   FunctionTransformer())]),\n",
       "                                                  ['GDP_cap']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['Status'])])),\n",
       "                ('imputation', KNNImputer()), ('ss', StandardScaler()),\n",
       "                ('model', Lasso())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = data_prep.create_pipeline(Lasso())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5b65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'imputation__n_neighbors': 17, 'imputation__weights': 'distance', 'model__alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'imputation__n_neighbors': np.arange(3, 21, 2), \n",
    "    'imputation__weights': ['uniform', 'distance'],\n",
    "    'model__alpha': np.linspace(0.01, 3, 15)  # sklearn calls it alpha instead of lambda\n",
    "}\n",
    "\n",
    "best_estimator, best_params = data_prep.exhaustive_search(X_train, y_train, pipe, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "final_model = best_estimator.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88369c6",
   "metadata": {},
   "source": [
    "### Evaluate model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bdc36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.9354627953926437\n",
      "Mean Squared Error = 5.228546530170564\n",
      "Root Mean Squared Error = 2.286601524133701\n",
      "Mean Absolute Error = 1.7660982540681942\n"
     ]
    }
   ],
   "source": [
    "r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, final_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c62dc9",
   "metadata": {},
   "source": [
    "### Save piepline for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc6e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/Lasso Regression.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(final_model, './saved_models/Lasso Regression.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b633b",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "- Year 2050\n",
    "- Infant Mortality of 6.99\n",
    "- 32.56% of GDP is spent on health in 2050\n",
    "- GDP per capita is 18,898\n",
    "- Employment to population ration (age 15+) is 30.08%\n",
    "- Developing Country\n",
    "- Average years of schooling is 9.66\n",
    "- 85% of the population has access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7f6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted life expectancy (using lasso regression) = 83.4312345456778\n"
     ]
    }
   ],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/Lasso Regression.joblib')\n",
    "input_data = [2050, 6.99, 32.56, 18898, 30.08, 'Developing', 9.66, 85]\n",
    "\n",
    "print(f\"Predicted life expectancy (using lasso regression) = {helper_funcs.make_prediction(input_data, saved_pipeline, X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
