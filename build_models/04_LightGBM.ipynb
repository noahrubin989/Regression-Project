{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a366a27",
   "metadata": {},
   "source": [
    "# Histogram Based Gradient Boosting / LightGBM\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "Self Study - July 2021\n",
    "\n",
    "Useful Resources:\n",
    "1. https://medium.com/@mqofori/a-first-look-at-sklearns-histgradientboostingclassifier-9f5bea611c6a\n",
    "2. https://machinelearningmastery.com/histogram-based-gradient-boosting-ensembles/\n",
    "\n",
    "---\n",
    "\n",
    "Main ideas:\n",
    "\n",
    "* A random forest is an algorithm that can scale well to larger datasets as all trees are built independently in parallel, exploiting multiple CPU cores. The small downside is that it might not predict out of sample data as accurately as a gradient boosting model that builds it trees sequentially, building off the errors that all the previous trees made.\n",
    "* However, gradient boosting can quickly become computationally expensive as the sample size increases\n",
    "* With [Histogram-Based Gradient Boosting](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting), we can create a model that is \"orders of magnitude faster than GradientBoostingRegressor when the samples is larger than tens of thousands of samples\" - Sklearn documentation\n",
    "* It does this through in-built [discretisation](https://www.javatpoint.com/discretization-in-data-mining) of continous variables into a fixed number of distinct buckets, allowing us to obtain the benefits of boosting models while still remaining efficient from both a training speed and memeory usage perspective\n",
    "* Ultimately, \"these fast estimators first bin the input samples $X$ into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees.\" - [Histogram-Based Gradient Boosting Scikit-Learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting)\n",
    "---\n",
    "* Despite the process of binning occuring, and resultantly, the approximation of input values in the dataset, Histogram-Based Gradient Boosting will often closely match the performance of regular gradient boosting\n",
    "* At times it might even result in a slight improvement. In cases where it can't outperform gradient boosting, the difference is often negligable and well worth it given the speed in which this algorithm can operate when compared to regular gradient boosting.\n",
    "* Another huge advantage for Histogram Based Gradient Boosting is that it has in-built techniques to handle missing values, and so imputers like a `KNNImputer()` (seen previously) are not needed.\n",
    "* Ultimately, Histogram Based Gradient Boosting employs in-built discritisation of continous values \"which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees.\" - Sklearn documentation. \n",
    "* As per the documentation, this Sklearn implementation has been inspired by the [LightGBM](https://lightgbm.readthedocs.io/en/latest/) gradient boosting framework developed by computer scientists at Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a988595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python files\n",
    "import data_prep\n",
    "import helper_funcs\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86beab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')\n",
    "\n",
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b2eb8",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8714a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters were...\n",
      "\n",
      "model__min_samples_leaf had optimal value as: 16\n",
      "model__max_iter had optimal value as: 265\n",
      "model__max_depth had optimal value as: 7\n",
      "model__loss had optimal value as: squared_error\n",
      "model__learning_rate had optimal value as: 0.15555555555555556\n",
      "model__l2_regularization had optimal value as: 0.5555555555555556\n",
      "\n",
      "The fitted model just initialised and fit now has all these parameters set up\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = data_prep.create_pipeline(HistGradientBoostingRegressor())\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_iter': np.arange(100, 300, 5),  # Number of boosting iterations ie trees\n",
    "    'model__learning_rate': np.linspace(0.05, 1, 10),\n",
    "    'model__min_samples_leaf': np.arange(3, 30, 1),\n",
    "    'model__max_depth': np.arange(3, 8),\n",
    "    'model__l2_regularization': np.linspace(0, 1, 10),\n",
    "    'model__loss': ['squared_error', 'absolute_error'],\n",
    "}\n",
    "\n",
    "# Get the best hyperparameters for each model and use that in the final model\n",
    "final_model, best_params = data_prep.randomised_search_wrapper(X_train,\n",
    "                                                               y_train,\n",
    "                                                               model_pipeline, \n",
    "                                                               param_grid, \n",
    "                                                               cv=10,\n",
    "                                                               n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ab89b",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad43fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.9887922404061699\n",
      "Mean Squared Error = 0.9080079140680164\n",
      "Root Mean Squared Error = 0.9528944926213061\n",
      "Mean Absolute Error = 0.6768143605417666\n"
     ]
    }
   ],
   "source": [
    "r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, final_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e39dc6",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa85a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/Histogram-Based Gradient Boosting.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(final_model, './saved_models/Histogram-Based Gradient Boosting.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
