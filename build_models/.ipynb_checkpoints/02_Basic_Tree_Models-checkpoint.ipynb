{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea864480",
   "metadata": {},
   "source": [
    "# **Basic Tree Models**\n",
    "\n",
    "Decision Trees & Random Forests\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "Self Study - June 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a2702",
   "metadata": {},
   "source": [
    "## **Decision Tree Regression**\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* [Decision Tree](https://gdcoder.com/decision-tree-regressor-explained-in-depth/) regression is a type of predictive model that utilises recursive splitting to make decisions based on particular binary conditions, using mean squared error (sometimes mean absolute error, though this is less common) as a metric find the optimal splits for particular nodes\n",
    "* Because of this, decision tree regression can handle data that is non-linear in nature\n",
    "* The decision tree has a root node (at the top of the tree), internal nodes (arrows pointing too them and away from them) and leaf nodes (aka terminal nodes, which only have arrows pointing to them)\n",
    "* Features don't need to be scaled for decision trees\n",
    "\n",
    "**The Algorithm**\n",
    "\n",
    "----\n",
    "\n",
    "* The genreal idea of decision tree regression is that we want to divide all possible values for $X_1, X_2, ...X_p$ (our feature space) into a total of $J$ distinct, non-overlapping regions $R_1, R_2, ..., R_J$\n",
    "* If multiple observations fall into a single region $R_j$ that represents a leaf node, the model prediction will simply be the mean of all the training observations in $R_j$\n",
    "* These regions that divide the predictor space are comprised of high dimensional rectangles, or boxes, to allow for easy model interpretation\n",
    "* Ultimately, the goal behind decision tree regression is to find the boxes $R_1, R_2, ..., R_J$ that minimises the residual sum of squares, given by \n",
    "\n",
    "$$\\sum_{j=1}^{J}\\sum_{i \\in R_j}(y_i - \\hat{y}_{R_j})^2$$\n",
    "\n",
    "in which the $\\hat{y}_{R_j}$ term is the mean of all the observations that fall into the $j^{th}$ box\n",
    "\n",
    "* To begin the process of recursive binary splitting, sweeping through all possible predictors $X_1, X_2, ..., X_p$ in our predictor space, for each $X_j$ we aim to find a vertical line \n",
    "cutoff point $X_j = s$ such that segementing the predictor space in to two regions $\\{X|X_j < s\\}$ and $\\{X|X_j \\geq s\\}$ results in the largest possible reduction in the residual sum of squares. These all become candidates for the root node \n",
    "* The predictor and cutoff combination $X_j$ and $X_j=s$ that results in the lowest residual sum of squares is what then gets chosen as the root node\n",
    "* To determine the remaining nodes, we keep on prioritising the features and $X_j=s$ cutoff value thresholds that minimise mean squared error, given previous conditions from the nodes above it are still true\n",
    "* Once we have reached a leaf node, the output is just the average of all the data points at that particular leaf node\n",
    "* Once we have built our tree, we can run a new observation down our tree get an output value, formally represented as\n",
    "\n",
    "$$f(X) = \\sum_{m=1}^M c_m 1_{(X \\in R_m)}$$\n",
    "\n",
    "whereby we segement our feature space into regions $R_1, ...R_M$ and make a prediction based of what leaf node our observation went to. The indicator function will be zero if a data point is not in $R_M$ \n",
    "\n",
    "---\n",
    "#### <u>Hyperparameters for Decision Tree Models</u>\n",
    "\n",
    "* To find the ideal tree that aims to have low bias and low variance we can use cross validation techniques such as grid search and randomised search built into the Scikit-Learn library \n",
    "* Here we can try many hyperparameter combos and choose the one that had the best cross validated $R^2$ score. We can go for other metrics too.\n",
    "* Some hyperparameters have been listed below:\n",
    "\n",
    "**min_samples_split**\n",
    "* The `min_samples_split` hyperparameter tells our decision tree model how many samples we need (minimum) if we want to make another split to any of the internal nodes of the tree\n",
    "* For instance, if we set this parameter to 10, but a split results in 3 observations being assigned to a particular value, it then becomes a leaf node.\n",
    "\n",
    "**min_samples_leaf**\n",
    "* In tree models, the leaf node (terminal node) is the node where there are no arrows pointing away from it. No further splitting can be done\n",
    "* In other words, it is the base of the tree that makes the final decision about an observation in our data\n",
    "* So this parameter tells the model how many observations in a particular leaf node we would like to have as a minimum. \n",
    "* For example, setting this parameter to 5 would mean that we can't have a situation where only 2 observations are part of a particular leaf node\n",
    "* Though it must be noted that sometimes the rules set on `min_samples_split` and `min_samples_leaf` can't both be adhered to. \n",
    "* For instance if we set the `min_samples_split` parameter to 10, we are telling our model that we need at least 10 samples to potentially do another split\n",
    "* But if we have say 11 observations at an internal node and then set `min_samples_leaf` to 8, a (7 and 4) or a (6 and 5) split would violate the rules we placed on `min_samples_leaf`.\n",
    "* So while we had enough samples to make the split, there are insuffient amounts of observations at a leaf node\n",
    "* In these situations, the split will not be performed\n",
    "\n",
    "**max_depth**\n",
    "* The `max_depth` parameter is a hyperparameter that allows us to determine how deep we want our tree to be\n",
    "* If we create a very large and complex tree that very deep, we are likely to get excellent results on the training set\n",
    "* However, the model will have a hard time generalising on out of sample data, and will likely perform poorly on the testing set as it has learned too much of the noise that exists in the training data, without adequately capturing the signal component of the model\n",
    "* This results in a model that is overfit to the training data, which is ultimately one of the drawbacks of decision trees\n",
    "* Though it is important to also consider the case where the tree isn't deep enough and isn't given the opportunity to recognise the relationships in the data \n",
    "* This results in the model being underfit, that is, poor training accuracy and poor testing accuracy where both high bias and variance is present\n",
    "* If we want to display our tree and interpret it, we would opt for a lower `max_depth`\n",
    "\n",
    "**max_features**\n",
    "* This represents how many predictor variables we would like to take into account when making decisions on particular nodes (e.g. selecting a root node)\n",
    "* We can use the square root of the number of features, the log in base 2 or just the total number of features. Fractions can be done too\n",
    "* Randomly selecting features (ie not using all of them at once) forms a large part of bagging and random forest models.\n",
    "\n",
    "**criterion**\n",
    "* The `criterion` hyperparameter in decision tree regression models is the metric we want our regressor to use when determining our nodes\n",
    "* Popular options include mean squared error, where nodes are created based off what minimises take the average of the sum of squared residuals \n",
    "* Mean absolute error is also sometimes used, and is where we make decisions based off what minimises the average of the sum of absolute value residual distances\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e5dcc",
   "metadata": {},
   "source": [
    "## **Random Forest Regression**\n",
    "\n",
    "**Introduction**\n",
    "* Because decision trees are typically high variance estimators, they are generally not the most effective model to choose \n",
    "* What if we could build multiple trees by selecting random observations (with replacement allowed), random features (at each step of building the tree) and take the average of all of the predictions from each tree & have that be our prediction instead. That is the essense of a random forest.\n",
    "* [Random Forests](https://gdcoder.com/random-forest-regressor-explained-in-depth/) are a machine learning algorithm that can be used for both classification and regression tasks\n",
    "* Random forest is an amalgamation (ensemble) of individual decision trees, that are each created using [bootstrapping](https://www.quantstart.com/articles/bootstrap-aggregation-random-forests-and-boosted-trees/) (sampling with replacement)\n",
    "* Now because many trees are built when building a random forest regression models, it is far less likely that our model will suffer from overfitting compared to building only one decision tree\n",
    "* Other forms of ensemble learning include boosting (e.g. Gradient Boosting, xGBoost etc.) and stacking.\n",
    "\n",
    "**The Algorithm**\n",
    "* We randomly select some samples from the original dataset (replacement is allowed) and we create a bootstrapped dataset that is the same size as the original dataset. Since replacement is allowed, we might get the same observation 2+ times in the training set (which is fine)\n",
    "* Using this bootstrapped dataset we create a regression tree (using the techniques previously used for decision trees). In doing so we also use a random subset of features <u>at each step</u> of building the tree\n",
    "* We repeat this process, and build $H$ regression trees. This is a hyperparameter that can be tuned.\n",
    "* Because not every tree sees all observations and all features, we ensure that the trees are 'de-correlated' and thus our model is less prone to overfitting\n",
    "* Ultimately, the idea is that we create many regression trees, $\\hat{f}^{[1]} (x), \\hat{f}^{[2]} (x), \\hat{f}^{[3]} (x), ..., \\hat{f}^{[H]} (x)$ using a total of $H$ bootstrapped datasets, performing node splits through considering only a subset of the features at a time.\n",
    "* Eventually, once all $H$ trees have been built, we average out the predictions made from all of the models to output our final prediction in which \n",
    "\n",
    "$$\\hat{f}_{\\text{final prediction}} (x) = \\frac{1}{H}\\sum_{h = 1} ^H \\hat{f}^{[h]} (x)$$\n",
    "\n",
    "becomes our final prediction. This process of boostrapping and then aggregating is sometimes referred to as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), and the fact that we only consider a subset of the predictors at each potential node split means that we have a random forest model rather than just bagging on its own.\n",
    "* It is often the case that we allow each tree $\\hat{f}^{[h]} (x)$ to grow very deep to reduce the bias of the individual tree to capture the true relationship. While this often results in a high variance estimate, taking the average will reduce the variance since the trees are built independently\n",
    "* It is not uncommon to use bagging for 100's if not 1000's of trees, though this can come at the expense of computation speed, and the fact that error might not neccessarily go down significantly after a few 100's of trees\n",
    "* Once we have built our model, we can also ascertain which variables across all our trees played the largest role in reducing the residual sum of squares using 'feature importance' functionality from Scikit-Learn. In this sense, we can even use random forests to help with feature selection for other models\n",
    "\n",
    "\n",
    "**Hyperparameters to Tune**\n",
    "* The hyperparameters for random forest regression is the same as for decision tree regression, but now we have access to an `n_estimators` hyperparameter which is the number of trees, $H$, to build when testing out each hyperparameter combination \n",
    "\n",
    "\n",
    "**Potential Disadvantages**\n",
    "* Because the random forest algorithm allows you to create $H$ bootastrapped datasets and average out all the predictions, when $H$ gets large, one can no longer visualise all the trees and the ability to interpret our model deteriorates. Though, while this is the case, they typically perform much better on out of sample data than just a single decision tree\n",
    "* One other disadvantage, similar to decision trees, is that they can't extrapolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7224ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python files\n",
    "import data_prep\n",
    "import helper_funcs\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing only 2dp\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0355a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')\n",
    "\n",
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d7019",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "* Constructs pipelines\n",
    "* Gets the relevant regression metrics like RMSE etc.\n",
    "* Saves each pipeline for further use\n",
    "* Appends the results to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b893e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proprocessing and Model Building Pipeline for DecisionTreeRegressor()\n",
      "\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('identity',\n",
      "                                                                   FunctionTransformer())]),\n",
      "                                                  ['GDP_cap']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('ohe',\n",
      "                                                                   OneHotEncoder(drop='first'))]),\n",
      "                                                  ['Status'])])),\n",
      "                ('imputation', KNNImputer()),\n",
      "                ('model', DecisionTreeRegressor())])\n",
      "\n",
      "Best Parameters were...\n",
      "\n",
      "model__min_samples_split had optimal value as: 19\n",
      "model__min_samples_leaf had optimal value as: 6\n",
      "model__max_depth had optimal value as: 8\n",
      "model__criterion had optimal value as: squared_error\n",
      "imputation__weights had optimal value as: distance\n",
      "imputation__n_neighbors had optimal value as: 7\n",
      "\n",
      "The fitted model just initialised now has all these parameters set up\n",
      "\n",
      "Evaluation metrics for DecisionTreeRegressor() model:\n",
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.9525859399971016\n",
      "Mean Squared Error = 3.8412977509285406\n",
      "Root Mean Squared Error = 1.9599228941283737\n",
      "Mean Absolute Error = 1.4014347129041511\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "Proprocessing and Model Building Pipeline for RandomForestRegressor()\n",
      "\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('numeric',\n",
      "                                                  Pipeline(steps=[('identity',\n",
      "                                                                   FunctionTransformer())]),\n",
      "                                                  ['GDP_cap']),\n",
      "                                                 ('categorical',\n",
      "                                                  Pipeline(steps=[('ohe',\n",
      "                                                                   OneHotEncoder(drop='first'))]),\n",
      "                                                  ['Status'])])),\n",
      "                ('imputation', KNNImputer()),\n",
      "                ('model', RandomForestRegressor())])\n",
      "\n",
      "Best Parameters were...\n",
      "\n",
      "model__n_estimators had optimal value as: 32\n",
      "model__min_samples_split had optimal value as: 13\n",
      "model__min_samples_leaf had optimal value as: 3\n",
      "model__max_depth had optimal value as: 11\n",
      "model__criterion had optimal value as: absolute_error\n",
      "imputation__weights had optimal value as: uniform\n",
      "imputation__n_neighbors had optimal value as: 7\n",
      "\n",
      "The fitted model just initialised now has all these parameters set up\n",
      "\n",
      "Evaluation metrics for RandomForestRegressor() model:\n",
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.9716023790770943\n",
      "Mean Squared Error = 2.3006618158455634\n",
      "Root Mean Squared Error = 1.5167932673392124\n",
      "Mean Absolute Error = 1.0815451065328343\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_models = [DecisionTreeRegressor(), RandomForestRegressor()]\n",
    "results = []\n",
    "for model in tree_models:\n",
    "    \n",
    "    # Create and show pipeline\n",
    "    model_pipeline = data_prep.create_pipeline(model)\n",
    "    print(f\"Proprocessing and Model Building Pipeline for {model}\\n\")\n",
    "    print(model_pipeline)\n",
    "    \n",
    "    # Create parameter grid used in every model\n",
    "    param_grid = {\n",
    "        'imputation__n_neighbors': np.arange(3, 21, 2), \n",
    "        'imputation__weights': ['uniform', 'distance'],  \n",
    "        'model__min_samples_split': np.arange(10, 40, 1),\n",
    "        'model__min_samples_leaf': np.arange(3, 30, 1),\n",
    "        'model__max_depth': np.arange(3, 12, 1),\n",
    "        'model__criterion': [\"squared_error\", \"absolute_error\"]\n",
    "    }\n",
    "    \n",
    "    # In the case of ElasticNet there is also an l1_ratio to consider\n",
    "    if isinstance(model, RandomForestRegressor):\n",
    "        param_grid.update({'model__n_estimators': np.arange(10, 500, 1)})\n",
    "        \n",
    "    # Get the best hyperparameters for each model and use that in the final model\n",
    "    final_model, best_params = data_prep.randomised_search_wrapper(X_train,\n",
    "                                                                   y_train,\n",
    "                                                                   model_pipeline, \n",
    "                                                                   param_grid, \n",
    "                                                                   cv=10,\n",
    "                                                                   n_iter=25)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Evaluation metrics\n",
    "    print(f\"\\nEvaluation metrics for {model} model:\")\n",
    "    r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, final_model.predict(X_test))\n",
    "    \n",
    "    # Save final model, using the name of each model\n",
    "    joblib.dump(final_model, f'./saved_models/{str(model)[:-2]}.joblib')\n",
    "    \n",
    "    results.append((str(model)[:-2], r2, mse, rmse, mae, best_params))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b25448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>$R^2$</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.40</td>\n",
       "      <td>{'model__min_samples_split': 19, 'model__min_samples_leaf': 6, 'model__max_depth': 8, 'model__criterion': 'squared_error', 'imputation__weights': 'distance', 'imputation__n_neighbors': 7}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.08</td>\n",
       "      <td>{'model__n_estimators': 32, 'model__min_samples_split': 13, 'model__min_samples_leaf': 3, 'model__max_depth': 11, 'model__criterion': 'absolute_error', 'imputation__weights': 'uniform', 'imputation__n_neighbors': 7}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  $R^2$  mse  rmse  mae  \\\n",
       "0  DecisionTreeRegressor   0.95 3.84  1.96 1.40   \n",
       "1  RandomForestRegressor   0.97 2.30  1.52 1.08   \n",
       "\n",
       "                                                                                                                                                                                                               best_params  \n",
       "0                              {'model__min_samples_split': 19, 'model__min_samples_leaf': 6, 'model__max_depth': 8, 'model__criterion': 'squared_error', 'imputation__weights': 'distance', 'imputation__n_neighbors': 7}  \n",
       "1  {'model__n_estimators': 32, 'model__min_samples_split': 13, 'model__min_samples_leaf': 3, 'model__max_depth': 11, 'model__criterion': 'absolute_error', 'imputation__weights': 'uniform', 'imputation__n_neighbors': 7}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In table format\n",
    "results_table = pd.DataFrame(results, columns=['Model', r'$R^2$', 'mse', 'rmse', 'mae', 'best_params'])\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad761c40",
   "metadata": {},
   "source": [
    "* Random Forest regression performed better across all basic regression metrics when compared to a Decision Tree (when utilising 10 fold cross validation). Make a prediction using a random forest (all preprocessing steps included too)\n",
    "* Random Forest models don't require the Gauss-Markov assumptions or classical linear model assumptions to be adhered too since they are a non-parametric supervised learning algorithm, and this is nice since we have an interpretable model that has still performed well on out of sample data (and makes no assumptions about the underlying distribution of the data)\n",
    "* While decision trees are notorious for being a low bias and high variance model (that overfits to the training data), a random forest model generally has far lower variance on test set data, especially when re-sampling procedures like cross validation are utilised, which encourages the model to generalise better. \n",
    "* Though perhaps the biggest disadvantage of a randfom forest trees is the fact that [they can't extrapolate](http://freerangestats.info/blog/2016/12/10/extrapolation) the same way that linear models or a neural network can. [Another resource about this](https://towardsdatascience.com/a-limitation-of-random-forest-regression-db8ed7419e9f#:~:text=Unfortunately%2C%20the%20Random%20Forest%20can,doesn't%20fix%20the%20problem.) \n",
    "\n",
    "\n",
    "**Make a Prediction**\n",
    "\n",
    "* Born in 2025\n",
    "* Infant mortality of 9.53\n",
    "* 6.31% of GDP is spent on health in 2025\n",
    "* GDP per capita is 13,984\n",
    "* Employment to population ratio is 56.11%\n",
    "* Developing Country\n",
    "* Average years of schooling for people over 25 is 10.80\n",
    "* 86.32% of the population has access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffe7481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted life expectancy (using Random Forest regression) = 76.31411890250001\n"
     ]
    }
   ],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/RandomForestRegressor.joblib')\n",
    "input_data = [2025, 9.53, 6.31, 13984, 56.11, 'Developing', 10.80, 86.32]\n",
    "\n",
    "print(f\"Predicted life expectancy (using Random Forest regression) = {helper_funcs.make_prediction(input_data, saved_pipeline, X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715b6c0",
   "metadata": {},
   "source": [
    "### Get feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28dfc19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numeric',\n",
       "  Pipeline(steps=[('identity', FunctionTransformer())]),\n",
       "  ['GDP_cap']),\n",
       " ('categorical',\n",
       "  Pipeline(steps=[('ohe', OneHotEncoder(drop='first'))]),\n",
       "  ['Status']),\n",
       " ('remainder', 'passthrough', [0, 1, 2, 4, 6, 7])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_pipeline['preprocessor'].transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d8b8a",
   "metadata": {},
   "source": [
    "Based off the output above, I'm assuming that the order of the features after the pipeline was fit is:\n",
    "\n",
    "* GDP_cap (index 3): Used in `FunctionTransformer()`\n",
    "* Status (index 5): Used in `OneHotEncoder()`\n",
    "\n",
    "Then all the columns that were under `remainder='passthrough'`, in this order:\n",
    "* Year \n",
    "* InfantMortality\n",
    "* Health_exp\n",
    "* Employment\n",
    "* MeanSchooling\n",
    "* ElectricityAccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce71e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "features = X_train.columns[[3, 5, 0, 1, 2, 4, 6, 7]]\n",
    "importances = saved_pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "sns.barplot(x=features, y=importances, color='green')\n",
    "ax.set(title='Feature Importance Scores for the Random Forest Model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbacc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
