{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ec2c55",
   "metadata": {},
   "source": [
    "## <u>**Ridge Regression in Python Notes**</u>\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "May 2021\n",
    "\n",
    "#### <u>**Main Ideas**</u>\n",
    "\n",
    "* [Ridge regression](https://machinelearningmastery.com/ridge-regression-with-python/) extends the concepts of OLS but makes some subtle adjustments through [Tikhonov regularisation](http://anderson.ece.gatech.edu/ece6254/assets/11-regression-regularisation.pdf)\n",
    "* The idea behind ridge regression is to address the concept of the bias-variance tradeoff in machine learning that suggests that optimising one tends to degrade the other\n",
    "* We purposely introduce bias into the regression model in an effort to reduce the variance, which can then potentially lower the mean squared error of our estimator, since $$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n",
    "* Even though by the Gauss-Markov theorem, OLS has the lowest sampling variance out of any linear unbiased estimator, there may be a biased estimator that can achieve a lower mean squared error, such as the ridge estimator\n",
    "* Ridge regression is also a tool to help reduce the impact of multicollinearity within our feature matrix \n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:black\"><u>**Algorithm Details**</u><a name=\"Ridge\"></a></span>\n",
    "\n",
    "The loss function for OLS regression is given as:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = RSS = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{i,j})^2.$$\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta})$$\n",
    "\n",
    "---\n",
    "\n",
    "Ridge regression makes a small modification to the OLS loss function, through adding a shrinkage penalty through [L2 regularisation penalty](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261#f810), hence for ridge regression:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j X_{i,j})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j ^2$$\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta}) + \\lambda\\vec{\\beta}^{T}\\vec{\\beta}$$\n",
    "\n",
    "<b>By convention, columns in $X$ are assumed to have zero mean and unit variance (after scaling), and the response vector $\\vec{y}$ is assumed to be centered to have mean zero.<b>\n",
    "\n",
    "---\n",
    "\n",
    "The lambda parameter $\\lambda \\in [0, \\infty)$ is a constant that can be chosen through resampling methods such as cross validation. Ultimately, if $\\lambda = 0$ in the final model, the shrinkage penalty (the second term) disappears and we get OLS coefficient estimates. As $\\lambda$ gets larger, the shrinkage penalty becomes increasingly pertinent, and coefficient estimates will tend towards zero (but will not be exactly zero). Since $\\lambda$ is a hyperparameter that can be tuned, we get different coefficient estimates depending on which value for $\\lambda$ is chosen. Ultimately the shrinkage penalty aims to encourage simpler models that have smaller values for the coefficients as \"it turns out that shrinking the coefficient estimates can significantly reduce their variance\" - *An Introduction to Statistical Learning: With Applications in R*.\n",
    "    \n",
    "Also, the size constraint on the coefficients in the ridge\n",
    "regression \"alleviates the problem of large coefficients (in absolute value) and its high variance, which may be a consequence of multicollinearity.\" - *Rice University STAT 410 Lecture Slides*\n",
    "\n",
    "[Resource linked here](https://cpb-us-e1.wpmucdn.com/blogs.rice.edu/dist/e/8375/files/2017/08/Lecture16-1l5v69b.pdf) \n",
    "\n",
    "--- \n",
    "\n",
    "Expanding the terms in the loss function, we get\n",
    "\n",
    "$$J(\\vec{\\beta}) = \\vec{y}^T\\vec{y} -2\\vec{\\beta}^TX^T \\vec{y} + \\beta^TX^TX\\vec{\\beta} + \\lambda\\vec{\\beta}^{T}\\vec{\\beta}$$\n",
    "\n",
    "which is a convex function with a closed form solution when optimising coefficients. Taking the derivative of the loss function with respect to the beta vector we obtain:\n",
    "\n",
    "$$\\frac{\\partial J(\\vec{\\beta})}{\\partial \\vec{\\beta}} = -2X^{T}\\vec{y} + 2X^{T}X\\vec{\\beta} + 2\\lambda\\vec{\\beta}$$\n",
    "\n",
    "Since $J(\\vec{\\beta})$ is convex, to minimise this quantity, we can set the derivative equal to 0 to find an estimate $\\vec{b}_{ridge}$ for $\\vec{\\beta}$ thus:\n",
    "\n",
    "$$-2X^{T}\\vec{y} + 2X^{T}X\\vec{b} + 2\\lambda\\vec{b} = 0$$\n",
    "\n",
    "Moving, $-2X^{T}\\vec{y}$ to the other side, and dividing terms by two, we get \n",
    "\n",
    "$$X^{T}X\\vec{b} + \\lambda\\vec{b} = X^{T}\\vec{y}$$\n",
    "\n",
    "Factorising out a common factor of $\\vec{b}$ we get\n",
    "\n",
    "$$(X^{T}X + \\lambda I)\\vec{b} = X^{T}\\vec{y}$$\n",
    "\n",
    "\"Pre-multiplying\" both sides by $(X^{T}X + \\lambda I)^{-1}$ allows us to obtain\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "Including a positive lambda ensures that we obtain a non singular matrix for $(X^{T}X + \\lambda I)^{-1}$, even if $X^TX$ is singular (not of full rank)\n",
    "\n",
    "This optimisation problem to find $\\vec{b}_{ridge}$ could have also been solved using [Lagrange Multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier), where we would find our estimator using the Karush Kuhn-Tucker (KKT) multiplier method.\n",
    "\n",
    "$$\\text{argmin}_{||\\vec{\\beta}||_2 ^2 \\leq c}||\\vec{y} - X\\vec{\\beta}||_2 ^2$$\n",
    "\n",
    "where we optimise the beta vector subject to the constraint that $\\sum_{j=1}^p \\beta_{j}^2 \\leq c$.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Proving that $\\vec{b}_{ridge}$ is biased:**\n",
    "\n",
    "From above,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "Let $M = X^{T}X$, then:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (M + \\lambda I)^{-1}M(M^{-1}X^{T}\\vec{y})$$\n",
    "\n",
    "Factorising $M$ out in the first term and substituting the expression for $M$ into the second term, we obtain:\n",
    "\n",
    "$$\\vec{b}_{ridge} = [M(I + \\lambda M^{-1})]^{-1}M[(X^TX)^{-1}X^T\\vec{y}]$$\n",
    "\n",
    "Since by matrix inverse laws, $(AB)^{-1} = B^{-1}A^{-1}$, and since $\\vec{b}_{ols} = (X^TX)^{-1}X^T\\vec{y}$:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (I + \\lambda M^{-1})^{-1}M^{-1}M\\vec{b}_{ols}$$\n",
    "\n",
    "Since $A^{-1}A$ is the identity matrix for a matrix $A$, then:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (I + \\lambda M^{-1})\\vec{b}_{ols}$$\n",
    "\n",
    "Taking the expectation of this simplified quantity, \n",
    "\n",
    "$$E(\\vec{b}_{ridge}) = E((I + \\lambda M^{-1})\\vec{b}_{ols})$$\n",
    "\n",
    "As $(I + \\lambda M^{-1})$ is not random and as the OLS estimator under Gauss Markov assumptions is unbiased, \n",
    "\n",
    "$$E(\\vec{b}_{ridge}) = (I + \\lambda M^{-1})\\vec{\\beta}_{ols}$$\n",
    "\n",
    "Which is not equal to $\\vec{\\beta}_{ols}$ if lambda is non-zero (and positive). But if lambda was zero then it is technically not ridge regression but rather just OLS.\n",
    "\n",
    "---\n",
    "\n",
    "**Variance of the ridge estimator**\n",
    "\n",
    "The variance of the OLS estimator was shown in a previous jupyter notebook to be given as:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ols}) = \\sigma^2(X^TX)^{-1}$$\n",
    "\n",
    "The ridge estimator of $\\vec{\\beta}$ can be given as \n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "This can also be expressed as,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}X(X^{T}X)^{-1}X^T\\vec{y}$$\n",
    "\n",
    "Since $(X^{T}X)^{-1}X^T\\vec{y} = \\vec{b}_{ols}$,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}X\\vec{b}_{ols}$$\n",
    "\n",
    "Taking the variance of both sides:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\text{Var}((X^{T}X + \\lambda I)^{-1}X^{T}X\\vec{b}_{ols})$$\n",
    "\n",
    "As $\\vec{b}_{ols}$ is a random vector, \n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = (X^{T}X + \\lambda I)^{-1}X^{T}X\\text{Var}(\\vec{b}_{ols})((X^{T}X + \\lambda I)^{-1}X^{T}X)^T$$\n",
    "\n",
    "Recognising that $\\text{Var}(\\vec{b}_{ols}) = \\sigma^2(X^TX)^{-1}$ under the homoskedasticity assumption, and by applying the idea that $(AB)^T = B^TA^T$ for matrices $A$ and $B$\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = (X^{T}X + \\lambda I)^{-1}X^{T}X\\sigma^2 (X^TX)^{-1}X^{T}X(X^{T}X + \\lambda I)^{-1}$$\n",
    "\n",
    "Cancelling terms out and assuming $\\sigma^2$ is constant,\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(X^{T}X + \\lambda I)^{-1}X^{T}X(X^{T}X + \\lambda I)^{-1}$$\n",
    "    \n",
    "It is indeed true that the variance of the ridge estimator is always lower than that of OLS. The [proof](https://www.statlect.com/fundamentals-of-statistics/ridge-regression ) is quite long so consider a case where\n",
    "$X^TX = I$\n",
    "    \n",
    "If we substitute $X^TX = I$ into the equation above, we obtain\n",
    "    \n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(I + \\lambda)^{-1}(I + \\lambda)^{-1}$$\n",
    "\n",
    "Factorising out the identity matrix\n",
    "    \n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(1 + \\lambda)^{-1}(1 + \\lambda)^{-1}I$$\n",
    "\n",
    "Simplifying, we get\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(1 + \\lambda)^{-2}I$$\n",
    "    \n",
    "Which is certainly lower than the variance of the OLS estimator. Ultimately, different values of lambda will allow us to control both the magnitiude of the variance and the coefficients\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Useful property of the ridge estimator**\n",
    "\n",
    "In cases whether the columns of $X$ are orthonormal (i.e. the columns are orthogonal and each have unit length), then this orthogonal matrix $X$ adheres to:\n",
    "$$X^TX = X^{-1}X = I.$$ \n",
    "\n",
    "More profoundly, if can also be shown that when this condition is met, the ridge estimator is a multiple of the OLS estimator such that,\n",
    "\n",
    "$$\\vec{b}_{ridge} = \\frac{1}{1 + \\lambda}\\vec{b}_{ols}$$\n",
    "    \n",
    "If we were now to take the expectation of this quantity, we'd see that ridge estimator, on average, underestimates the true coefficient since \n",
    "$$E(\\vec{b}_{ridge}) = \\frac{1}{1+\\lambda}E(\\vec{b}_{ols}) = \\frac{1}{1+\\lambda}\\beta$$\n",
    "\n",
    "\n",
    "\n",
    "Extra resource [here](https://arxiv.org/pdf/1509.09169.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241e92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107ef42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c350087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b65ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5059c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c9bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82574884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971399a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "138aa93d",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/randomized-or-grid-search-with-pipeline-cheatsheet-719c72eda68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bca4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/Train_updated.csv')\n",
    "test = pd.read_csv('../data/Test_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use regular GDP per cap rather than ln(GDP_cap) \n",
    "# since idk what the go is with scaling an already logged variable\n",
    "\n",
    "X_train = train.drop(['Country', 'ln(GDP_cap)', 'Life_exp', 'HDI'], axis = 1)\n",
    "y_train = train['Life_exp']\n",
    "\n",
    "X_test = test.drop(['Country', 'ln(GDP_cap)', 'Life_exp', 'HDI'], axis = 1)\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abceaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute, scale then run ridge regression\n",
    "steps = [('knn', KNNImputer()), ('scaler', StandardScaler()), ('ridge', Ridge())]\n",
    "\n",
    "# Construct pipeline with specified steps\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea93537",
   "metadata": {},
   "source": [
    "At any point (before or after fitting), you can access any/all of the individual estimators in the pipeline with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps['knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208243e",
   "metadata": {},
   "source": [
    "3) Define your hyperparameter ranges using any of the instantiated pipeline’s param keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e45df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First access the parameter of the individual estimators\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0494d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[key for key in pipeline.get_params().keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "[val for val in pipeline.get_params().values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbecbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict()\n",
    "\n",
    "# KNN imputer parameter grid with k parameter something from 1-25\n",
    "param_grid['knn__n_neighbors'] = np.arange(1, 26)\n",
    "param_grid['knn__weights'] = ['uniform', 'distance']\n",
    "\n",
    "# Ridge params (lambda, called alpha in sklearn)\n",
    "param_grid['ridge__alpha'] = np.linspace(0, 5, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe run cell magic timeit here\n",
    "print(\"Fitting started...\")\n",
    "\n",
    "# Set random state for replicateble results\n",
    "randomised_search = RandomizedSearchCV(pipeline, \n",
    "                                       n_iter=200,\n",
    "                                       param_distributions=param_grid, \n",
    "                                       cv=10, \n",
    "                                       verbose=10, \n",
    "                                       n_jobs=-1, \n",
    "                                       random_state=3)\n",
    "\n",
    "randomised_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76adcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomised_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39829ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(randomised_search.cv_results_).loc[:, 'params':]\n",
    "scores_df.sort_values(by=['rank_test_score'], inplace=True)\n",
    "\n",
    "# Get insi\n",
    "scores_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a895a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2a12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a42161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {\n",
    "    \"reduce_dim\": [\"passthrough\", TruncatedSVD(10), TruncatedSVD(20)],\n",
    "    \"tfidf__analyzer\": [\"word\", \"char\"],\n",
    "    \"tfidf__smooth_idf\": [True, False],\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__use_idf\": [True, False],\n",
    "    \"tfidf__stop_words\": [None, STOP_WORDS],\n",
    "    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "    \"classifier__C\": [1, 10, 100, 1000],\n",
    "    \"classifier__gamma\": [0.001, 0.0001],\n",
    "    \"classifier__kernel\": [\"linear\", \"rbf\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106429b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7ba46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ec2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f142d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7a09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
