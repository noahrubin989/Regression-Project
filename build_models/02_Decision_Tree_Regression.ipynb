{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0a2702",
   "metadata": {},
   "source": [
    "## **Decision Tree Regression**\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "Self Study - June 2021\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* [Decision Tree](https://gdcoder.com/decision-tree-regressor-explained-in-depth/) regression is a type of predictive model that utilises recursive splitting to make decisions based on particular binary conditions, using mean squared error (sometimes mean absolute error, though this is less common) as a metric find the optimal splits for particular nodes\n",
    "* Because of this, decision tree regression can handle data that is non-linear in nature\n",
    "* The decision tree has a root node (at the top of the tree), internal nodes (arrows pointing too them and away from them) and leaf nodes (aka terminal nodes, which only have arrows pointing to them)\n",
    "* Features don't need to be scaled unlike previously seen algorithms such as Ridge, Lasso and ElasticNet\n",
    "\n",
    "**The Algorithm**\n",
    "\n",
    "----\n",
    "\n",
    "* The genreal idea of decision tree regression is that we want to divide all possible values for $X_1, X_2, ...X_p$ (our feature space) into a total of $J$ distinct, non-overlapping regions $R_1, R_2, ..., R_J$\n",
    "* If multiple observations fall into a single region $R_j$ that represents a leaf node, the model prediction will simply be the mean of all the training observations in $R_j$\n",
    "* These regions that divide the predictor space are comprised of high dimensional rectangles, or boxes, to allow for easy model interpretation\n",
    "* Ultimately, the goal behind decision tree regression is to find the boxes $R_1, R_2, ..., R_J$ that minimises the residual sum of squares, given by \n",
    "\n",
    "$$\\sum_{j=1}^{J}\\sum_{i \\in R_j}(y_i - \\hat{y}_{R_j})^2$$\n",
    "\n",
    "in which the $\\hat{y}_{R_j}$ term is the mean of all the observations that fall into the $j^{th}$ box\n",
    "\n",
    "* To begin the process of recursive binary splitting, sweeping through all possible predictors $X_1, X_2, ..., X_p$ in our predictor space, for each $X_j$ we aim to find a vertical line \n",
    "cutoff point $X_j = s$ such that segementing the predictor space in to two regions $\\{X|X_j < s\\}$ and $\\{X|X_j \\geq s\\}$ results in the largest possible reduction in the residual sum of squares. These all become candidates for the root node \n",
    "* The predictor and cutoff combination $X_j$ and $X_j=s$ that results in the lowest residual sum of squares is what then gets chosen as the root node\n",
    "* To determine the remaining nodes, we keep on prioritising the features and $X_j=s$ cutoff value thresholds that minimise mean squared error, given previous conditions from the nodes above it are still true\n",
    "* Once we have reached a leaf node, the output is just the average of all the data points at that particular leaf node\n",
    "* Once we have built our tree, we can run a new observation down our tree get an output value, formally represented as\n",
    "\n",
    "$$f(X) = \\sum_{m=1}^M c_m 1_{(X \\in R_m)}$$\n",
    "\n",
    "whereby we segement our feature space into regions $R_1, ...R_M$ and make a prediction based of what leaf node our observation went to. The indicator function will be zero if a data point is not in $R_M$ \n",
    "\n",
    "---\n",
    "#### <u>Hyperparameters for Decision Tree Models</u>\n",
    "\n",
    "* To find the ideal tree that aims to have low bias and low variance we can use cross validation techniques such as grid search and randomised search built into the Scikit-Learn library \n",
    "* Here we can try many hyperparameter combos and choose the one that had the best cross validated $R^2$ score. We can go for other metrics too.\n",
    "* Some hyperparameters have been listed below:\n",
    "\n",
    "**min_samples_split**\n",
    "* The `min_samples_split` hyperparameter tells our decision tree model how many samples we need (minimum) if we want to make another split to any of the internal nodes of the tree\n",
    "* For instance, if we set this parameter to 10, but a split results in 3 observations being assigned to a particular value, it then becomes a leaf node.\n",
    "\n",
    "**min_samples_leaf**\n",
    "* In tree models, the leaf node (terminal node) is the node where there are no arrows pointing away from it. No further splitting can be done\n",
    "* In other words, it is the base of the tree that makes the final decision about an observation in our data\n",
    "* So this parameter tells the model how many observations in a particular leaf node we would like to have as a minimum. \n",
    "* For example, setting this parameter to 5 would mean that we can't have a situation where only 2 observations are part of a particular leaf node\n",
    "* Though it must be noted that sometimes the rules set on `min_samples_split` and `min_samples_leaf` can't both be adhered to. \n",
    "* For instance if we set the `min_samples_split` parameter to 10, we are telling our model that we need at least 10 samples to potentially do another split\n",
    "* But if we have say 11 observations at an internal node and then set `min_samples_leaf` to 8, a (7 and 4) or a (6 and 5) split would violate the rules we placed on `min_samples_leaf`.\n",
    "* So while we had enough samples to make the split, there are insuffient amounts of observations at a leaf node\n",
    "* In these situations, the split will not be performed\n",
    "\n",
    "**max_depth**\n",
    "* The `max_depth` parameter is a hyperparameter that allows us to determine how deep we want our tree to be\n",
    "* If we create a very large and complex tree that very deep, we are likely to get excellent results on the training set\n",
    "* However, the model will have a hard time generalising on out of sample data, and will likely perform poorly on the testing set as it has learned too much of the noise that exists in the training data, without adequately capturing the signal component of the model\n",
    "* This results in a model that is overfit to the training data, which is ultimately one of the drawbacks of decision trees\n",
    "* Though it is important to also consider the case where the tree isn't deep enough and isn't given the opportunity to recognise the relationships in the data \n",
    "* This results in the model being underfit, that is, poor training accuracy and poor testing accuracy where both high bias and variance is present\n",
    "* If we want to display our tree and interpret it, we would opt for a lower `max_depth`\n",
    "\n",
    "**max_features**\n",
    "* This represents how many predictor variables we would like to take into account when making decisions on particular nodes (e.g. selecting a root node)\n",
    "* We can use the square root of the number of features, the log in base 2 or just the total number of features. Fractions can be done too\n",
    "* Randomly selecting features (ie not using all of them at once) forms a large part of random forest models, which I'll go into after first fitting a decision tree model to the data\n",
    "\n",
    "**criterion**\n",
    "* The `criterion` hyperparameter in decision tree regression models is the metric we want our regressor to use when determining our nodes\n",
    "* Popular options include mean squared error, where nodes are created based off what minimises take the average of the sum of squared residuals \n",
    "* Mean absolute error is also sometimes used, and is where we make decisions based off what minimises the average of the sum of absolute value residual distances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7224ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing only 2dp\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python files\n",
    "import data_prep\n",
    "import helper_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0355a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')\n",
    "\n",
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159c7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('numeric',\n",
       "                                                  Pipeline(steps=[('identity',\n",
       "                                                                   FunctionTransformer())]),\n",
       "                                                  ['GDP_cap']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['Status'])])),\n",
       "                ('imputation', KNNImputer()),\n",
       "                ('model', DecisionTreeRegressor())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = data_prep.create_pipeline(DecisionTreeRegressor())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f679a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the best hyperparameters for our model based off the cross validated accuracy score\n",
    "param_grid = {\n",
    "    'imputation__n_neighbors': np.arange(3, 21, 2), \n",
    "    'imputation__weights': ['uniform', 'distance'],  \n",
    "    'model__min_samples_split': np.arange(10, 40, 1),\n",
    "    'model__min_samples_leaf': np.arange(3, 30, 1),\n",
    "    'model__max_depth': np.arange(3, 12, 1),\n",
    "    'model__criterion': [\"squared_error\", \"absolute_error\"] \n",
    "}\n",
    "\n",
    "tuned_model = data_prep.randomised_search_wrapper(X_train,\n",
    "                                                  y_train,\n",
    "                                                  pipe, \n",
    "                                                  param_grid, \n",
    "                                                  cv=10,\n",
    "                                                  n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, tuned_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf6034",
   "metadata": {},
   "source": [
    "* This decision tree regression model has returned the lowest MSE out of all the models so far (OLS, Ridge, Lasso), and has outperformed all the aforementioned linear models based of $R^2$\n",
    "* Decision trees don't require the Gauss-Markov assumptions or classical linear model assumptions to be adhered too since decision tree regression is a non-parametric supervised learning algorithm, and this is nice since we have an interpretable model that has still performed well on out of sample data (and makes no assumptions about the underlying distribution of the data)\n",
    "* While decision trees are notorious for being a low bias and high variance model (that overfits to the training data), the coefficient of determination $R^2$ on the testing set was high, so our tree model was able to predict out of sample data well, made possible through cross validation (through a randomised search) which encouraged the model to generalise better.\n",
    "* Though perhaps the biggest disadvantage of the decision trees is the fact that [they can't extrapolate](http://freerangestats.info/blog/2016/12/10/extrapolation) the same way that linear models or a neural network can. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c62dc9",
   "metadata": {},
   "source": [
    "### Save piepline for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tuned_model, './saved_models/Decision Tree Regression.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715b6c0",
   "metadata": {},
   "source": [
    "### Get feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/Decision Tree Regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_pipeline['preprocessor'].transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d8b8a",
   "metadata": {},
   "source": [
    "Based off the output above, I'm assuming that the order of the features after the pipeline was fit is:\n",
    "\n",
    "* GDP_cap (index 3): Used in `FunctionTransformer()`\n",
    "* Status (index 5): Used in `OneHotEncoder()`\n",
    "\n",
    "Then all the columns that were under `remainder='passthrough'`, in this order:\n",
    "* Year \n",
    "* InfantMortality\n",
    "* Health_exp\n",
    "* Employment\n",
    "* MeanSchooling\n",
    "* ElectricityAccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce71e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "features = X_train.columns[[3, 5, 0, 1, 2, 4, 6, 7]]\n",
    "importances = saved_pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "sns.barplot(x=features, y=importances, color='green')\n",
    "ax.set(title='Feature Importance Scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b633b",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "- Born in 2036\n",
    "- Infant Mortality of 2.99\n",
    "- 40% of GDP is spent on health in 2036\n",
    "- GDP per capita is 84,991\n",
    "- Employment to population ration (age 15+) is 70.41%\n",
    "- Developed Country\n",
    "- Average years of schooling is 11.89\n",
    "- 99.99% of the population has access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/Decision Tree Regression.joblib')\n",
    "input_data = [2036, 2.99, 40, 84991, 70.41, 'Developed', 11.89, 99.99]\n",
    "\n",
    "print(f\"Predicted life expectancy (using decision tree regression) = {helper_funcs.make_prediction(input_data, saved_pipeline, X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
