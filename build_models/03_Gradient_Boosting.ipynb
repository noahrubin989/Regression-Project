{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b227591",
   "metadata": {},
   "source": [
    "# **Gradient Boosting Machines**\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "Self Study - June 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5285ba2",
   "metadata": {},
   "source": [
    "## **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca952cf1",
   "metadata": {},
   "source": [
    "#### **Intro**\n",
    "\n",
    "* While random forests remains are a low variance model when compared to decision trees, all the trees are built independently, hence they are not built off the results of the previous tree\n",
    "* Gradient boosting\n",
    "* What if we could build an ensemble of trees but have each subsequent tree build off the errors made in the one before it? \n",
    "* [Gradient Boosting Machines (GBM)](https://en.wikipedia.org/wiki/Gradient_boosting) is a non-parametric model that aims to resolve this problem using partial derivatives, hence the name \"gradient boosting\"\n",
    "* The idea is similar to algorithms like [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost#:~:text=AdaBoost%2C%20short%20for%20Adaptive%20Boosting,G%C3%B6del%20Prize%20for%20their%20work.&text=AdaBoost%20is%20adaptive%20in%20the,instances%20misclassified%20by%20previous%20classifiers.), gradient boosting leverages the power of many \"weak learners\" to create a single single strong learner (in an iterative fashion).\n",
    "* It can deal with data measured in different units but like other tree models like random forest and decision trees it can't extrapolate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28682cb",
   "metadata": {},
   "source": [
    "#### **Algorithm Details**\n",
    "\n",
    "<span style=\"color:blue\"><ins><b>Step 0: Define our dataset and a loss function that we can differentiate</b></ins><a name=\"GradientBoosting\"></a></span>\n",
    "\n",
    "So lets say we have a training dataset with $n$ observations $\\{(x_i, y_i)\\}^{n}_{i = 1}$ and a differentiable loss function $L(y_i, F(x)).$\n",
    "\n",
    "---\n",
    "Our differentiable loss function can be defined as \n",
    "$$L(y_i, F(x)) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{2}\\sum_{i=1}^{n} (y_i - F(x))^2$$ \n",
    "\n",
    "\n",
    "which ultimately follows the same framework as OLS where we aim to minimise the sum of the squared residuals In fact, the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) notes that the default parameter for the loss function to be used using gradient boosting is \"squared error\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef5836",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"><b><ins>Step 1: Initialise the model with a constant value</ins></b> $F_0(x) = \\text{argmin}_\\gamma \\sum_{i=1}^{n} L(y_i, \\gamma)$</span>.\n",
    "\n",
    "* The $L(y_i, \\gamma)$ is just our original loss function but instead of $F(x)$ we have $\\gamma$ because we need an initial constant which will be our prediction to build on. It acts as our initial educated guess.\n",
    "* To do this we find the optimal constant $\\gamma$ that minimises the sum of squared residuals (our loss function)\n",
    "\n",
    "---\n",
    "\n",
    "Now if $L(y_i, \\gamma) = \\frac{1}{2}\\sum_{i=1}^{n} (y_i - \\gamma)^2$, then it is true that $\\frac{\\partial{L(y_i, \\gamma)}}{\\partial\\gamma} = -\\sum_{i=1}^{n} (y_i - \\gamma)$   by the chain rule. Setting this quantity equal to 0 to find the stationary point we obtain the result that\n",
    "\n",
    "$$-\\sum_{i=1}^{n} (y_i - \\gamma) = 0$$\n",
    "\n",
    "Dividing by $-1$ on both sides\n",
    "\n",
    "$$\\sum_{i=1}^{n} (y_i - \\gamma) = 0$$\n",
    "\n",
    "Taking the sum of each term individually,\n",
    "\n",
    "$$\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\gamma = 0$$\n",
    "\n",
    "Recognising that $\\sum_{i=1}^{n} y_i = {n}\\bar{y}$, and that we are taking $n$ copies of gamma,\n",
    "\n",
    "$$n\\bar{y} - n\\gamma = 0$$\n",
    "\n",
    "Dividing through by $n$ and moving the gamma term to other side,\n",
    "\n",
    "$$\\gamma = \\bar{y}$$\n",
    "\n",
    "This means that the intitial predicted value, representing the single isolated leaf, is $F_0(x) = \\gamma = \\bar{y} = \\text{The mean of the Life_exp column}$\n",
    "\n",
    "This is a minimum since $L$ is convex and $\\frac{\\partial ^2 L(y_i, \\gamma)}{\\partial\\gamma^2} = n, \\text{and }n > 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45dd79",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"><ins>**Step 2: Start Building Trees**</ins></span>\n",
    "\n",
    "* Setup a for loop: for $m = 1$ to $M$. This means we want to make $M$ trees. For example when we are on the 24th tree, $m = 24$. When we are on the 67th tree, $m = 67$. If we decide to build 200 trees then we have $M = 200$ trees\n",
    "* With our $m^{th}$ tree we want to:\n",
    "\n",
    "#### <span style=\"color:red\">a) Compute $\\text{}r_{im}$</span>\n",
    "\n",
    "$$r_{im} = -\\big{[}\\frac{\\partial{L(y_i, F(x_i))}}{\\partial{F(x_i)}}\\big{]}_{F(x) = F_{(m-1)}(x)} \\forall i = 1, 2, ...n.$$\n",
    "\n",
    "\n",
    "$r_{im}$ is referring to the $i^{th}$ \"pseudo residual\" for the $m^{th}$ tree we are trying to build. Inside the square brackets is just the partial derivative of our loss function with respect to $F(x_i)$ (the predicted value) for each individual observation $i = 1, 2, ...n$ \n",
    "\n",
    "As we are looking to find the derivative of the loss function and apply it to each individual observation, our loss for each row can be expressed without the summation that was present originally. This means that for this step of the process we can set \n",
    "\n",
    "$$L(y_i, F(x_i)) = \\frac{1}{2}(y_i - F(x_i))^2$$\n",
    "\n",
    "So the partial derivative with respect to $F(x_i)$ must then be\n",
    "\n",
    "$$\\frac{\\partial{L(y_i, F(x_i))}}{\\partial{F(x_i)}} = -(y_i - F(x_i))$$\n",
    "\n",
    "If we look left of the big square brackets in our $r_{im}$ equation, this will cancel the minus we just got to give us a \"pseudo-residual\" of $(y_i - F(x_i))$. With this result, substitute in $F(x) = F_{m-1}(x)$ which makes sense because for the first tree where $m = 1$, it is taking into account our 0th tree (the single leaf) which was our initial prediction $\\gamma = \\bar{y}$. Once evaluated, the whole thing is just the pseudo-residual for row $i$ in our $m^{th}$ tree we are trying to build. It is a [pseudo residual](https://www.youtube.com/watch?v=2xudPOBz-vs) because it would have been different if we decided not to have the $\\frac{1}{2}$ out the front of the loss function. But this isn't a problem because the same process is used to minimise the sum of squared residuals and the sum of squared residuals multiplied by some constant.\n",
    "\n",
    "---\n",
    "#### <span style=\"color:red\">b) Fit a regression tree to the pseudo-residual and create terminal regions $R_{j,m}$ for $j = 1.....J_m$</span>\n",
    "\n",
    "* This is the part where we fit a regression tree, with our newly formed pseudo-residual column $r_{i1}$ acting as the dependent variable. We build a tree where instead of trying to predict life expectancy, we predict the new pseudo-residuals that were just calculated for each observation in the last step\n",
    "* As all regression trees have leaf nodes, we can say that the $j^{th}$ leaf in our $m^{th}$ tree can be expressed as $R_{j,m}.$\n",
    "* The regression tree is built using traditional methods such as minimising mean squared error or mean absolute error if we choose\n",
    "* In summary, this part is about fitting a regression tree (using mae or mse criterion) to the residuals and assigning $j$ index numbers to the leaves. We call a leaf in a tree $R_{j,m}$ which means that we are referring to the the $j^{th}$ leaf in our $m^{th}$ tree. We have a total of $J_m$ leaves in our $m^{th}$ tree\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">c) For $j = 1.....J_m$ compute $\\gamma_{jm} = \\text{argmin}_\\gamma \\sum_{x_i \\in R_{jm}}^{} L(y_i, F_{m-1}(x_i) +  \\gamma)$</span>\n",
    "\n",
    "\n",
    "* From the regression tree that was just built, what do we do if many values fall in the same leaf node? It turns out we take the average, but this can be shown\n",
    "* The output value for each leaf is the value for gamma that minimises the summation above, which is ultimately very similar to step 1.\n",
    "* However there are differences in what we are summing over. This new formula in this heading only sums over very particular values whereas the one before summed over all the $n$ samples\n",
    "* The $x_i \\in R_{j,m}$ part implies that if only row 56 got put into the first leaf of our first tree $R_{1,1}$, then only observation number 56 is used to calculate the output value for $R_{1,1}$\n",
    "* But in the case where multiple observations get sent to the same leaf, we need calculate the output value for when this happens\n",
    "* The output value for a leaf with multiple values in it is just the mean of all of the residual values that fell in that leaf\n",
    "* Also note that $F_{m-1}(x_i)$ ensures that we take the previous prediction into account. In the initial summation in step 1, there was no previous prediction as that step was about initialising a constant value to start the process for gradient boosting.\n",
    "\n",
    "Our loss function becomes $L(y_i, F_{m-1}(x_i) + \\gamma_{j, m}) = \\frac{1}{2}\\sum_{i=1}^{k} (y_i - F_{m-1}(x_i) - \\gamma)^2$. If we assume that there are a total of $k$ elements in a terminal region $R_{jm}.$ \n",
    "\n",
    "Differentiating our loss function and setting the quantity equal to zero we get\n",
    "\n",
    "$$-\\sum_{i=1}^{k}(y_i - F_{m-1}(x_i) - \\gamma) = 0$$\n",
    "\n",
    "Dividing both sides by -1,\n",
    "\n",
    "$$\\sum_{i=1}^{k}(y_i - F_{m-1}(x_i) - \\gamma) = 0$$\n",
    "\n",
    "Splitting the summation into two parts\n",
    "\n",
    "$$\\sum_{i=1}^{k}(y_i - F_{m-1}(x_i)) - \\sum_{i=1}^{k} \\gamma = 0$$\n",
    "\n",
    "Noticing that we have $k$ copies of $\\gamma$, moving this term to the other side\n",
    "\n",
    "$$\\sum_{i=1}^{k}(y_i - F_{m-1}(x_i)) = k\\gamma$$\n",
    "\n",
    "Dividing both sides by $k$ we obtain the result \n",
    "\n",
    "$$\\gamma_{j, m} = \\frac{\\sum_{i=1}^{k}(y_i - F_{m-1}(x_i))}{k}$$\n",
    "\n",
    "Which implies that the output of the $j^{th}$ leaf in our $m^{th}$ tree is the sum of all the residual values $y_i - F_{m-1}(x_i)$ in a leaf $j$, divided by the amount of values $k$ in the particular leaf.\n",
    "\n",
    "Summary\n",
    "\n",
    "* This step was about finding the output value $\\gamma_{j, m}$ for a particular leaf node for when we build our $m^{th}$ regression tree\n",
    "* The output for a leaf will be the mean of all the values that were in that leaf\n",
    "---\n",
    "\n",
    "<span style=\"color:red\">d) Update $F_m(x) = F_{m-1}(x) + \\alpha \\sum_{j=1}^{J_m} \\gamma_{j,m} I(x \\in R_{jm})$</span>\n",
    "\n",
    "\n",
    "* In this step we update our predicted valiue for life expectancy using our initial estimate, plus the results from the first tree we built\n",
    "* As this is the first time we are passing through step two, the new prediction will be called $F_1(x)$\n",
    "* So this $F_1(x)$ is given by $F_0(x)$ (from step one) plus a learning rate (which is a hyperparameter that we can tune) multiplied by the output value for a particular leaf in our $m^{th}$ regression tree\n",
    "* If we set alpha = 0.1 the small learning rate reduces the effect each tree has on the final prediction, and this improves accuracy in the long run\n",
    "* \"Empirically it has been found that using small learning rates (such as $\\alpha < 0.1$) yields dramatic improvements in models' generalisation ability over gradient boosting without shrinking ($\\alpha = 1$) However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.\" - Wikipedia \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0f4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python files\n",
    "import data_prep\n",
    "import helper_funcs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d86793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')\n",
    "\n",
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559e73e",
   "metadata": {},
   "source": [
    "# Preprocessing & Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ffd5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = data_prep.create_pipeline(GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbcb274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters were...\n",
      "\n",
      "model__n_estimators had optimal value as: 180\n",
      "model__min_samples_split had optimal value as: 29\n",
      "model__min_samples_leaf had optimal value as: 8\n",
      "model__max_depth had optimal value as: 7\n",
      "model__criterion had optimal value as: squared_error\n",
      "imputation__weights had optimal value as: uniform\n",
      "imputation__n_neighbors had optimal value as: 29\n",
      "\n",
      "The fitted model just initialised and fit now has all these parameters set up\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for knn imputer that we can try\n",
    "param_grid = {\n",
    "    'imputation__n_neighbors': np.arange(5, 36, 2), \n",
    "    'imputation__weights': ['uniform', 'distance'],\n",
    "    'model__n_estimators': np.arange(100, 300, 5),\n",
    "    'model__min_samples_split': np.arange(10, 40, 1),\n",
    "    'model__min_samples_leaf': np.arange(3, 30, 1),\n",
    "    'model__max_depth': np.arange(3, 8),\n",
    "    'model__criterion': ['squared_error', 'absolute_error']\n",
    "}\n",
    "\n",
    "# Get the best hyperparameters for each model and use that in the final model\n",
    "final_model, best_params = data_prep.randomised_search_wrapper(X_train,\n",
    "                                                               y_train,\n",
    "                                                               model_pipeline, \n",
    "                                                               param_grid, \n",
    "                                                               cv=10,\n",
    "                                                               n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610916c",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95d46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics are in terms of the unseen test set\n",
      "\n",
      "R^2 = 0.9862984406482037\n",
      "Mean Squared Error = 1.1100456092359872\n",
      "Root Mean Squared Error = 1.0535870202484403\n",
      "Mean Absolute Error = 0.7315371637506867\n"
     ]
    }
   ],
   "source": [
    "r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, final_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492d843",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f62bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/GradientBoosting.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(final_model, './saved_models/GradientBoosting.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
