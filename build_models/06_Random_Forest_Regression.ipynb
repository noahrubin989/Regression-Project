{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9b55c7",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\"><u>**Random Forest Regression**</u><a name=\"RandomForest\"></a></span>\n",
    "\n",
    "**Introduction**\n",
    "* Because decision trees are typically high variance estimators, they are generally not the most effective model to choose \n",
    "* What if we could build multiple trees by selecting random observations (with replacement allowed), random features (at each step of building the tree) and take the average of all of the predictions from each tree & have that be our prediction instead. That is the essense of a random forest.\n",
    "* [Random Forests](https://gdcoder.com/random-forest-regressor-explained-in-depth/) are a machine learning algorithm that can be used for both classification and regression tasks\n",
    "* Random forest is an amalgamation (ensemble) of individual decision trees, that are each created using [bootstrapping](https://www.quantstart.com/articles/bootstrap-aggregation-random-forests-and-boosted-trees/) (sampling with replacement)\n",
    "* Now because many trees are built when building a random forest regression models, it is far less likely that our model will suffer from overfitting compared to building only one decision tree, like I did before\n",
    "* Other forms of ensemble learning include boosting (e.g. Gradient Boosting, xGBoost etc.) and stacking.\n",
    "\n",
    "**The Algorithm**\n",
    "* We randomly select some samples from the original dataset (replacement is allowed) and we create a bootstrapped dataset that is the same size as the original dataset. Since replacement is allowed, we might get the same observation 2+ times in the training set (which is fine)\n",
    "* Using this bootstrapped dataset we create a regression tree (using the techniques previously used for decision trees). In doing so we also use a random subset of features <u>at each step</u> of building the tree\n",
    "* We repeat this process, and build $H$ regression trees. This is a hyperparameter that can be tuned.\n",
    "* Because not every tree sees all observations and all features, we ensure that the trees are 'de-correlated' and thus our model is less prone to overfitting\n",
    "* Ultimately, the idea is that we create many regression trees, $\\hat{f}^1 (x), \\hat{f}^2 (x), \\hat{f}^3 (x), ..., \\hat{f}^H (x)$ using a total of $H$ bootstrapped datasets, performing node splits through considering only a subset of the features at a time.\n",
    "* Eventually, once all $H$ trees have been built, we average out the predictions made from all of the models to output our final prediction in which \n",
    "\n",
    "$$\\hat{f}_{\\text{final prediction}} (x) = \\frac{1}{H}\\sum_{h = 1} ^H \\hat{f}^{h} (x)$$\n",
    "\n",
    "becomes our final prediction. This process of boostrapping and then aggregating is sometimes referred to as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), and the fact that we only consider a subset of the predictors at each potential node split means that we have a random forest model rather than just bagging on its own.\n",
    "* It is often the case that we allow each tree $\\hat{f}^{h} (x)$ to grow very deep to reduce the bias of the individual tree to capture the true relationship. While this often results in a high variance estimate, taking the average will reduce the variance since the trees are built independently\n",
    "* It is not uncommon to use bagging for 100's if not 1000's of trees, though this can come at the expense of computation speed, and the fact that error might not neccessarily go down significantly after a few 100's of trees\n",
    "* Once we have built our model, we can also ascertain which variables across all our trees played the largest role in reducing the residual sum of squares using 'feature importance' functionality from Scikit-Learn. In this sense, we can even use random forests to help with feature selection for other models\n",
    "\n",
    "\n",
    "**Hyperparameters to Tune**\n",
    "* The hyperparameters for random forest regression is the same as for decision tree regression, but now we have access to an `n_estimators` hyperparameter which is the number of trees, $H$, to build when testing out each hyperparameter combination \n",
    "\n",
    "\n",
    "**Potential Disadvantages**\n",
    "* Because the random forest algorithm allows you to create $H$ bootastrapped datasets and average out all the predictions, when $H$ gets large, one can no longer visualise all the trees and the ability to interpret our model deteriorates. Though, while this is the case, they typically perform much better on out of sample data than just a single decision tree\n",
    "* One other disadvantage, similar to decision trees, is that they can't extrapolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e1aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing only 2dp\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python files\n",
    "import data_prep\n",
    "import helper_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5962af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')\n",
    "\n",
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5311c53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('numeric',\n",
       "                                                  Pipeline(steps=[('identity',\n",
       "                                                                   FunctionTransformer())]),\n",
       "                                                  ['GDP_cap']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['Status'])])),\n",
       "                ('imputation', KNNImputer()),\n",
       "                ('model', RandomForestRegressor())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = data_prep.create_pipeline(RandomForestRegressor())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b6f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters were...\n",
      "model__n_estimators had optimal value as: 187\n",
      "model__min_samples_split had optimal value as: 15\n",
      "model__min_samples_leaf had optimal value as: 7\n",
      "model__max_depth had optimal value as: 9\n",
      "model__criterion had optimal value as: absolute_error\n",
      "imputation__weights had optimal value as: uniform\n",
      "imputation__n_neighbors had optimal value as: 7\n",
      "\n",
      "The fitted model just initialised now has all these parameters set up\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'imputation__n_neighbors': np.arange(3, 21, 2), \n",
    "    'imputation__weights': ['uniform', 'distance'], \n",
    "    'model__n_estimators': np.arange(10, 200, 1),  # lots of trees\n",
    "    'model__min_samples_split': np.arange(10, 40, 1),\n",
    "    'model__min_samples_leaf': np.arange(3, 30, 1),\n",
    "    'model__max_depth': np.arange(3, 12, 1),\n",
    "    'model__criterion': [\"mse\", \"absolute_error\"] \n",
    "}\n",
    "\n",
    "tuned_model = data_prep.randomised_search_wrapper(X_train,\n",
    "                                                  y_train,\n",
    "                                                  pipe, \n",
    "                                                  param_grid, \n",
    "                                                  cv=10,\n",
    "                                                  n_iter=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
