{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ec2c55",
   "metadata": {},
   "source": [
    "## **Ridge Regression in Python Notes**\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "May 2021\n",
    "\n",
    "#### **Main Ideas**\n",
    "\n",
    "* [Ridge regression](https://machinelearningmastery.com/ridge-regression-with-python/) extends the concepts of OLS but makes some subtle adjustments through [Tikhonov regularisation](http://anderson.ece.gatech.edu/ece6254/assets/11-regression-regularisation.pdf)\n",
    "* The idea behind ridge regression is to address the concept of the bias-variance tradeoff in machine learning that suggests that optimising one tends to degrade the other\n",
    "* Ridge regression purposely introduces bias into the regression model in an effort to reduce the variance, which can then potentially lower the mean squared error of our estimator, since $$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n",
    "* Even though by the Gauss-Markov theorem, OLS has the lowest sampling variance out of any linear unbiased estimator, there may be a biased estimator that can achieve a lower mean squared error, such as the ridge estimator\n",
    "* Ridge regression is also a tool to help reduce the impact of multicollinearity within our feature matrix \n",
    "\n",
    "---\n",
    "\n",
    "#### **Algorithm Details**\n",
    "\n",
    "The loss function for OLS regression is given as:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = RSS = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{i,j})^2.$$\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta})$$\n",
    "\n",
    "---\n",
    "\n",
    "Ridge regression makes a small modification to the OLS loss function, through adding a shrinkage penalty through [L2 regularisation penalty](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261#f810), hence for ridge regression:\n",
    "\n",
    "$$J(\\beta_0, \\beta_1, ... , \\beta_p) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j X_{i,j})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j ^2$$\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\vec{\\beta})^T(y - X\\vec{\\beta}) + \\lambda\\vec{\\beta}^{T}\\vec{\\beta}$$\n",
    "\n",
    "<b>By convention, columns in $X$ are assumed to have zero mean and unit variance (after scaling), and the response vector $\\vec{y}$ is sometimes centered to have mean zero.<b>\n",
    "\n",
    "---\n",
    "\n",
    "The lambda parameter $\\lambda \\in [0, \\infty)$ is a constant that can be chosen through resampling methods such as cross validation. Ultimately, if $\\lambda = 0$ in the final model, the shrinkage penalty (the second term) disappears and we get OLS coefficient estimates. As $\\lambda$ gets larger, the shrinkage penalty becomes increasingly pertinent, and coefficient estimates will tend towards zero (but will not be exactly zero). Since $\\lambda$ is a hyperparameter that can be tuned, we get different coefficient estimates depending on which value for $\\lambda$ is chosen. Ultimately the shrinkage penalty aims to encourage simpler models that have smaller values for the coefficients as \"it turns out that shrinking the coefficient estimates can significantly reduce their variance\" - *An Introduction to Statistical Learning: With Applications in R*.\n",
    "    \n",
    "Also, the size constraint on the coefficients in the ridge\n",
    "regression \"alleviates the problem of large coefficients (in absolute value) and its high variance, which may be a consequence of multicollinearity.\" - *Rice University STAT 410 Lecture Slides*\n",
    "\n",
    "[Resource linked here](https://cpb-us-e1.wpmucdn.com/blogs.rice.edu/dist/e/8375/files/2017/08/Lecture16-1l5v69b.pdf) \n",
    "\n",
    "--- \n",
    "\n",
    "Expanding the terms in the loss function, we get\n",
    "\n",
    "$$J(\\vec{\\beta}) = \\vec{y}^T\\vec{y} -2\\vec{\\beta}^TX^T \\vec{y} + \\beta^TX^TX\\vec{\\beta} + \\lambda\\vec{\\beta}^{T}\\vec{\\beta}$$\n",
    "\n",
    "which is a convex function with a closed form solution when optimising coefficients. Taking the derivative of the loss function with respect to the beta vector we obtain:\n",
    "\n",
    "$$\\frac{\\partial J(\\vec{\\beta})}{\\partial \\vec{\\beta}} = -2X^{T}\\vec{y} + 2X^{T}X\\vec{\\beta} + 2\\lambda\\vec{\\beta}$$\n",
    "\n",
    "Since $J(\\vec{\\beta})$ is convex, to minimise this quantity, we can set the derivative equal to 0 to find an estimate $\\vec{b}_{ridge}$ for $\\vec{\\beta}$ thus:\n",
    "\n",
    "$$-2X^{T}\\vec{y} + 2X^{T}X\\vec{b} + 2\\lambda\\vec{b} = 0$$\n",
    "\n",
    "Moving, $-2X^{T}\\vec{y}$ to the other side, and dividing terms by two, we get \n",
    "\n",
    "$$X^{T}X\\vec{b} + \\lambda\\vec{b} = X^{T}\\vec{y}$$\n",
    "\n",
    "Factorising out a common factor of $\\vec{b}$ we get\n",
    "\n",
    "$$(X^{T}X + \\lambda I)\\vec{b} = X^{T}\\vec{y}$$\n",
    "\n",
    "\"Pre-multiplying\" both sides by $(X^{T}X + \\lambda I)^{-1}$ allows us to obtain\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "Including a positive lambda ensures that we obtain a non singular matrix for $(X^{T}X + \\lambda I)^{-1}$, even if $X^TX$ is singular (not of full rank)\n",
    "\n",
    "This optimisation problem to find $\\vec{b}_{ridge}$ could have also been solved using [Lagrange Multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier), where we would find our estimator using the Karush Kuhn-Tucker (KKT) multiplier method.\n",
    "\n",
    "$$\\text{argmin}_{||\\vec{\\beta}||_2 ^2 \\leq c}||\\vec{y} - X\\vec{\\beta}||_2 ^2$$\n",
    "\n",
    "where we optimise the beta vector subject to the constraint that $\\sum_{j=1}^p \\beta_{j}^2 \\leq c$.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Proving that $\\vec{b}_{ridge}$ is biased:**\n",
    "\n",
    "From above,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "Let $M = X^{T}X$, then:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (M + \\lambda I)^{-1}M(M^{-1}X^{T}\\vec{y})$$\n",
    "\n",
    "Factorising $M$ out in the first term and substituting the expression for $M$ into the second term, we obtain:\n",
    "\n",
    "$$\\vec{b}_{ridge} = [M(I + \\lambda M^{-1})]^{-1}M[(X^TX)^{-1}X^T\\vec{y}]$$\n",
    "\n",
    "Since by matrix inverse laws, $(AB)^{-1} = B^{-1}A^{-1}$, and since $\\vec{b}_{ols} = (X^TX)^{-1}X^T\\vec{y}$:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (I + \\lambda M^{-1})^{-1}M^{-1}M\\vec{b}_{ols}$$\n",
    "\n",
    "Since $A^{-1}A$ is the identity matrix for a matrix $A$, then:\n",
    "\n",
    "$$\\vec{b}_{ridge} = (I + \\lambda M^{-1})\\vec{b}_{ols}$$\n",
    "\n",
    "Taking the expectation of this simplified quantity, \n",
    "\n",
    "$$E(\\vec{b}_{ridge}) = E((I + \\lambda M^{-1})\\vec{b}_{ols})$$\n",
    "\n",
    "As $(I + \\lambda M^{-1})$ is not random and as the OLS estimator under Gauss Markov assumptions is unbiased, \n",
    "\n",
    "$$E(\\vec{b}_{ridge}) = (I + \\lambda M^{-1})\\vec{\\beta}_{ols}$$\n",
    "\n",
    "Which is not equal to $\\vec{\\beta}_{ols}$ if lambda is non-zero (and positive). But if lambda was zero then it is technically not ridge regression but rather just OLS.\n",
    "\n",
    "---\n",
    "\n",
    "**Variance of the ridge estimator**\n",
    "\n",
    "The variance of the OLS estimator was shown in a previous jupyter notebook to be given as:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ols}) = \\sigma^2(X^TX)^{-1}$$\n",
    "\n",
    "The ridge estimator of $\\vec{\\beta}$ can be given as \n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}\\vec{y}$$\n",
    "\n",
    "This can also be expressed as,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}X(X^{T}X)^{-1}X^T\\vec{y}$$\n",
    "\n",
    "Since $(X^{T}X)^{-1}X^T\\vec{y} = \\vec{b}_{ols}$,\n",
    "\n",
    "$$\\vec{b}_{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}X\\vec{b}_{ols}$$\n",
    "\n",
    "Taking the variance of both sides:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\text{Var}((X^{T}X + \\lambda I)^{-1}X^{T}X\\vec{b}_{ols})$$\n",
    "\n",
    "As $\\vec{b}_{ols}$ is a random vector, \n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = (X^{T}X + \\lambda I)^{-1}X^{T}X\\text{Var}(\\vec{b}_{ols})((X^{T}X + \\lambda I)^{-1}X^{T}X)^T$$\n",
    "\n",
    "Recognising that $\\text{Var}(\\vec{b}_{ols}) = \\sigma^2(X^TX)^{-1}$ under the homoskedasticity assumption, and by applying the idea that $(AB)^T = B^TA^T$ for matrices $A$ and $B$\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = (X^{T}X + \\lambda I)^{-1}X^{T}X\\sigma^2 (X^TX)^{-1}X^{T}X(X^{T}X + \\lambda I)^{-1}$$\n",
    "\n",
    "Cancelling terms out and assuming $\\sigma^2$ is constant,\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(X^{T}X + \\lambda I)^{-1}X^{T}X(X^{T}X + \\lambda I)^{-1}$$\n",
    "    \n",
    "The variance of the ridge estimator is always lower than that of OLS. The [proof](https://www.statlect.com/fundamentals-of-statistics/ridge-regression ) is quite long so consider a case where\n",
    "$X^TX = I$\n",
    "    \n",
    "If we substitute $X^TX = I$ into the equation above, we obtain\n",
    "    \n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(I + \\lambda)^{-1}(I + \\lambda)^{-1}$$\n",
    "\n",
    "Factorising out the identity matrix\n",
    "    \n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(1 + \\lambda)^{-1}(1 + \\lambda)^{-1}I$$\n",
    "\n",
    "Simplifying, we get\n",
    "\n",
    "$$\\text{Var}(\\vec{b}_{ridge}) = \\sigma^2(1 + \\lambda)^{-2}I$$\n",
    "    \n",
    "Which is certainly lower than the variance of the OLS estimator. Ultimately, different values of lambda will allow us to control both the magnitiude of the variance and the coefficients\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Useful property of the ridge estimator**\n",
    "\n",
    "In cases whether the columns of $X$ are orthonormal (i.e. the columns are orthogonal and each have unit length), then this orthogonal matrix $X$ adheres to:\n",
    "$$X^TX = X^{-1}X = I.$$ \n",
    "\n",
    "More profoundly, if can also be shown that when this condition is met, the ridge estimator is a multiple of the OLS estimator such that,\n",
    "\n",
    "$$\\vec{b}_{ridge} = \\frac{1}{1 + \\lambda}\\vec{b}_{ols}$$\n",
    "    \n",
    "If we were now to take the expectation of this quantity, we'd see that ridge estimator, on average, underestimates the true coefficient since \n",
    "$$E(\\vec{b}_{ridge}) = \\frac{1}{1+\\lambda}E(\\vec{b}_{ols}) = \\frac{1}{1+\\lambda}\\beta$$\n",
    "\n",
    "\n",
    "\n",
    "Extra resource [here](https://arxiv.org/pdf/1509.09169.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f107ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing only 2dp\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python files\n",
    "import data_prep\n",
    "import helper_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646b6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train_updated.csv')\n",
    "test = pd.read_csv('../datasets/test_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674c5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c350087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('numeric',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['GDP_cap']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['Status'])])),\n",
       "                ('imputation', KNNImputer()), ('model', Ridge())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = data_prep.create_pipeline(Ridge())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5b65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'imputation__n_neighbors': 3, 'imputation__weights': 'distance', 'model__alpha': 3.0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'imputation__n_neighbors': np.arange(3, 21, 2), \n",
    "    'imputation__weights': ['uniform', 'distance'],\n",
    "    'model__alpha': np.linspace(0, 3, 15)  # sklearn calls it alpha instead of lambda\n",
    "}\n",
    "\n",
    "best_estimator, best_params = data_prep.exhaustive_search(X_train, y_train, pipe, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "final_model = best_estimator.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88369c6",
   "metadata": {},
   "source": [
    "### Evaluate model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bdc36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.9357379493482635\n",
      "Mean Squared Error = 5.206254655759958\n",
      "Root Mean Squared Error = 2.281721862050666\n",
      "Mean Absolute Error = 1.7668402548997668\n"
     ]
    }
   ],
   "source": [
    "r2, mse, rmse, mae = helper_funcs.display_regression_metrics(y_test, final_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7ef8d",
   "metadata": {},
   "source": [
    "### Save piepline for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc6e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/Ridge Regression.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(final_model, './saved_models/Ridge Regression.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa453e6",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "- Year 2038\n",
    "- Infant Mortality of 2.91\n",
    "- 32.56% of GDP is spent on health in 2038\n",
    "- GDP per capita is 89,570\n",
    "- Employment to population ration (age 15+) is 64.49%\n",
    "- Developed Country\n",
    "- Average years of schooling is 11.79\n",
    "- 100% of the population has access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7f6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted life expectancy (using ridge regression) = 90.96883045598514\n"
     ]
    }
   ],
   "source": [
    "saved_pipeline = joblib.load('./saved_models/Ridge Regression.joblib')\n",
    "input_data = [2038, 2.91, 32.56, 89570, 64.49, 'Developed', 11.79, 100]\n",
    "\n",
    "print(f\"Predicted life expectancy (using ridge regression) = {helper_funcs.make_prediction(input_data, saved_pipeline, X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
