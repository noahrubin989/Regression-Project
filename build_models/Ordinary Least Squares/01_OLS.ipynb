{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7319fad3",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares Regression\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "May 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68249c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import joblib\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import xgboost as xgb\n",
    "# import matplotlib.pyplot as plt\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# from scipy import stats\n",
    "# from itertools import combinations\n",
    "# from IPython.display import display, Math\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.stats import diagnostic as diag\n",
    "# from statsmodels.stats.stattools import durbin_watson\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import r2_score, mean_squared_error\n",
    "# from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, ElasticNet\n",
    "# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "\n",
    "# # Personal display settings\n",
    "# #===========================\n",
    "\n",
    "# # Suppress scientific notation\n",
    "# np.set_printoptions(suppress=True)\n",
    "\n",
    "# # Get dataset values showing only 2dp\n",
    "# pd.options.display.float_format = '{:.2f}'.format\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# # For HD plots with my favourite background\n",
    "# plt.style.use('seaborn-whitegrid') \n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# # Updates when i change any of the relevant python files\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bca4e",
   "metadata": {},
   "source": [
    "### <u>Main Ideas</u>\n",
    "\n",
    "- OLS tries to create a fit that minimises the sum of the squared residuals (sometimes called residual sum of squares, RSS)\n",
    "- Residuals are the (red) vertical distances from the fit to our data points\n",
    "- OLS with multiple independent variables is referred to as Multiple Regression\n",
    "![mr_viz](OLS_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b3dd5",
   "metadata": {},
   "source": [
    "### <u>Algorithm Details</u>\n",
    "\n",
    "In the population, the multiple regression model can be given as:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + ... + \\beta_p x_{i,p} + \\epsilon_i$$\n",
    "\n",
    "* $y_i$ represents the $i^{\\text{ih}}$ observation for our response variable\n",
    "* $\\beta_0$ is the true intercept in the population\n",
    "* $\\beta_1$ is the true coefficient for our first predictor\n",
    "* $\\beta_2$ is the true coefficient for our second predictor\n",
    "* $\\beta_p$ is the true coefficient for our pth predictor\n",
    "* $x_1, x_2,..., x_p$ are predictor variables\n",
    "* $\\epsilon_i$ represent the unobservables/error term which is assumed to be distributed normally with:\n",
    "\n",
    "$$E(\\epsilon_i) = 0$$\n",
    "$$Var(\\epsilon_i) = \\sigma^2 \\text{(constant but unknown error variance)}$$ \n",
    "\n",
    "It turns out that when deriving OLS with multiple predictor variables, it is easier to see it work as a process of manipulating matrices. Applying the logic from above, it can be seen that\n",
    "\n",
    "$$y_1 = \\beta_0(1) + \\beta_1 x_{1,1} + \\beta_2 x_{1,2} + ... + \\beta_p x_{1,p} + \\epsilon_1$$\n",
    "$$y_2 = \\beta_0(1) + \\beta_1 x_{2,1} + \\beta_2 x_{2,2} + ... + \\beta_p x_{2,p} + \\epsilon_2$$\n",
    "$$y_3 = \\beta_0(1) + \\beta_1 x_{3,1} + \\beta_2 x_{3,2} + ... + \\beta_p x_{3,p} + \\epsilon_3$$\n",
    "$$\\vdots$$\n",
    "$$y_n = \\beta_0(1) + \\beta_1 x_{n,1} + \\beta_2 x_{n,2} + ... + \\beta_p x_{n,p} + \\epsilon_n$$\n",
    "\n",
    "\n",
    "From this it can be seen that we can store our all of our $y_i$ values in an $(n \\times 1)$ column vector and our of errors in a vector of the same size, such that:\n",
    "\n",
    "$$\\overrightarrow{y} = \\begin{pmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{pmatrix} \n",
    "\\text{, } \n",
    "\\overrightarrow{\\epsilon} = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Each of the $\\beta_j$'s never change across observations so we can store all of our population beta coefficients in a column vecor with dimension $((p+1) \\times 1)$ as we have $p$ predictor variables as well as an intercept term. Hence, our coefficient vector of betas can be expressed as \n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\beta} = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\vdots\\\\\\beta_p\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Lastly if we look at each row we see that each beta is multiplied by an $x_{i,j}$ term, though $\\beta_0$ is just multiplied by 1. So if we take all of our $x_{i, j}$'s out and place them in a [design matrix](http://gradientdescending.com/design-matrix-for-regression-explained/), we simply get all of our $p$ predictor variables as well as a column of ones (which caters for the existence of an intercept term). Hence our design matrix is of dimension $(n \\times (p+1))$ and takes the form\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & ... & x_{1,p}\\\\\n",
    "1 & x_{2,1} & x_{2,2} & ... & x_{2,p}\\\\\n",
    "1 & x_{3,1} & x_{3,2} & ... & x_{3,p}\\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    "1 & x_{n,1} & x_{n,2} & ... & x_{n,p}\\\\\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "If we multiply our design matrix and our vector of coefficients, and then add the vector of error terms, the following equation allows us to find each individual $y_i$ using matrix multiplication and vector addition\n",
    "\n",
    "$$\\begin{pmatrix}y_1\\\\y_2\\\\y_3\\\\\\vdots\\\\y_n\\end{pmatrix} = \\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & ... & x_{1,p}\\\\\n",
    "1 & x_{2,1} & x_{2,2} & ... & x_{2,p}\\\\\n",
    "1 & x_{3,1} & x_{3,2} & ... & x_{3,p}\\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    "1 & x_{n,1} & x_{n,2} & ... & x_{n,p}\\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}{\\beta_0}\\\\\\beta_1\\\\{\\beta_2}\\\\\\vdots\\\\{\\beta_p}\\end{pmatrix} + \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\epsilon_3\\\\\\vdots\\\\\\epsilon_n\\end{pmatrix}$$\n",
    "\n",
    "In short,\n",
    "\n",
    "$$\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888aaa7",
   "metadata": {},
   "source": [
    "As we wish to minimise the sum of squared deviations between the actual value of $y$ and its expectated value $X\\vec{\\beta}$ (assuming errors are noramlly distribited with mean zero and unknown but constant variance),\n",
    "we can minimise the quantity \n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - (X\\vec{\\beta})_i)^2 = \\vec{\\epsilon}^T \\vec{\\epsilon} = \\epsilon_1^2 + \\epsilon_2^2 + ... + \\epsilon_n^2$$\n",
    "\n",
    "Also expressable as \n",
    "\n",
    "$$J(\\vec{\\beta}) = (\\vec{y} - X\\beta)^T(\\vec{y} - X\\beta)$$ \n",
    "\n",
    "Expanding the brackets,\n",
    "\n",
    "$$J(\\vec{\\beta}) = \\vec{y}^T\\vec{y} -2\\vec{b}^TX^T \\vec{y} + \\vec{\\beta}^TX^TX\\vec{\\beta}$$\n",
    "\n",
    "Now, as we wish to minimise this cost function $J(\\vec{\\beta})$ we can take the partial derivative with respect to $\\vec{\\beta}$ and set this to zero. It always ends up being a minimum as $J(\\vec{\\beta})$ is known to be a [convex function](https://en.wikipedia.org/wiki/Convex_function) with a positive definite Hessian matrix when considering second derivatives\n",
    "\n",
    "Thus, taking the partial derivative with respect to beta (using matrix calculus) we obtain, \n",
    "\n",
    "$$\\frac{\\partial J(\\vec{\\beta})}{\\partial \\vec{\\beta}} = -2X{^T}\\vec{y} + 2X^TX\\vec{\\beta}$$ \n",
    "\n",
    "Setting this quantity to 0 for a mininum, we can obtain an estimnator $\\vec{b}$ for $\\vec{\\beta}$\n",
    "\n",
    "$$-2X{^T}\\vec{y} + 2X^TX\\vec{b} = 0$$\n",
    "\n",
    "Dividing by two and rearranging,\n",
    "\n",
    "$$X^TX\\vec{b} = X{^T}\\vec{y}.$$\n",
    "\n",
    "\"Pre-multiplying\" both sides by $(X^TX)^{-1}$ we minimise the sum of squared residuals through \n",
    "\n",
    "$$\\vec{b} = (X^TX)^{-1}X{^T}\\vec{y}$$\n",
    "\n",
    "...assuming the model is a full rank linear model where $(X^TX)$ is non-singular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5990d",
   "metadata": {},
   "source": [
    "#### <ins>Expectation & Variance</ins>\n",
    "\n",
    "Assuming a full rank linear model, the OLS estimator $\\vec{b}$ is unbiased since:\n",
    "$$E(\\vec{b}) = E((X^TX)^{-1}X^{T}\\vec{y}) = (X^TX)^{-1}X^{T}E(\\vec{y})$$\n",
    "\n",
    "Since $E(\\vec{y}) = E(X\\vec{\\beta} + \\vec{\\epsilon}) = X\\vec{\\beta}$ it follows that:\n",
    "\n",
    "$$E(\\vec{b}) = (X^TX)^{-1}X^{T}X\\vec{\\beta} = \\vec{\\beta}$$\n",
    "\n",
    "---\n",
    "\n",
    "Similarly the variance of our $\\vec{b}$ estimator is given as:\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\text{Var}((X^TX)^{-1}X^{T}\\vec{y})$$\n",
    "\n",
    "As $(X^TX)^{-1}X^{T}$ is a $(p+1) \\times n$ matrix of real numbers and $\\vec{y}$ is an $n \\times 1$ column-vector-valued random variable, it follows that\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = (X^TX)^{-1}X^T\\text{Var}\\vec{(y)}((X^TX)^{-1}X^T)^T$$\n",
    "\n",
    "Using matrix transpose laws, and the fact that $\\text{Var}\\vec{(y)} = \\text{Var}(X\\vec{\\beta} + \\vec{\\epsilon}) = \\sigma^2I$ (where $I$ is an $n \\times n$ identity matrix)\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = (X^TX)^{-1}X^T\\sigma^2IX((X^TX)^T)^{-1}$$\n",
    "\n",
    "Assuming constant error variance, \n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\sigma^2I(X^TX)^{-1}X^TX((X^TX)^T)^{-1}$$\n",
    "\n",
    "Cancelling terms $(X^TX)^{-1}$ and $X^TX$, it follows that \n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\sigma^2((X^TX)^T)^{-1}$$\n",
    "\n",
    "Since $X^TX$ is symmetric,\n",
    "\n",
    "$$\\text{Var}(\\vec{b}) = \\sigma^2(X^TX)^{-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafbf08",
   "metadata": {},
   "source": [
    "#### <ins>The Gauss Markov Theorem</ins>\n",
    "\n",
    "The Gauss Markov Theorem implies that the OLS estimator has the lowest sampling variance out of all linear unbiased estimators if Gauss Markov Assumptions are adhered to. Since we are dealing with unbiased estimators, the fact that OLS has the lowest sampling variance implies it has the lowest mean squared error.\n",
    "\n",
    "Consider $\\vec{b^*}$, an arbitrary linear unbiased estimator of $\\beta$ where $\\vec{b^*} = (X^TX)^{-1}X^T + M)\\vec{y}$, where $M$ is a $((p+1) \\times n)$ non-zero matrtix\n",
    "\n",
    "Taking the expectation of $\\vec{b^*}$, we obtain \n",
    "\n",
    "$$E(\\vec{b^*}) = E((X^TX)^{-1}X^T + M)\\vec{y})$$\n",
    "\n",
    "Substituting $\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}$\n",
    "\n",
    "$$E(\\vec{b^*}) = E((X^TX)^{-1}X^T + M)(X\\vec{\\beta} + \\vec{\\epsilon}))$$\n",
    "\n",
    "Expanding the brackets, and through linearity of the expectation operator\n",
    "\n",
    "$$E(\\vec{b^*}) = E([(X^TX)^{-1}X^T + M)]X\\vec{\\beta}) + [(X^TX)^{-1}X^T + M)]E(\\vec{\\epsilon})$$\n",
    "\n",
    "Since we assume $E(\\epsilon_i) = 0$\n",
    "\n",
    "$$E(\\vec{b^*}) = E([(X^TX)^{-1}X^T + M)]X\\vec{\\beta})$$\n",
    "\n",
    "Expanding the brackets, and the fact that $X$ is not considered random,\n",
    "\n",
    "$$E(\\vec{b^*}) = (X^TX)^{-1}X^TX\\vec{\\beta} + MX\\vec{\\beta}$$\n",
    "\n",
    "Due to cancellation in the first term\n",
    "\n",
    "$$E(\\vec{b^*}) = \\vec{\\beta} + MX\\vec{\\beta} = (I + MX)\\vec{\\beta}$$\n",
    "\n",
    "implying that this estimator can only be unbiased if $MX = 0.$\n",
    "\n",
    "---\n",
    "\n",
    "If we consider the variance of this arbitrary estimator\n",
    "\n",
    "\n",
    "\\begin{align} \n",
    "\\text{Var}(\\vec{b^*}) &= \\text{Var}((X^TX)^{-1}X^T + M)\\vec{y}) \\\\\n",
    "&= ((X^TX)^{-1}X^T + M)\\text{Var}(\\vec{y})((X^TX)^{-1}X^T + M)^T \\tag{since $((X^TX)^{-1}X^T + M)$ is not random}\\\\ \n",
    "&= \\sigma^2((X^TX)^{-1}X^T + M)((X^TX)^{-1}X^T + M)^T \\tag{assuming constant, unknown error variance}\\\\ \n",
    "&= \\sigma^2[((X^TX)^{-1}X^T + M)(X(X^TX)^{-1} + M^T)] \\\\ \n",
    "&= \\sigma^2[  (X^TX)^{-1}X^TX(X^TX)^{-1} + (X^TX)^{-1}X^TM^T + MX(X^TX)^{-1} + MM^T] \\\\ \n",
    "&= \\sigma^2(X^TX)^{-1} + \\sigma^2(X^TX)^{-1}X^TM^T + \\sigma^2MX(X^TX)^{-1} + \\sigma^2MM^T \\\\\n",
    "&= \\text{Var}(\\vec{\\beta}) + \\sigma^2(X^TX)^{-1}(MX)^T + \\sigma^2MX(X^TX)^{-1} + \\sigma^2MM^T \\tag{since $\\text{Var}(\\vec{\\beta}) = \\sigma^2(X^TX)^{-1}$}\\\\\n",
    "&= \\text{Var}(\\vec{\\beta}) + \\sigma^2MM^T \\tag{as $MX=0$ for unbiased estimate} \\\\\n",
    "&\\geq \\text{Var}(\\vec{\\beta}) \\tag{since $MM^T$ is positive semi-definite}\n",
    "\\end{align} \n",
    "\n",
    "Hence, as $MM^T$ is positive semi definite, then by the Gauss Markov theorem, $\\text{Var}(\\vec{b}) \\leq \\text{Var}(\\vec{b^*})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39fdf82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regression_project'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c282b10e80c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mols_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mregression_project\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_prep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regression_project'"
     ]
    }
   ],
   "source": [
    "import ols_functions\n",
    "from build_models import data_prep\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Personal display settings\n",
    "#===========================\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Get dataset values showing only 2dp\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For clear plots with a nice background\n",
    "plt.style.use('seaborn-whitegrid') \n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_updated.csv')\n",
    "test = pd.read_csv('../data/test_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "to_drop = ['Country', 'HDI', 'Life_exp']\n",
    "\n",
    "X_train = train.drop(to_drop, axis='columns')\n",
    "X_test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b9acd",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "* Log the GDP Variable\n",
    "* Apply one hot encoding to the 'Status' feature\n",
    "* Perform [KNN imputation](https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e)\n",
    "* Once the previous steps have been implemented, run OLS regression on the training data and through cross validation eventually see which value of K was optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function included in all the steps\n",
    "pipe = ols_functions.create_pipeline(LinearRegression())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f73e9b",
   "metadata": {},
   "source": [
    "Hyperparamter Tuning\n",
    "* Attempting to find the best vaklue for $k$ when using knn based data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'imputation__n_neighbors':np.arange(3, 21, 2), 'imputation__weights':['uniform', 'distance']}\n",
    "\n",
    "best_estimator, best_params = ols_functions.exhaustive_search(X_train,\n",
    "                                                              y_train, \n",
    "                                                              pipe, \n",
    "                                                              param_grid, \n",
    "                                                              cv=10,\n",
    "                                                              scoring='neg_mean_squared_error')\n",
    "final_model = best_estimator.fit(X_train, y_train)\n",
    "print(best_params)\n",
    "\n",
    "# print(final_model.named_steps['model'].intercept_)\n",
    "# print(final_model.named_steps['model'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a98b5",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\"><u>Model Evaluation</u></span>\n",
    "* Statsmodels is useful for model evaluation and hypothesis testing, though it does not support preprocessing and model building pipelines the way sklearn does\n",
    "* Will therefore perform the previous steps from scratch so that we can evaluate the model in statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d74a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually applies all the steps specified in the pipeline\n",
    "X_train_statsmodels, X_test_statsmodels = ols_functions.apply_preprocessing_steps(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table for our model\n",
    "model = sm.OLS(y_train, X_train_statsmodels).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76e1c9",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\"><u>Heteroskedasticity</u></span>\n",
    "\n",
    "* Homoskedasticity in regression analysis is the idea that the variance of the unobserved factors $\\epsilon$ should not change across different segments of the population\n",
    "* Hence $\\text{Var}(\\epsilon|x_1, x_2, ... x_p) = \\sigma^2$ (constant error variance) must be adhered to to ensure homoskedasticity\n",
    "* OLS coefficients can still remain unbiased in the presence of heteroskedasticity, and we can still interpret $R^2$ and adjusted $R^2$ to assess goodness-of-fit. Though the problem is that with heteroskedasticity our $F$ statistic (in the above table) is no longer F-distributed and the various $t$ statistics are not t-distributed. Hypothesis testing on our $\\hat{\\beta}_j$ coefficients can't be done as our p-values will be distorted due to incorrect standard errors\n",
    "* This means that most of the output in the table above can't be interpreted, and hence we are less sure of the effects of certain variables\n",
    "\n",
    "<u>Detecting Heteroscedasticity</u>\n",
    "\n",
    "* To detect heteroskedasticity, we can use tools such as the [Breusch-Pagan Test](https://www.real-statistics.com/multiple-regression/heteroskedasticity/breusch-pagan-test/), or we can simply graph a residual plot. \n",
    "* There are also other methods such as using [White's Test](https://www.youtube.com/watch?v=M5xqpKzhyAM), which is similar to the Breusch Pagan test, but considers squared terms and well as cross products of independent variable combinations.\n",
    "\n",
    "---\n",
    "\n",
    "**The Breusch-Pagan Test**\n",
    "\n",
    "We define a null and alternate hypothesis\n",
    "\n",
    "$$H_0: \\text{Var}(\\epsilon|x_1, x_2, ... x_p) = \\sigma^2 (\\text{ homoskedastic errors})$$\n",
    "\n",
    "$$H_1: \\text{Var}(\\epsilon|x_1, x_2, ... x_p) \\neq \\sigma^2 (\\text{ heteroskedastic errors})$$\n",
    "\n",
    "It is also possible to define the null hypothesis as $E(\\epsilon^2|x_1, x_2, ... x_p) = \\sigma^2$ since  $\\text{Var}(\\epsilon|x_1, x_2, ... x_p) = E(\\epsilon^2|x_1, x_2, ... x_p) - [E(\\epsilon|x_1, x_2, ... x_p)]^2$. Due to the zero conditional mean assumption (no correlation between any of the regressors and the error term) that $E(\\epsilon|x_1, x_2, ... x_p) = 0$, this second term disappears, and we can therefore test whether $\\epsilon^2$ is related (in expected value) to one or more of the regressors in our model\n",
    "\n",
    "---\n",
    "\n",
    "1. First, we run a regression just like before, with our dependent variable $y$ and all of our predictor $x$ variables\n",
    "2. As the regression estimate will likely never be a perfect fit for the data, there will be residuals present\n",
    "3. Store the squared residuals and then let it be the dependent variable for a new regression model, taking the form:\n",
    "\n",
    "$$\\hat{u}^2 = \\hat\\delta_0 + \\hat\\delta_1x_1 + \\hat\\delta_2x_2 + \\hat\\delta_3x_3 + ... \\hat\\delta_px_p$$\n",
    "\n",
    "where $\\hat{u}^2$ represents the fitted values for the squared residuals which became our new dependent variable, the $\\hat\\delta$'s represents the parameters that minimise RSS for this new model, and the $x$ variables represents InfantMortality. MeanSchooling etc.\n",
    "\n",
    "4. We then calculate the $R^2$ coefficient of determination for this model and use this to calculate a test statistic known as the <ins>Lagrange Multiplier</ins>. This is given as:\n",
    "\n",
    "$$\\text{LM}=nR^{2}$$\n",
    "\n",
    "where $n$ refers to the number of observations in our training set. Under the null hypothesis, this is distributed as a [Chi-squared](https://www.sciencedirect.com/topics/mathematics/chi-square-distribution) random variable with $p$ degrees of freedom, where $p$ is the number of covariates ($x$ variables) in our current model. Depending on the significance level and $p$-values we either reject or fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_functions.test_heteroskedasticity(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42fc674",
   "metadata": {},
   "source": [
    "Add Taylor Series stuff on heteroskedasticity here \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa98f9",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\"><u>Multicollinearity</u></span>\n",
    "\n",
    "* It is nice to have features strongly correlated with life expectancy, but what if our predictor variables are correlated with each other?\n",
    "* \"In statistics, multicollinearity is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\" ~ Wikipedia\n",
    "\n",
    "--- \n",
    "\n",
    "**Why it can be problematic?**\n",
    "\n",
    "* Essentially if multicollinearity exists, it becomes very difficult to discern which variable is playing a larger role in our regression model as the two (or more) variables themselves are so similar.\n",
    "* The idea in regression is that our coefficients represent the change in $y$ when there is a one unit increase in a predictor variable $x$, Ceteris Paribus\n",
    "* But in cases where multicolinearity exists, often it is not possible to hold other predictor variables constant when one increases \n",
    "\n",
    "---\n",
    "\n",
    "**Mathematically...**\n",
    "\n",
    "- A model with significant multicolinearity will inflate the standard errors of the beta coefficients impacted by multicollinearity\n",
    "- As well as standard errors increasing in the prescence of multicollinearity, multicollinearity also makes our coefficients very sensitive to large swings. For example, taking a variable out or adding one in might cause large changes in our current coefficients. Hence we become less certain about what our coefficients might be in the population\n",
    "- Large standard errors will also distort your p-values as the standard error is used to calculate the t-statistic, which is then used to calculate the p-value. We might then conclude that a variable is insignificant and has no impact on $y$ when in fact it might\n",
    "- Though even when multicollinearity is present, the least-squares estimator can still be unbiased and efficient.\n",
    "- Though in cases where there is [perfect multicollinearity](https://www.dummies.com/education/economics/econometrics/perfect-multicollinearity-and-your-econometric-model/), the matrix $X^TX$ becomes singular, which means that OLS will not have a unique solution\n",
    "\n",
    "---\n",
    "\n",
    "**How can we check for multicollinearity?**\n",
    "\n",
    "1. Using a correlation heatmap\n",
    "    \n",
    "* This would involve looking at our correlation heatmap from before and seeing which **predictor** variables have correlation with one another close to -1 and 1.\n",
    "* This is nice for eyeballing 'red flags' but Variance Inflation Factor is a better option\n",
    "\n",
    "2. Using [Variance Inflation Factors (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor)\n",
    "\n",
    "The variance of the estimated coefficient for the jth predictor in our regression model is given as:\n",
    "\n",
    "$$\\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{\\sum_{i=1}^{n} {(x_{ij} - \\bar{x}_j)^2}} \\times \\text{VIF}_j$$\n",
    "\n",
    "Hence VIF is the factor by which $\\text{Var}(\\hat{\\beta}_j)$ increases due there being correlation between the $j^{th}$ predictor and other $x$ variables\n",
    "\n",
    "* VIF for each predictor variable is calculated using something known as **tolerance**. \n",
    "\n",
    "$$\\text{tolerance} = 1 - R^2$$\n",
    "\n",
    "* The VIF is then computed as:\n",
    "\n",
    "$$\\text{VIF} = \\frac{1}{\\text{tolerance}}$$\n",
    "\n",
    "---\n",
    "\n",
    "When calculating VIF for each predictor variable, we build auxilliary regression models. With an auxiliary regression model, we set one of our $x$ variables to be the response. There is no $y$ here as we are just trying to detect multicollinearity among our regressors\n",
    "\n",
    "$$\\hat{x}_1 = \\hat\\delta_0 + \\hat\\delta_1x_2 + \\hat\\delta_2x_3  + \\hat\\delta_3x_4 +...+ \\hat\\delta_px_p$$\n",
    "\n",
    "In similar fashion to this if we wanted to calculate the VIF for $x_2$ we would build an auxilliary regression of the form\n",
    "\n",
    "$$\\hat{x}_2 = \\hat\\gamma_0 + \\hat\\gamma_1x_1 + \\hat\\gamma_2x_3  + \\hat\\gamma_3x_4 +...+ \\hat\\gamma_px_p$$\n",
    "\n",
    "Now, because we don't want a model where one $x$ variable can be linearly predicted from other $x$ variables, we want $R^2$ to be rather low. A high $R^2$ (i.e. one very close to 1) will mean that **tolerance** gets closer to 0 (refer to the tolerance formula), which would then mean that our VIF becomes very large, such that:\n",
    "\n",
    "\\\\[\\frac{\\text{1}}{\\text{a small tolerance number close to 0}} = \\text{a very large number} \\\\]\n",
    "\n",
    "Typically if our VIF for a particular variable is over 5 (some argue 10), then the prescence of the variable is causing multicolinearity. Statsmodels suggests in its documentation to go for 5 as the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Display VIF function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a8974",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\"><u>Distribution of the Error Term</u></span>\n",
    "\n",
    "Ideally, we would like the population error $\\epsilon$ to be independent of our regressors and distributed normally (with zero mean and constant error variance). While we don't have the population errors, we can use our sample residuals to make inferences about the population. Though in our case the fact that there was heteroskedasticity implies that there is non-constant error variance present. Nevertheless, it can still be useful to test for normality\n",
    "\n",
    "Some ways of testing for normality include\n",
    "\n",
    "- Plot a density curve\n",
    "- Measure skewness\n",
    "- QQ-plot\n",
    "- Kolmogorov-Smirnov test \n",
    "- Jarque-Bera test\n",
    "- Shapiro-Wilk test\n",
    "- Anderson-Darling test\n",
    "- D'Agostino-Pearson test\n",
    "\n",
    "<u>Kolmogorov-Smirnov Test (KS Test)</u>\n",
    "* The KS Test is a non-parametric statistical test, so it does assume anything about the distribution of the data\n",
    "* The KS Test relies on what it known as a [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (CDF). \n",
    "\n",
    "\n",
    "<u>How does the concept of a CDF relate to the KS Test?</u>\n",
    "\n",
    "* The purpose of the KS Test is to test for differences in the overall shape of two distributions\n",
    "* The two distributions in our case refer to a the CDF of a standard normal distribution and what we call an Empirical CDF (ECDF)\n",
    "* An ECDF represents a CDF but for empirical data (observed data), which in turn gives us the probability of observing an $x$ value less than or equal to the one in question\n",
    "* Though because we are comparing our observed data (the model residuals) against a standard normal distribution, we need to standardise our values as well\n",
    "* The data needs to be continous in order to do this test.\n",
    "\n",
    "<u>Hypothesis Testing</u>\n",
    "\n",
    "1. Null and alternate hypothesis \n",
    "\n",
    "* $H_0$: The two samples have been drawn from the same population distribution\n",
    "* $H_1$: The two samples have NOT been drawn from the same population distribution\n",
    "\n",
    "2. Determine a level of significance $\\alpha$\n",
    "\n",
    "* I'll do the statistical test at a 95% confidence level, hence $\\alpha = 0.05$\n",
    "\n",
    "3. Calculate the test statistic 'D' and the corresponding p-value.\n",
    "\n",
    "* In this case, our test statistic 'D' simply refers to the absolute value of the largest vertical distance from our ECDF to the CDF of the standard normal distribution (generic example graphed below). The p-value is then interpreted as the following: \n",
    "\n",
    "* If the samples have been drawn from the same population distribution (i.e. if the null hypothesis is true), what is the probability of obtaining a D statistic value at least as extreme as the one that was calculated\n",
    "* We calculate our p-value based off the Kolmogorov distribution and if our p-value is less than 0.05, there is enough evidence to reject the null hypothesis\n",
    "---\n",
    "\n",
    "This graph shows a CDF (red) vs an ECDF (blue), with the D statistic being the maximum vertical distance shown (the black arrow)\n",
    "\n",
    "\n",
    "![Random Unsplash Image](https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64120952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4499c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552603be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c8859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0ae35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8cbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89c22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7141c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279892c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f87bb2",
   "metadata": {},
   "source": [
    "<ins>**Implementation (using real data now)**</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b013e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing sets\n",
    "X_train = train.drop(['Country', 'GDP_cap', 'Life_exp'], axis='columns')\n",
    "X_test = test.drop(['Country', 'GDP_cap', 'Life_exp'], axis='columns')\n",
    "\n",
    "y_train = train['Life_exp']\n",
    "y_test = test['Life_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ef032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing data using KNNImputer\n",
    "# Will be applying cross validation to this process for some of the other fancier models\n",
    "\n",
    "imp = KNNImputer(n_neighbors=5)\n",
    "X_train_new = pd.DataFrame(imp.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_new = pd.DataFrame(imp.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build linear model with all these variables included (apart from the ones dropped)\n",
    "X_train_new1 = sm.add_constant(X_train_new)\n",
    "\n",
    "# Fit the model\n",
    "linear_regression = sm.OLS(endog=y_train, exog=X_train_new1).fit()\n",
    "\n",
    "# Our b vector is the column under coef\n",
    "linear_regression.summary().tables[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a464d5",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\">**Model Diagnostics and Gauss Markov Assumptions**</span>.\n",
    "\n",
    "* Is this linear model our Best Linear <b>Unbiased</b> Estimator (BLUE)? \n",
    "* It is common to look into Gauss Markov regression assumptions when assessing this\n",
    "\n",
    "1. The model is linear in the parameters\n",
    "2. A random sample has been drawn from the population\n",
    "3. There is no perfect collinearity (there are no situations where one variable is an exact linear combination of others) ✅\n",
    "4. The error term is not correlated with any of the regressors, hence $E(\\epsilon_i | x_1, x_2, ..., x_p) = 0$ (Zero Conditional Mean)\n",
    "\n",
    "If these assumptions are met, each $\\hat{\\beta}_j$ coefficient is unbiased, hence:\n",
    "\n",
    "$$E(\\hat{\\beta}_j) = \\beta_j$$ \n",
    "\n",
    "So we expect the estimator to be equal to the true value of the population parameter as we take an increasing number of samples\n",
    "\n",
    "There is another Gauss Markov assumption, [Homoscedasticity](https://statisticsbyjim.com/regression/heteroscedasticity-regression/) which is required in order for the 'Best' part of '<b>Best</b> Linear Unbiased Estimator' to be true. Not having homoskedasticity (i.e having heteroskedasticity) does not cause OLS estimators to be biased, but it is no longer BLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae65fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b26277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0374593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d6271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00a324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8093d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad731364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef010c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9687f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0955c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45963c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
